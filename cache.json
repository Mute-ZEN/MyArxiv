{"2025-01-31T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.05362v2","updated":"2025-01-31T18:58:24Z","published":"2024-10-07T17:45:00Z","title":"LLMs Are In-Context Bandit Reinforcement Learners","summary":"  Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.\n","authors":["Giovanni Monea","Antoine Bosselut","Kianté Brantley","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.05362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18101v2","updated":"2025-01-31T18:57:58Z","published":"2025-01-30T02:47:41Z","title":"Diverse Preference Optimization","summary":"  Post-training of language models, either through reinforcement learning,\npreference optimization or supervised finetuning, tends to sharpen the output\nprobability distribution and reduce the diversity of generated responses. This\nis particularly a problem for creative generative tasks where varied responses\nare desired. In this work we introduce Diverse Preference Optimization (DivPO),\nan optimization method which learns to generate much more diverse responses\nthan standard pipelines, while maintaining the quality of the generations. In\nDivPO, preference pairs are selected by first considering a pool of responses,\nand a measure of diversity among them, and selecting chosen examples as being\nmore rare but high quality, while rejected examples are more common, but low\nquality. DivPO results in generating 45.6% more diverse persona attributes, and\nan 74.6% increase in story diversity, while maintaining similar win rates as\nstandard baselines.\n","authors":["Jack Lanchantin","Angelica Chen","Shehzaad Dhuliawala","Ping Yu","Jason Weston","Sainbayar Sukhbaatar","Ilia Kulikov"],"pdf_url":"https://arxiv.org/pdf/2501.18101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19399v1","updated":"2025-01-31T18:55:35Z","published":"2025-01-31T18:55:35Z","title":"Scalable-Softmax Is Superior for Attention","summary":"  The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.\n","authors":["Ken M. Nakanishi"],"pdf_url":"https://arxiv.org/pdf/2501.19399v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.19393v1","updated":"2025-01-31T18:48:08Z","published":"2025-01-31T18:48:08Z","title":"s1: Simple test-time scaling","summary":"  Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1 exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling s1\nwith budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1.\n","authors":["Niklas Muennighoff","Zitong Yang","Weijia Shi","Xiang Lisa Li","Li Fei-Fei","Hannaneh Hajishirzi","Luke Zettlemoyer","Percy Liang","Emmanuel Candès","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2501.19393v1.pdf","comment":"46 pages (9 main), 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2501.19383v1","updated":"2025-01-31T18:37:42Z","published":"2025-01-31T18:37:42Z","title":"Decoding-based Regression","summary":"  Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.\n","authors":["Xingyou Song","Dara Bahri"],"pdf_url":"https://arxiv.org/pdf/2501.19383v1.pdf","comment":"Google DeepMind Technical Report, 25 pages. Code can be found at\n  https://github.com/google-research/optformer/tree/main/optformer/decoding_regression"},{"id":"http://arxiv.org/abs/2501.19378v1","updated":"2025-01-31T18:31:31Z","published":"2025-01-31T18:31:31Z","title":"TableMaster: A Recipe to Advance Table Understanding with Language\n  Models","summary":"  Tables serve as a fundamental format for representing structured relational\ndata. While current language models (LMs) excel at many text-based tasks, they\nstill face challenges in table understanding due to the complex characteristics\nof tabular data, such as their structured nature. In this paper, we aim to\nenhance LMs for improved table understanding. We identify four key challenges:\n1) difficulty in locating target data, 2) deficiency in table semantics, 3)\nnumerical inaccuracies in textual reasoning, and 4) semantic inflexibility in\nsymbolic reasoning. To address these issues, we propose TableMaster, a recipe\nand comprehensive framework that integrates multiple solutions to overcome\nthese obstacles. TableMaster first extracts relevant table content and\nverbalizes it with enriched semantic context. Additionally, we introduce\nadaptive reasoning, a flexible approach that dynamically adjusts between\ntextual and symbolic reasoning, tailoring the reasoning process to each query.\nExtensive analyses and experiments demonstrate our findings and the\neffectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an\naccuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.\n","authors":["Lang Cao"],"pdf_url":"https://arxiv.org/pdf/2501.19378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19377v1","updated":"2025-01-31T18:30:36Z","published":"2025-01-31T18:30:36Z","title":"SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions","summary":"  In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.\n","authors":["Dominik Wagner","Alexander Churchill","Siddarth Sigtia","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2501.19377v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.02755v3","updated":"2025-01-31T18:21:59Z","published":"2024-10-03T17:58:29Z","title":"GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data","summary":"  Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data.\n","authors":["Jifan Zhang","Ziyue Luo","Jia Liu","Ness Shroff","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2410.02755v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19361v1","updated":"2025-01-31T18:12:41Z","published":"2025-01-31T18:12:41Z","title":"We're Different, We're the Same: Creative Homogeneity Across LLMs","summary":"  Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.\n","authors":["Emily Wenger","Yoed Kenett"],"pdf_url":"https://arxiv.org/pdf/2501.19361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19353v1","updated":"2025-01-31T18:02:19Z","published":"2025-01-31T18:02:19Z","title":"Do Large Multimodal Models Solve Caption Generation for Scientific\n  Figures? Lessons Learned from SCICAP Challenge 2023","summary":"  Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?\n","authors":["Ting-Yao E. Hsu","Yi-Li Hsu","Shaurya Rohatgi","Chieh-Yang Huang","Ho Yin Sam Ng","Ryan Rossi","Sungchul Kim","Tong Yu","Lun-Wei Ku","C. Lee Giles","Ting-Hao K. Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19353v1.pdf","comment":"Accepted to TACL 2025"},{"id":"http://arxiv.org/abs/2410.16665v2","updated":"2025-01-31T18:01:12Z","published":"2024-10-22T03:38:37Z","title":"SafetyAnalyst: Interpretable, transparent, and steerable safety\n  moderation for AI behavior","summary":"  The ideal AI safety moderation system would be both structurally\ninterpretable (so its decisions can be reliably explained) and steerable (to\nalign to safety standards and reflect a community's values), which current\nsystems fall short on. To address this gap, we present SafetyAnalyst, a novel\nAI safety moderation framework. Given an AI behavior, SafetyAnalyst uses\nchain-of-thought reasoning to analyze its potential consequences by creating a\nstructured \"harm-benefit tree,\" which enumerates harmful and beneficial actions\nand effects the AI behavior may lead to, along with likelihood, severity, and\nimmediacy labels that describe potential impact on any stakeholders.\nSafetyAnalyst then aggregates all harmful and beneficial effects into a\nharmfulness score using fully interpretable weight parameters, which can be\naligned to particular safety preferences. We applied this conceptual framework\nto develop, test, and release an open-source LLM prompt safety classification\nsystem, distilled from 18.5 million harm-benefit features generated by frontier\nLLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we\nshow that SafetyReporter (average F1=0.81) outperforms existing LLM safety\nmoderation systems (average F1$<$0.72) on prompt safety classification, while\noffering the additional advantages of interpretability, transparency, and\nsteerability.\n","authors":["Jing-Jing Li","Valentina Pyatkin","Max Kleiman-Weiner","Liwei Jiang","Nouha Dziri","Anne G. E. Collins","Jana Schaich Borg","Maarten Sap","Yejin Choi","Sydney Levine"],"pdf_url":"https://arxiv.org/pdf/2410.16665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17348v2","updated":"2025-01-31T17:51:30Z","published":"2025-01-28T23:50:02Z","title":"Better Slow than Sorry: Introducing Positive Friction for Reliable\n  Dialogue Systems","summary":"  While theories of discourse and cognitive science have long recognized the\nvalue of unhurried pacing, recent dialogue research tends to minimize friction\nin conversational systems. Yet, frictionless dialogue risks fostering\nuncritical reliance on AI outputs, which can obscure implicit assumptions and\nlead to unintended consequences. To meet this challenge, we propose integrating\npositive friction into conversational AI, which promotes user reflection on\ngoals, critical thinking on system response, and subsequent re-conditioning of\nAI systems. We hypothesize systems can improve goal alignment, modeling of user\nmental states, and task success by deliberately slowing down conversations in\nstrategic moments to ask questions, reveal assumptions, or pause. We present an\nontology of positive friction and collect expert human annotations on\nmulti-domain and embodied goal-oriented corpora. Experiments on these corpora,\nalong with simulated interactions using state-of-the-art systems, suggest\nincorporating friction not only fosters accountable decision-making, but also\nenhances machine understanding of user beliefs and goals, and increases task\nsuccess rates.\n","authors":["Mert İnan","Anthony Sicilia","Suvodip Dey","Vardhan Dongre","Tejas Srinivasan","Jesse Thomason","Gökhan Tür","Dilek Hakkani-Tür","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2501.17348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19339v1","updated":"2025-01-31T17:39:21Z","published":"2025-01-31T17:39:21Z","title":"PixelWorld: Towards Perceiving Everything as Pixels","summary":"  Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.\n","authors":["Zhiheng Lyu","Xueguang Ma","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2501.19339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14713v2","updated":"2025-01-31T17:38:07Z","published":"2025-01-24T18:46:37Z","title":"FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing","summary":"  The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.\n","authors":["James Seale Smith","Chi-Heng Lin","Shikhar Tuli","Haris Jeelani","Shangqian Gao","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"pdf_url":"https://arxiv.org/pdf/2501.14713v2.pdf","comment":"Accepted to NAACL 2025 - Main Conference"},{"id":"http://arxiv.org/abs/2501.19337v1","updated":"2025-01-31T17:36:12Z","published":"2025-01-31T17:36:12Z","title":"Homogeneity Bias as Differential Sampling Uncertainty in Language Models","summary":"  Prior research show that Large Language Models (LLMs) and Vision-Language\nModels (VLMs) represent marginalized groups more homogeneously than dominant\ngroups. However, the mechanisms underlying this homogeneity bias remain\nrelatively unexplored. We propose that this bias emerges from systematic\ndifferences in the probability distributions from which tokens are sampled at\ninference-time. Analyzing three measures of uncertainty in token sampling\ndistributions-entropy, perplexity, and probability of differentiation-we find\nthat in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled\nmore deterministically when generating texts about marginalized groups (i.e.,\nBlack Americans and women) compared to their dominant group counterparts (i.e.,\nWhite Americans and men). While these findings may help explain homogeneity\nbias in certain models, the patterns did not replicate across all VLMs tested,\nsuggesting multiple mechanisms may contribute to homogeneity bias in AI.\n","authors":["Messi H. J. Lee","Soyeon Jeon"],"pdf_url":"https://arxiv.org/pdf/2501.19337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16347v2","updated":"2025-01-31T17:36:02Z","published":"2024-07-23T09:50:14Z","title":"FACTTRACK: Time-Aware World State Tracking in Story Outlines","summary":"  While accurately detecting and correcting factual contradictions in language\nmodel outputs has become increasingly important as their capabilities improve,\ndoing so is highly challenging. We propose a novel method, FACTTRACK, for\ntracking atomic facts and addressing factual contradictions. Crucially,\nFACTTRACK also maintains time-aware validity intervals for each fact, allowing\nfor change over time. At a high level, FACTTRACK consists of a four-step\npipeline to update a world state data structure for each new event: (1)\ndecompose the event into directional atomic facts; (2) determine the validity\ninterval of each atomic fact using the world state; (3) detect contradictions\nwith existing facts in the world state; and finally (4) add new facts to the\nworld state and update existing atomic facts. When we apply FACTTRACK to\ncontradiction detection on structured story outlines, we find that FACTTRACK\nusing LLaMA2-7B-Chat substantially outperforms a fair baseline using\nLLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.\nMoreover, when using GPT4, FACTTRACK significantly outperforms the GPT4\nbaseline.\n","authors":["Zhiheng Lyu","Kevin Yang","Lingpeng Kong","Daniel Klein"],"pdf_url":"https://arxiv.org/pdf/2407.16347v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2501.17282v3","updated":"2025-01-31T17:26:12Z","published":"2025-01-28T20:30:36Z","title":"From Natural Language to Extensive-Form Game Representations","summary":"  We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.\n","authors":["Shilong Deng","Yongzhao Wang","Rahul Savani"],"pdf_url":"https://arxiv.org/pdf/2501.17282v3.pdf","comment":"This work has been accepted as a full paper for AAMAS 2025. This is a\n  full version of the AAMAS 2025 proceedings"},{"id":"http://arxiv.org/abs/2501.19324v1","updated":"2025-01-31T17:19:57Z","published":"2025-01-31T17:19:57Z","title":"Reward-Guided Speculative Decoding for Efficient LLM Reasoning","summary":"  We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios.\n","authors":["Baohao Liao","Yuhui Xu","Hanze Dong","Junnan Li","Christof Monz","Silvio Savarese","Doyen Sahoo","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2501.19324v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2501.19321v1","updated":"2025-01-31T17:16:45Z","published":"2025-01-31T17:16:45Z","title":"Language Bias in Self-Supervised Learning For Automatic Speech\n  Recognition","summary":"  Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.\n","authors":["Edward Storey","Naomi Harte","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2501.19321v1.pdf","comment":"Accepted to Speech and Language Technology Workshop (SLT) 2024\n  accessible on IEEE Xplore"},{"id":"http://arxiv.org/abs/2501.19317v1","updated":"2025-01-31T17:12:55Z","published":"2025-01-31T17:12:55Z","title":"LLM-based Affective Text Generation Quality Based on Different\n  Quantization Values","summary":"  Large language models exhibit a remarkable capacity in language generation\nand comprehension. These advances enable AI systems to produce more human-like\nand emotionally engaging text. However, these models rely on a large number of\nparameters, requiring significant computational resources for training and\ninference. In some scenarios, accessing these resources can be challenging\n(e.g., budget or hardware limitations). Techniques like reducing precision bits\ncan make models more memory-efficient, reducing the computational resources\nneeded, at the cost of reduced accuracy. This paper addresses the trade-off\nbetween different quantization values, GPU RAM utilization, and text quality in\naffective text generation (e.g., \"I really enjoy running in the snow-covered\nforest\"). To evaluate, we use an emotion classifier and ten seed prompts to\ngenerate affective text. We test three setups of precision bits (8, 16, and 32)\nacross five open-weight language models from two different families. Our\nfindings demonstrate that bit reductions lead to memory savings, achieving a\nreduction of 76%. However, this optimization comes with a trade-off, leading to\na decrease of up to 10 pp in F1 score for larger models and an increase of 10\npp for smaller models, along with roughly double the inference time. In terms\nof text quality, larger models at lower quantization levels generally\noutperform smaller, higher-precision models -- while requiring similar memory.\n","authors":["Yarik Menchaca Resendiz","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2501.19317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19316v1","updated":"2025-01-31T17:12:53Z","published":"2025-01-31T17:12:53Z","title":"Reverse Probing: Evaluating Knowledge Transfer via Finetuned Task\n  Embeddings for Coreference Resolution","summary":"  In this work, we reimagine classical probing to evaluate knowledge transfer\nfrom simple source to more complex target tasks. Instead of probing frozen\nrepresentations from a complex source task on diverse simple target probing\ntasks (as usually done in probing), we explore the effectiveness of embeddings\nfrom multiple simple source tasks on a single target task. We select\ncoreference resolution, a linguistically complex problem requiring contextual\nunderstanding, as focus target task, and test the usefulness of embeddings from\ncomparably simpler tasks tasks such as paraphrase detection, named entity\nrecognition, and relation extraction. Through systematic experiments, we\nevaluate the impact of individual and combined task embeddings.\n  Our findings reveal that task embeddings vary significantly in utility for\ncoreference resolution, with semantic similarity tasks (e.g., paraphrase\ndetection) proving most beneficial. Additionally, representations from\nintermediate layers of fine-tuned models often outperform those from final\nlayers. Combining embeddings from multiple tasks consistently improves\nperformance, with attention-based aggregation yielding substantial gains. These\ninsights shed light on relationships between task-specific representations and\ntheir adaptability to complex downstream tasks, encouraging further exploration\nof embedding-level task transfer.\n","authors":["Tatiana Anikina","Arne Binder","David Harbecke","Stalin Varanasi","Leonhard Hennig","Simon Ostermann","Sebastian Möller","Josef van Genabith"],"pdf_url":"https://arxiv.org/pdf/2501.19316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19314v1","updated":"2025-01-31T17:11:45Z","published":"2025-01-31T17:11:45Z","title":"An Efficient Approach for Machine Translation on Low-resource Languages:\n  A Case Study in Vietnamese-Chinese","summary":"  Despite the rise of recent neural networks in machine translation, those\nnetworks do not work well if the training data is insufficient. In this paper,\nwe proposed an approach for machine translation in low-resource languages such\nas Vietnamese-Chinese. Our proposed method leveraged the power of the\nmultilingual pre-trained language model (mBART) and both Vietnamese and Chinese\nmonolingual corpus. Firstly, we built an early bird machine translation model\nusing the bilingual training dataset. Secondly, we used TF-IDF technique to\nselect sentences from the monolingual corpus which are the most related to\ndomains of the parallel dataset. Finally, the first model was used to\nsynthesize the augmented training data from the selected monolingual corpus for\nthe translation model. Our proposed scheme showed that it outperformed 8%\ncompared to the transformer model. The augmented dataset also pushed the model\nperformance.\n","authors":["Tran Ngoc Son","Nguyen Anh Tu","Nguyen Minh Tri"],"pdf_url":"https://arxiv.org/pdf/2501.19314v1.pdf","comment":"Technical report of VLSP 2022 NMT; The first two authors contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2501.19309v1","updated":"2025-01-31T17:09:53Z","published":"2025-01-31T17:09:53Z","title":"Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment","summary":"  The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively.\n","authors":["Gregor Bachmann","Sotiris Anagnostidis","Albert Pumarola","Markos Georgopoulos","Artsiom Sanakoyeu","Yuming Du","Edgar Schönfeld","Ali Thabet","Jonas Kohler"],"pdf_url":"https://arxiv.org/pdf/2501.19309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19306v1","updated":"2025-01-31T17:03:16Z","published":"2025-01-31T17:03:16Z","title":"SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling","summary":"  Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.\n","authors":["Jiefeng Chen","Jie Ren","Xinyun Chen","Chengrun Yang","Ruoxi Sun","Sercan Ö Arık"],"pdf_url":"https://arxiv.org/pdf/2501.19306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02675v3","updated":"2025-01-31T17:00:03Z","published":"2024-10-03T17:02:21Z","title":"FAN: Fourier Analysis Networks","summary":"  Despite the remarkable successes of general-purpose neural networks, such as\nMLPs and Transformers, we find that they exhibit notable shortcomings in\nmodeling and reasoning about periodic phenomena, achieving only marginal\nperformance within the training domain and failing to generalize effectively to\nout-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature and\nscience. Therefore, neural networks should be equipped with the essential\nability to model and handle periodicity. In this work, we propose FAN, a novel\ngeneral-purpose neural network that offers broad applicability similar to MLP\nwhile effectively addressing periodicity modeling challenges. Periodicity is\nnaturally integrated into FAN's structure and computational processes by\nintroducing the Fourier Principle. Unlike existing Fourier-based networks,\nwhich possess particular periodicity modeling abilities but are typically\ndesigned for specific tasks, our approach maintains the general-purpose\nmodeling capability. Therefore, FAN can seamlessly replace MLP in various model\narchitectures with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the superiority of FAN in periodicity modeling tasks and the\neffectiveness and generalizability of FAN across a range of real-world tasks,\ne.g., symbolic formula representation, time series forecasting, language\nmodeling, and image recognition.\n","authors":["Yihong Dong","Ge Li","Yongding Tao","Xue Jiang","Kechi Zhang","Jia Li","Jinliang Deng","Jing Su","Jun Zhang","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02675v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19301v1","updated":"2025-01-31T16:57:01Z","published":"2025-01-31T16:57:01Z","title":"Beyond checkmate: exploring the creative chokepoints in AI text","summary":"  Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding.\n","authors":["Nafis Irtiza Tripto","Saranya Venkatraman","Mahjabin Nahar","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2501.19301v1.pdf","comment":"18 pages, single columns, under review at Nature Machine Intelligence"},{"id":"http://arxiv.org/abs/2411.01703v2","updated":"2025-01-31T16:47:16Z","published":"2024-11-03T22:19:20Z","title":"UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but remain vulnerable to multimodal jailbreak attacks, where\nadversarial inputs are meticulously crafted to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard trains\na multimodal guardrail to minimize the likelihood of generating harmful\nresponses in a toxic corpus. The guardrail can be seamlessly applied to any\ninput prompt during inference with minimal computational costs. Extensive\nexperiments demonstrate the generalizability of UniGuard across multiple\nmodalities, attack strategies, and multiple state-of-the-art MLLMs, including\nLLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust\ndefense mechanism maintains the models' overall vision-language understanding\ncapabilities.\n","authors":["Sejoon Oh","Yiqiao Jin","Megha Sharma","Donghyun Kim","Eric Ma","Gaurav Verma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01703v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.19278v1","updated":"2025-01-31T16:42:31Z","published":"2025-01-31T16:42:31Z","title":"Pheromone-based Learning of Optimal Reasoning Paths","summary":"  Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities through chain-of-thought prompting, yet discovering effective\nreasoning methods for complex problems remains challenging due to the vast\nspace of possible intermediate steps. We introduce Ant Colony\nOptimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines\nACO with LLMs to discover optimal reasoning paths for complex problems\nefficiently. Drawing inspiration from Hebbian learning in neurological systems,\nour method employs a collection of distinctly fine-tuned LLM \"ants\" to traverse\nand lay pheromone trails through a centralized tree of thought, with each ant's\nmovement governed by a weighted combination of existing pheromone trails and\nits own specialized expertise. The algorithm evaluates complete reasoning paths\nusing a mixture-of-experts-based scoring function, with pheromones reinforcing\nproductive reasoning paths across iterations. Experiments on three challenging\nreasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT\nperforms significantly better than existing chain-of-thought optimization\napproaches, suggesting that incorporating biologically inspired collective\nsearch mechanisms into LLM inference can substantially enhance reasoning\ncapabilities.\n","authors":["Anirudh Chari","Aditya Tiwari","Richard Lian","Suraj Reddy","Brian Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.19278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06274v2","updated":"2025-01-31T16:41:17Z","published":"2025-01-10T08:00:58Z","title":"Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts\n  on Social Media","summary":"  The rise of misinformation and fake news in online political discourse poses\nsignificant challenges to democratic processes and public engagement. While\ndebunking efforts aim to counteract misinformation and foster fact-based\ndialogue, these discussions often involve language toxicity and emotional\npolarization. We examined over 86 million debunking tweets and more than 4\nmillion Reddit debunking comments to investigate the relationship between\nlanguage toxicity, pessimism, and social polarization in debunking efforts.\nFocusing on discussions of the 2016 and 2020 U.S. presidential elections and\nthe QAnon conspiracy theory, our analysis reveals three key findings: (1)\nperipheral participants (1-degree users) play a disproportionate role in\nshaping toxic discourse, driven by lower community accountability and emotional\nexpression; (2) platform mechanisms significantly influence polarization, with\nTwitter amplifying partisan differences and Reddit fostering higher overall\ntoxicity due to its structured, community-driven interactions; and (3) a\nnegative correlation exists between language toxicity and pessimism, with\nincreased interaction reducing toxicity, especially on Reddit. We show that\nplatform architecture affects informational complexity of user interactions,\nwith Twitter promoting concentrated, uniform discourse and Reddit encouraging\ndiverse, complex communication. Our findings highlight the importance of user\nengagement patterns, platform dynamics, and emotional expressions in shaping\npolarization in debunking discourse. This study offers insights for\npolicymakers and platform designers to mitigate harmful effects and promote\nhealthier online discussions, with implications for understanding\nmisinformation, hate speech, and political polarization in digital\nenvironments.\n","authors":["Wentao Xu","Wenlu Fan","Shiqian Lu","Tenghao Li","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.06274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09861v3","updated":"2025-01-31T16:28:15Z","published":"2024-07-13T12:01:52Z","title":"Towards Systematic Monolingual NLP Surveys: GenA of Greek NLP","summary":"  Natural Language Processing (NLP) research has traditionally been\npredominantly focused on English, driven by the availability of resources, the\nsize of the research community, and market demands. Recently, there has been a\nnoticeable shift towards multilingualism in NLP, recognizing the need for\ninclusivity and effectiveness across diverse languages and cultures.\nMonolingual surveys have the potential to complement the broader trend towards\nmultilingualism in NLP by providing foundational insights and resources,\nnecessary for effectively addressing the linguistic diversity of global\ncommunication. However, monolingual NLP surveys are extremely rare in the\nliterature. This study introduces a generalizable methodology for creating\nsystematic and comprehensive monolingual NLP surveys, aimed at optimizing the\nprocess of constructing such surveys and thoroughly addressing a language's NLP\nsupport. Our approach integrates a structured search protocol to avoid\nselection bias and ensure reproducibility, an NLP task taxonomy to organize the\nsurveyed material coherently, and language resources (LRs) taxonomies to\nidentify potential benchmarks and highlight opportunities for improving\nresource availability (e.g., through better maintenance or licensing). We apply\nthis methodology to Greek NLP (2012-2023), providing a comprehensive overview\nof its current state and challenges. We discuss the progress of Greek NLP and\noutline the Greek LRs found, classified by availability and usability,\nassessing language support per NLP task. The presented systematic literature\nreview of Greek NLP serves as an application of our method that showcases the\nbenefits of monolingual NLP surveys more broadly. Similar applications could be\nconsidered for the myriads of languages whose progress in NLP lags behind that\nof well-supported languages.\n","authors":["Juli Bakagianni","Kanella Pouli","Maria Gavriilidou","John Pavlopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.09861v3.pdf","comment":"77 pages"},{"id":"http://arxiv.org/abs/2501.19264v1","updated":"2025-01-31T16:24:46Z","published":"2025-01-31T16:24:46Z","title":"mFollowIR: a Multilingual Benchmark for Instruction Following in\n  Retrieval","summary":"  Retrieval systems generally focus on web-style queries that are short and\nunderspecified. However, advances in language models have facilitated the\nnascent rise of retrieval models that can understand more complex queries with\ndiverse intents. However, these efforts have focused exclusively on English;\ntherefore, we do not yet understand how they work across languages. We\nintroduce mFollowIR, a multilingual benchmark for measuring\ninstruction-following ability in retrieval models. mFollowIR builds upon the\nTREC NeuCLIR narratives (or instructions) that span three diverse languages\n(Russian, Chinese, Persian) giving both query and instruction to the retrieval\nmodels. We make small changes to the narratives and isolate how well retrieval\nmodels can follow these nuanced changes. We present results for both\nmultilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong\ncross-lingual performance with English-based retrievers that trained using\ninstructions, but find a notable drop in performance in the multilingual\nsetting, indicating that more work is needed in developing data for\ninstruction-based multilingual retrievers.\n","authors":["Orion Weller","Benjamin Chang","Eugene Yang","Mahsa Yarmohammadi","Sam Barham","Sean MacAvaney","Arman Cohan","Luca Soldaini","Benjamin Van Durme","Dawn Lawrie"],"pdf_url":"https://arxiv.org/pdf/2501.19264v1.pdf","comment":"Accepted to ECIR 2025"},{"id":"http://arxiv.org/abs/2501.19258v1","updated":"2025-01-31T16:16:52Z","published":"2025-01-31T16:16:52Z","title":"VisualSpeech: Enhance Prosody with Visual Context in TTS","summary":"  Text-to-Speech (TTS) synthesis faces the inherent challenge of producing\nmultiple speech outputs with varying prosody from a single text input. While\nprevious research has addressed this by predicting prosodic information from\nboth text and speech, additional contextual information, such as visual\nfeatures, remains underutilized. This paper investigates the potential of\nintegrating visual context to enhance prosody prediction. We propose a novel\nmodel, VisualSpeech, which incorporates both visual and textual information for\nimproved prosody generation. Empirical results demonstrate that visual features\nprovide valuable prosodic cues beyond the textual input, significantly\nenhancing the naturalness and accuracy of the synthesized speech. Audio samples\nare available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.\n","authors":["Shumin Que","Anton Ragni"],"pdf_url":"https://arxiv.org/pdf/2501.19258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06833v3","updated":"2025-01-31T16:06:52Z","published":"2024-03-11T15:48:56Z","title":"Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?","summary":"  Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.\n","authors":["Egor Zverev","Sahar Abdelnabi","Soroush Tabesh","Mario Fritz","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2403.06833v3.pdf","comment":"Published as a conference paper at ICLR 2025, GitHub:\n  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 10 pages main\n  text, 30 pages in total"},{"id":"http://arxiv.org/abs/2410.06981v2","updated":"2025-01-31T15:27:10Z","published":"2024-10-09T15:18:57Z","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","summary":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.\n","authors":["Michael Lan","Philip Torr","Austin Meek","Ashkan Khakzar","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.06981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19202v1","updated":"2025-01-31T15:12:20Z","published":"2025-01-31T15:12:20Z","title":"Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning","summary":"  Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.\n","authors":["Dang Huu-Tien","Hoang Thanh-Tung","Le-Minh Nguyen","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2501.19202v1.pdf","comment":"12 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2501.19201v1","updated":"2025-01-31T15:10:29Z","published":"2025-01-31T15:10:29Z","title":"Efficient Reasoning with Hidden Thinking","summary":"  Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.\n","authors":["Xuan Shen","Yizhou Wang","Xiangxi Shi","Yanzhi Wang","Pu Zhao","Jiuxiang Gu"],"pdf_url":"https://arxiv.org/pdf/2501.19201v1.pdf","comment":"Preprint version"},{"id":"http://arxiv.org/abs/2401.01879v2","updated":"2025-01-31T15:10:21Z","published":"2024-01-03T18:39:13Z","title":"Theoretical guarantees on the best-of-n alignment policy","summary":"  A simple and effective method for the inference-time alignment of generative\nmodels is the best-of-$n$ policy, where $n$ samples are drawn from a reference\npolicy, ranked based on a reward function, and the highest ranking one is\nselected. A commonly used analytical expression in the literature claims that\nthe KL divergence between the best-of-$n$ policy and the reference policy is\nequal to $\\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show\nthat it is an upper bound on the actual KL divergence. We also explore the\ntightness of this upper bound in different regimes, and propose a new estimator\nfor the KL divergence and empirically show that it provides a tight\napproximation. We also show that the win rate of the best-of-$n$ policy against\nthe reference policy is upper bounded by $n/(n+1)$ and derive bounds on the\ntightness of this characterization. We conclude with analyzing the tradeoffs\nbetween win rate and KL divergence of the best-of-$n$ alignment policy, which\ndemonstrate that very good tradeoffs are achievable with $n < 1000$.\n","authors":["Ahmad Beirami","Alekh Agarwal","Jonathan Berant","Alexander D'Amour","Jacob Eisenstein","Chirag Nagpal","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2401.01879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17284v4","updated":"2025-01-31T15:04:34Z","published":"2024-11-26T10:13:39Z","title":"AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling","summary":"  Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.\n","authors":["Alexander Capstick","Rahul G. Krishnan","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2411.17284v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12109v2","updated":"2025-01-31T14:30:07Z","published":"2024-08-22T03:49:18Z","title":"RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual\n  Preference Data","summary":"  Large vision-language models (LVLMs) often fail to align with human\npreferences, leading to issues like generating misleading content without\nproper visual context (also known as hallucination). A promising solution to\nthis problem is using human-preference alignment techniques, such as best-of-n\nsampling and reinforcement learning. However, these techniques face the\ndifficulty arising from the scarcity of visual preference data, which is\nrequired to train a visual reward model (VRM). In this work, we continue the\nline of research. We present a Robust Visual Reward Model (RoVRM) which\nimproves human-preference alignment for LVLMs. RoVRM leverages auxiliary\ntextual preference data through a three-phase progressive training and optimal\ntransport-based preference data selection to effectively mitigate the scarcity\nof visual preference data. We experiment with RoVRM on the commonly used\nvision-language tasks based on the LLaVA-1.5-7B and -13B models. Experimental\nresults demonstrate that RoVRM consistently outperforms traditional VRMs.\nFurthermore, our three-phase progressive training and preference data selection\napproaches can yield consistent performance gains over ranking-based alignment\ntechniques, such as direct preference optimization.\n","authors":["Chenglong Wang","Yang Gan","Yifu Huo","Yongyu Mu","Murun Yang","Qiaozhi He","Tong Xiao","Chunliang Zhang","Tongran Liu","Quan Du","Di Yang","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.12109v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.16085v3","updated":"2025-01-31T13:56:58Z","published":"2024-11-25T04:36:01Z","title":"Cautious Optimizers: Improving Training with One Line of Code","summary":"  AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim.\n","authors":["Kaizhao Liang","Lizhang Chen","Bo Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17771v2","updated":"2025-01-31T13:51:38Z","published":"2024-07-25T04:58:08Z","title":"Banyan: Improved Representation Learning with Explicit Structure","summary":"  We present Banyan, a model that efficiently learns semantic representations\nby leveraging explicit hierarchical structure. While transformers excel at\nscale, they struggle in low-resource settings. Conversely recent structured\nmodels have shown promise as efficient learners, but lack performance. Banyan\nbridges this gap with two key innovations: an entangled hierarchical tree\nstructure and diagonalized message passing, enabling it to outperform larger\ntransformer models with just 14 non-embedding parameters. It excels in\nlow-resource settings, offering a viable alternative for under-represented\nlanguages and highlighting its potential for efficient, interpretable NLP in\nresource-constrained environments.\n","authors":["Mattia Opper","N. Siddharth"],"pdf_url":"https://arxiv.org/pdf/2407.17771v2.pdf","comment":"v2"},{"id":"http://arxiv.org/abs/2501.19134v1","updated":"2025-01-31T13:44:46Z","published":"2025-01-31T13:44:46Z","title":"Mixed Feelings: Cross-Domain Sentiment Classification of Patient\n  Feedback","summary":"  Sentiment analysis of patient feedback from the public health domain can aid\ndecision makers in evaluating the provided services. The current paper focuses\non free-text comments in patient surveys about general practitioners and\npsychiatric healthcare, annotated with four sentence-level polarity classes --\npositive, negative, mixed and neutral -- while also attempting to alleviate\ndata scarcity by leveraging general-domain sources in the form of reviews. For\nseveral different architectures, we compare in-domain and out-of-domain\neffects, as well as the effects of training joint multi-domain models.\n","authors":["Egil Rønningstad","Lilja Charlotte Storset","Petter Mæhlum","Lilja Øvrelid","Erik Velldal"],"pdf_url":"https://arxiv.org/pdf/2501.19134v1.pdf","comment":"Accepted for NoDaLiDa / Baltic-HLT 2025"},{"id":"http://arxiv.org/abs/2407.09197v3","updated":"2025-01-31T13:11:47Z","published":"2024-07-12T11:53:40Z","title":"A Chatbot for Asylum-Seeking Migrants in Europe","summary":"  We present ACME: A Chatbot for asylum-seeking Migrants in Europe. ACME relies\non computational argumentation and aims to help migrants identify the highest\nlevel of protection they can apply for. This would contribute to a more\nsustainable migration by reducing the load on territorial commissions, Courts,\nand humanitarian organizations supporting asylum applicants. We describe the\nbackground context, system architecture, underlying technologies, and a case\nstudy used to validate the tool with domain experts.\n","authors":["Bettina Fazzinga","Elena Palmieri","Margherita Vestoso","Luca Bolognini","Andrea Galassi","Filippo Furfaro","Paolo Torroni"],"pdf_url":"https://arxiv.org/pdf/2407.09197v3.pdf","comment":"Copyright 2024 IEEE"},{"id":"http://arxiv.org/abs/2407.12703v5","updated":"2025-01-31T12:56:16Z","published":"2024-07-17T16:25:37Z","title":"Subgraph-Aware Training of Language Models for Knowledge Graph\n  Completion Using Structure-Aware Contrastive Learning","summary":"  Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nfocus solely on encoding textual information, neglecting the long-tailed nature\nof knowledge graphs and their various topological structures, e.g., subgraphs,\nshortest paths, and degrees. We claim that this is a major obstacle to\nachieving higher accuracy of PLMs for KGC. To this end, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) with two ideas: (i)\nsubgraph-aware mini-batching to encourage hard negative sampling and to\nmitigate an imbalance in the frequency of entity occurrences during training,\nand (ii) new contrastive learning to focus more on harder in-batch negative\ntriples and harder positive triples in terms of the structural properties of\nthe knowledge graph. To the best of our knowledge, this is the first study to\ncomprehensively incorporate the structural inductive bias of the knowledge\ngraph into fine-tuning PLMs. Extensive experiments on three KGC benchmarks\ndemonstrate the superiority of SATKGC. Our code is available.\n","authors":["Youmin Ko","Hyemin Yang","Taeuk Kim","Hyunjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.12703v5.pdf","comment":"Accepted to The Web Conference 2025"},{"id":"http://arxiv.org/abs/2501.19093v1","updated":"2025-01-31T12:39:28Z","published":"2025-01-31T12:39:28Z","title":"Improving Low-Resource Sequence Labeling with Knowledge Fusion and\n  Contextual Label Explanations","summary":"  Sequence labeling remains a significant challenge in low-resource,\ndomain-specific scenarios, particularly for character-dense languages like\nChinese. Existing methods primarily focus on enhancing model comprehension and\nimproving data diversity to boost performance. However, these approaches still\nstruggle with inadequate model applicability and semantic distribution biases\nin domain-specific contexts. To overcome these limitations, we propose a novel\nframework that combines an LLM-based knowledge enhancement workflow with a\nspan-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.\nOur workflow employs explanation prompts to generate precise contextual\ninterpretations of target entities, effectively mitigating semantic biases and\nenriching the model's contextual understanding. The KnowFREE model further\nintegrates extension label features, enabling efficient nested entity\nextraction without relying on external knowledge during inference. Experiments\non multiple Chinese domain-specific sequence labeling datasets demonstrate that\nour approach achieves state-of-the-art performance, effectively addressing the\nchallenges posed by low-resource settings.\n","authors":["Peichao Lai","Jiaxin Gan","Feiyang Ye","Yilei Wang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2501.19093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18988v5","updated":"2025-01-31T12:28:44Z","published":"2024-04-29T17:36:58Z","title":"Markovian Transformers for Informative Language Modeling","summary":"  Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language\nmodel's underlying decision process. We address this by making CoT text\ncausally essential in a \"Markovian\" language model, factoring next-token\nprediction through an intermediate CoT and training it to predict future tokens\nindependently of the original prompt. We formalize this via an\n\"informativeness\" objective that quantifies how much a trained CoT improves\nnext-token predictions over a baseline. Using policy gradient, we show that\nLlama 3.1 8B achieves a 33.2% absolute accuracy improvement on GSM8K.\nPerturbation tests confirm stronger reliance on the CoT, while cross-model\ntransfers indicate these reasoning traces generalize across interpreters. Our\napproach enhances both accuracy and interpretability, potentially extending CoT\nreasoning to arbitrarily long contexts and diverse tasks.\n","authors":["Scott Viteri","Max Lamparth","Peter Chatain","Clark Barrett"],"pdf_url":"https://arxiv.org/pdf/2404.18988v5.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.03340v2","updated":"2025-01-31T12:15:57Z","published":"2024-05-20T13:09:32Z","title":"A Multi-Modal Explainability Approach for Human-Aware Robots in\n  Multi-Party Conversation","summary":"  The addressee estimation (understanding to whom somebody is talking) is a\nfundamental task for human activity recognition in multi-party conversation\nscenarios. Specifically, in the field of human-robot interaction, it becomes\neven more crucial to enable social robots to participate in such interactive\ncontexts. However, it is usually implemented as a binary classification task,\nrestricting the robot's capability to estimate whether it was addressed\n\\review{or not, which} limits its interactive skills. For a social robot to\ngain the trust of humans, it is also important to manifest a certain level of\ntransparency and explainability. Explainable artificial intelligence thus plays\na significant role in the current machine learning applications and models, to\nprovide explanations for their decisions besides excellent performance. In our\nwork, we a) present an addressee estimation model with improved performance in\ncomparison with the previous state-of-the-art; b) further modify this model to\ninclude inherently explainable attention-based segments; c) implement the\nexplainable addressee estimation as part of a modular cognitive architecture\nfor multi-party conversation in an iCub robot; d) validate the real-time\nperformance of the explainable model in multi-party human-robot interaction; e)\npropose several ways to incorporate explainability and transparency in the\naforementioned architecture; and f) perform an online user study to analyze the\neffect of various explanations on how human participants perceive the robot.\n","authors":["Iveta Bečková","Štefan Pócoš","Giulia Belgiovine","Marco Matarese","Omar Eldardeer","Alessandra Sciutti","Carlo Mazzola"],"pdf_url":"https://arxiv.org/pdf/2407.03340v2.pdf","comment":"32pp (+6pp sup.mat.) Accepted in Computer Vision and Image\n  Understanding Journal on January 23, 2025. This research received funding\n  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and\n  Development Agency, project no. APVV-21-0105"},{"id":"http://arxiv.org/abs/2410.02558v2","updated":"2025-01-31T11:47:07Z","published":"2024-10-03T15:04:00Z","title":"Improving Unsupervised Constituency Parsing via Maximizing Semantic\n  Information","summary":"  Unsupervised constituency parsers organize phrases within a sentence into a\ntree-shaped syntactic constituent structure that reflects the organization of\nsentence semantics. However, the traditional objective of maximizing sentence\nlog-likelihood (LL) does not explicitly account for the close relationship\nbetween the constituent structure and the semantics, resulting in a weak\ncorrelation between LL values and parsing accuracy. In this paper, we introduce\na novel objective for training unsupervised parsers: maximizing the information\nbetween constituent structures and sentence semantics (SemInfo). We introduce a\nbag-of-substrings model to represent the semantics and apply the\nprobability-weighted information metric to estimate the SemInfo. Additionally,\nwe develop a Tree Conditional Random Field (TreeCRF)-based model to apply the\nSemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)\ninduction, the state-of-the-art method for unsupervised constituency parsing.\nExperiments demonstrate that SemInfo correlates more strongly with parsing\naccuracy than LL. Our algorithm significantly enhances parsing accuracy by an\naverage of 7.85 points across five PCFG variants and in four languages,\nachieving new state-of-the-art results in three of the four languages.\n","authors":["Junjie Chen","Xiangheng He","Yusuke Miyao","Danushka Bollegala"],"pdf_url":"https://arxiv.org/pdf/2410.02558v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2501.19056v1","updated":"2025-01-31T11:32:05Z","published":"2025-01-31T11:32:05Z","title":"Enabling Autonomic Microservice Management through Self-Learning Agents","summary":"  The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement.\n","authors":["Fenglin Yu","Fangkai Yang","Xiaoting Qin","Zhiyang Zhang","Jue Zhang","Qingwei Lin","Hongyu Zhang","Yingnong Dang","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.19056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14487v2","updated":"2025-01-31T11:16:51Z","published":"2024-07-19T17:41:08Z","title":"Evaluating the Reliability of Self-Explanations in Large Language Models","summary":"  This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.\n","authors":["Korbinian Randl","John Pavlopoulos","Aron Henriksson","Tony Lindgren"],"pdf_url":"https://arxiv.org/pdf/2407.14487v2.pdf","comment":"Non peer-reviewed preprint. Presented at Discovery Science 2024.\n  Peer-reviewed version published in the Springer Lecture Notes in Computer\n  Science (vol 15243)"},{"id":"http://arxiv.org/abs/2412.04235v2","updated":"2025-01-31T11:14:25Z","published":"2024-12-05T15:11:12Z","title":"Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM\n  Chatbots","summary":"  I combine detection and mitigation techniques to addresses hallucinations in\nLarge Language Models (LLMs). Mitigation is achieved in a question-answering\nRetrieval-Augmented Generation (RAG) framework while detection is obtained by\nintroducing the Negative Missing Information Scoring System (NMISS), which\naccounts for contextual relevance in responses. While RAG mitigates\nhallucinations by grounding answers in external data, NMISS refines the\nevaluation by identifying cases where traditional metrics incorrectly flag\ncontextually accurate responses as hallucinations. I use Italian health news\narticles as context to evaluate LLM performance. Results show that Gemma2 and\nGPT-4 outperform the other models, with GPT-4 producing answers closely aligned\nwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral\nbenefit significantly from NMISS, highlighting their ability to provide richer\ncontextual information. This combined approach offers new insights into the\nreduction and more accurate assessment of hallucinations in LLMs, with\napplications in real-world healthcare tasks and other domains.\n","authors":["Maria Paola Priola"],"pdf_url":"https://arxiv.org/pdf/2412.04235v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19022v1","updated":"2025-01-31T10:45:24Z","published":"2025-01-31T10:45:24Z","title":"On the Impact of Noise in Differentially Private Text Rewriting","summary":"  The field of text privatization often leverages the notion of\n$\\textit{Differential Privacy}$ (DP) to provide formal guarantees in the\nrewriting or obfuscation of sensitive textual data. A common and nearly\nubiquitous form of DP application necessitates the addition of calibrated noise\nto vector representations of text, either at the data- or model-level, which is\ngoverned by the privacy parameter $\\varepsilon$. However, noise addition almost\nundoubtedly leads to considerable utility loss, thereby highlighting one major\ndrawback of DP in NLP. In this work, we introduce a new sentence infilling\nprivatization technique, and we use this method to explore the effect of noise\nin DP text rewriting. We empirically demonstrate that non-DP privatization\ntechniques excel in utility preservation and can find an acceptable empirical\nprivacy-utility trade-off, yet cannot outperform DP methods in empirical\nprivacy protections. Our results highlight the significant impact of noise in\ncurrent DP rewriting mechanisms, leading to a discussion of the merits and\nchallenges of DP in NLP, as well as the opportunities that non-DP methods\npresent.\n","authors":["Stephen Meisenbacher","Maulik Chevli","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2501.19022v1.pdf","comment":"19 pages, 3 figures, 9 tables. Accepted to NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2501.16813v2","updated":"2025-01-31T10:39:11Z","published":"2025-01-28T09:30:29Z","title":"Multimodal Magic Elevating Depression Detection with a Fusion of Text\n  and Audio Intelligence","summary":"  This study proposes an innovative multimodal fusion model based on a\nteacher-student architecture to enhance the accuracy of depression\nclassification. Our designed model addresses the limitations of traditional\nmethods in feature fusion and modality weight allocation by introducing\nmulti-head attention mechanisms and weighted multimodal transfer learning.\nLeveraging the DAIC-WOZ dataset, the student fusion model, guided by textual\nand auditory teacher models, achieves significant improvements in\nclassification accuracy. Ablation experiments demonstrate that the proposed\nmodel attains an F1 score of 99. 1% on the test set, significantly\noutperforming unimodal and conventional approaches. Our method effectively\ncaptures the complementarity between textual and audio features while\ndynamically adjusting the contributions of the teacher models to enhance\ngeneralization capabilities. The experimental results highlight the robustness\nand adaptability of the proposed framework in handling complex multimodal data.\nThis research provides a novel technical framework for multimodal large model\nlearning in depression analysis, offering new insights into addressing the\nlimitations of existing methods in modality fusion and feature extraction.\n","authors":["Lindy Gan","Yifan Huang","Xiaoyang Gao","Jiaming Tan","Fujun Zhao","Tao Yang"],"pdf_url":"https://arxiv.org/pdf/2501.16813v2.pdf","comment":"21 pages,7 figures.1 table"},{"id":"http://arxiv.org/abs/2501.19018v1","updated":"2025-01-31T10:39:04Z","published":"2025-01-31T10:39:04Z","title":"Scalable Multi-phase Word Embedding Using Conjunctive Propositional\n  Clauses","summary":"  The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness\nin Machine Learning (ML), particularly within Natural Language Processing\n(NLP). It has been utilized to construct word embedding using conjunctive\npropositional clauses, thereby significantly enhancing our understanding and\ninterpretation of machine-derived decisions. The previous approach performed\nthe word embedding over a sequence of input words to consolidate the\ninformation into a cohesive and unified representation. However, that approach\nencounters scalability challenges as the input size increases. In this study,\nwe introduce a novel approach incorporating two-phase training to discover\ncontextual embeddings of input sequences. Specifically, this method\nencapsulates the knowledge for each input word within the dataset's vocabulary,\nsubsequently constructing embeddings for a sequence of input words utilizing\nthe extracted knowledge. This technique not only facilitates the design of a\nscalable model but also preserves interpretability. Our experimental findings\nrevealed that the proposed method yields competitive performance compared to\nthe previous approaches, demonstrating promising results in contrast to\nhuman-generated benchmarks. Furthermore, we applied the proposed approach to\nsentiment analysis on the IMDB dataset, where the TM embedding and the TM\nclassifier, along with other interpretable classifiers, offered a transparent\nend-to-end solution with competitive performance.\n","authors":["Ahmed K. Kadhim","Lei Jiao","Rishad Shafik","Ole-Christoffer Granmo","Bimal Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2501.19018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19017v1","updated":"2025-01-31T10:37:48Z","published":"2025-01-31T10:37:48Z","title":"Calling a Spade a Heart: Gaslighting Multimodal Large Language Models\n  via Negation","summary":"  Multimodal Large Language Models (MLLMs) have exhibited remarkable\nadvancements in integrating different modalities, excelling in complex\nunderstanding and generation tasks. Despite their success, MLLMs remain\nvulnerable to conversational adversarial inputs, particularly negation\narguments. This paper systematically evaluates state-of-the-art MLLMs across\ndiverse benchmarks, revealing significant performance drops when negation\narguments are introduced to initially correct responses. We show critical\nvulnerabilities in the reasoning and alignment mechanisms of these models.\nProprietary models such as GPT-4o and Claude-3.5-Sonnet demonstrate better\nresilience compared to open-source counterparts like Qwen2-VL and LLaVA.\nHowever, all evaluated MLLMs struggle to maintain logical consistency under\nnegation arguments during conversation. This paper aims to offer valuable\ninsights for improving the robustness of MLLMs against adversarial inputs,\ncontributing to the development of more reliable and trustworthy multimodal AI\nsystems.\n","authors":["Bin Zhu","Hui yan Qi","Yinxuan Gui","Jingjing Chen","Chong-Wah Ngo","Ee Peng Lim"],"pdf_url":"https://arxiv.org/pdf/2501.19017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19012v1","updated":"2025-01-31T10:26:18Z","published":"2025-01-31T10:26:18Z","title":"Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities","summary":"  Large Language Models (LLMs) have become an essential tool in the\nprogrammer's toolkit, but their tendency to hallucinate code can be used by\nmalicious actors to introduce vulnerabilities to broad swathes of the software\nsupply chain. In this work, we analyze package hallucination behaviour in LLMs\nacross popular programming languages examining both existing package references\nand fictional dependencies. By analyzing this package hallucination behaviour\nwe find potential attacks and suggest defensive strategies to defend against\nthese attacks. We discover that package hallucination rate is predicated not\nonly on model choice, but also programming language, model size, and\nspecificity of the coding task request. The Pareto optimality boundary between\ncode generation performance and package hallucination is sparsely populated,\nsuggesting that coding models are not being optimized for secure code.\nAdditionally, we find an inverse correlation between package hallucination rate\nand the HumanEval coding benchmark, offering a heuristic for evaluating the\npropensity of a model to hallucinate packages. Our metrics, findings and\nanalyses provide a base for future models, securing AI-assisted software\ndevelopment workflows against package supply chain attacks.\n","authors":["Arjun Krishna","Erick Galinkin","Leon Derczynski","Jeffrey Martin"],"pdf_url":"https://arxiv.org/pdf/2501.19012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19010v1","updated":"2025-01-31T10:25:42Z","published":"2025-01-31T10:25:42Z","title":"DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech\n  Recognition","summary":"  Dysarthric speech recognition often suffers from performance degradation due\nto the intrinsic diversity of dysarthric severity and extrinsic disparity from\nnormal speech. To bridge these gaps, we propose a Dynamic Phoneme-level\nContrastive Learning (DyPCL) method, which leads to obtaining invariant\nrepresentations across diverse speakers. We decompose the speech utterance into\nphoneme segments for phoneme-level contrastive learning, leveraging dynamic\nconnectionist temporal classification alignment. Unlike prior studies focusing\non utterance-level embeddings, our granular learning allows discrimination of\nsubtle parts of speech. In addition, we introduce dynamic curriculum learning,\nwhich progressively transitions from easy negative samples to\ndifficult-to-distinguishable negative samples based on phonetic similarity of\nphoneme. Our approach to training by difficulty levels alleviates the inherent\nvariability of speakers, better identifying challenging speeches. Evaluated on\nthe UASpeech dataset, DyPCL outperforms baseline models, achieving an average\n22.10\\% relative reduction in word error rate (WER) across the overall\ndysarthria group.\n","authors":["Wonjun Lee","Solee Im","Heejin Do","Yunsu Kim","Jungseul Ok","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2501.19010v1.pdf","comment":"NAACL 2025, 9pages, 1 page appendix"},{"id":"http://arxiv.org/abs/2409.18028v3","updated":"2025-01-31T10:15:43Z","published":"2024-09-26T16:34:35Z","title":"Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective","summary":"  A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.\n","authors":["Yotam Wolf","Binyamin Rothberg","Dorin Shteyman","Amnon Shashua"],"pdf_url":"https://arxiv.org/pdf/2409.18028v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11403v2","updated":"2025-01-31T10:08:36Z","published":"2025-01-20T11:06:05Z","title":"Verifying Cross-modal Entity Consistency in News using Vision-language\n  Models","summary":"  The web has become a crucial source of information, but it is also used to\nspread disinformation, often conveyed through multiple modalities like images\nand text. The identification of inconsistent cross-modal information, in\nparticular entities such as persons, locations, and events, is critical to\ndetect disinformation. Previous works either identify out-of-context\ndisinformation by assessing the consistency of images to the whole document,\nneglecting relations of individual entities, or focus on generic entities that\nare not relevant to news. So far, only few approaches have addressed the task\nof validating entity consistency between images and text in news. However, the\npotential of large vision-language models (LVLMs) has not been explored yet. In\nthis paper, we propose an LVLM-based framework for verifying Cross-modal Entity\nConsistency~(LVLM4CEC), to assess whether persons, locations and events in news\narticles are consistent across both modalities. We suggest effective prompting\nstrategies for LVLMs for entity verification that leverage reference images\ncrawled from web. Moreover, we extend three existing datasets for the task of\nentity verification in news providing manual ground-truth data. Our results\nshow the potential of LVLMs for automating cross-modal entity verification,\nshowing improved accuracy in identifying persons and events when using evidence\nimages. Moreover, our method outperforms a baseline for location and event\nverification in documents. The datasets and source code are available on GitHub\nat https://github.com/TIBHannover/LVLM4CEC.\n","authors":["Sahar Tahmasebi","David Ernst","Eric Müller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2501.11403v2.pdf","comment":"Accepted for publication in: European Conference on Information\n  Retrieval (ECIR) 2025"},{"id":"http://arxiv.org/abs/2501.18998v1","updated":"2025-01-31T10:06:27Z","published":"2025-01-31T10:06:27Z","title":"Adversarial Attacks on AI-Generated Text Detection Models: A Token\n  Probability-Based Approach Using Embeddings","summary":"  In recent years, text generation tools utilizing Artificial Intelligence (AI)\nhave occasionally been misused across various domains, such as generating\nstudent reports or creative writings. This issue prompts plagiarism detection\nservices to enhance their capabilities in identifying AI-generated content.\nAdversarial attacks are often used to test the robustness of AI-text generated\ndetectors. This work proposes a novel textual adversarial attack on the\ndetection models such as Fast-DetectGPT. The method employs embedding models\nfor data perturbation, aiming at reconstructing the AI generated texts to\nreduce the likelihood of detection of the true origin of the texts.\nSpecifically, we employ different embedding techniques, including the Tsetlin\nMachine (TM), an interpretable approach in machine learning for this purpose.\nBy combining synonyms and embedding similarity vectors, we demonstrates the\nstate-of-the-art reduction in detection scores against Fast-DetectGPT.\nParticularly, in the XSum dataset, the detection score decreased from 0.4431 to\n0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.\n","authors":["Ahmed K. Kadhim","Lei Jiao","Rishad Shafik","Ole-Christoffer Granmo"],"pdf_url":"https://arxiv.org/pdf/2501.18998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20463v2","updated":"2025-01-31T09:29:23Z","published":"2024-10-27T14:43:23Z","title":"A Derivational ChainBank for Modern Standard Arabic","summary":"  We introduce the new concept of an Arabic Derivational Chain Bank CHAINBANK\nto leverage the relationship between form and meaning in modeling Arabic\nderivational morphology. We constructed a knowledge graph network of abstract\npatterns and their derivational relations and aligned it with the lemmas of the\nCAMELMORPH morphological analyzer database. This process produced chains of\nderived words' lemmas linked to their corresponding lemma bases through\nderivational relations, encompassing 23,333 derivational connections.\n","authors":["Reham Marzouk","Sondos Krouna","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2410.20463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12799v2","updated":"2025-01-31T09:27:26Z","published":"2024-08-23T02:27:14Z","title":"Preference Consistency Matters: Enhancing Preference Learning in\n  Language Models with Automated Self-Curation of Training Corpora","summary":"  Inconsistent annotations in training corpora, particularly within preference\nlearning datasets, pose challenges in developing advanced language models.\nThese inconsistencies often arise from variability among annotators and\ninherent multi-dimensional nature of the preferences. To address these issues,\nwe introduce a self-curation method that preprocesses annotated datasets by\nleveraging proxy models trained directly on them. Our method enhances\npreference learning by automatically detecting and selecting consistent\nannotations. We validate the proposed approach through extensive\ninstruction-following tasks, demonstrating performance improvements of up to\n33\\% across various learning algorithms and proxy capabilities. This work\noffers a straightforward and reliable solution to address preference\ninconsistencies without relying on heuristics, serving as an initial step\ntoward the development of more advanced preference learning methodologies. Code\nis available at https://github.com/Self-Curation/ .\n","authors":["JoonHo Lee","JuYoun Son","Juree Seok","Wooseok Jang","Yeong-Dae Kwon"],"pdf_url":"https://arxiv.org/pdf/2408.12799v2.pdf","comment":"Accepted to NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2405.06424v3","updated":"2025-01-31T09:26:56Z","published":"2024-05-10T12:14:11Z","title":"Improving Instruction Following in Language Models through Proxy-Based\n  Uncertainty Estimation","summary":"  Assessing response quality to instructions in language models is vital but\nchallenging due to the complexity of human language across different contexts.\nThis complexity often results in ambiguous or inconsistent interpretations,\nmaking accurate assessment difficult. To address this issue, we propose a novel\nUncertainty-aware Reward Model (URM) that introduces a robust uncertainty\nestimation for the quality of paired responses based on Bayesian approximation.\nTrained with preference datasets, our uncertainty-enabled proxy not only scores\nrewards for responses but also evaluates their inherent uncertainty. Empirical\nresults demonstrate significant benefits of incorporating the proposed proxy\ninto language model training. Our method boosts the instruction following\ncapability of language models by refining data curation for training and\nimproving policy optimization objectives, thereby surpassing existing methods\nby a large margin on benchmarks such as Vicuna and MT-bench. These findings\nhighlight that our proposed approach substantially advances language model\ntraining and paves a new way of harnessing uncertainty within language models.\n","authors":["JoonHo Lee","Jae Oh Woo","Juree Seok","Parisa Hassanzadeh","Wooseok Jang","JuYoun Son","Sima Didari","Baruch Gutow","Heng Hao","Hankyu Moon","Wenjun Hu","Yeong-Dae Kwon","Taehee Lee","Seungjai Min"],"pdf_url":"https://arxiv.org/pdf/2405.06424v3.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2407.05327v2","updated":"2025-01-31T08:49:58Z","published":"2024-07-07T10:48:04Z","title":"Can Model Uncertainty Function as a Proxy for Multiple-Choice Question\n  Item Difficulty?","summary":"  Estimating the difficulty of multiple-choice questions would be great help\nfor educators who must spend substantial time creating and piloting stimuli for\ntheir tests, and for learners who want to practice. Supervised approaches to\ndifficulty estimation have yielded to date mixed results. In this contribution\nwe leverage an aspect of generative large models which might be seen as a\nweakness when answering questions, namely their uncertainty, and exploit it\ntowards exploring correlations between two different metrics of uncertainty,\nand the actual student response distribution. While we observe some present but\nweak correlations, we also discover that the models' behaviour is different in\nthe case of correct vs wrong answers, and that correlations differ\nsubstantially according to the different question types which are included in\nour fine-grained, previously unused dataset of 451 questions from a\nBiopsychology course. In discussing our findings, we also suggest potential\navenues to further leverage model uncertainty as an additional proxy for item\ndifficulty.\n","authors":["Leonidas Zotos","Hedderik van Rijn","Malvina Nissim"],"pdf_url":"https://arxiv.org/pdf/2407.05327v2.pdf","comment":"13 pages, 7 figures, Published in The 31st International Conference\n  on Computational Linguistics, available in the ACL Anthology:\n  https://aclanthology.org/2025.coling-main.749/"},{"id":"http://arxiv.org/abs/2501.18957v1","updated":"2025-01-31T08:32:32Z","published":"2025-01-31T08:32:32Z","title":"Intrinsic Tensor Field Propagation in Large Language Models: A Novel\n  Approach to Contextual Information Flow","summary":"  Context propagation remains a central challenge in language model\narchitectures, particularly in tasks requiring the retention of long-range\ndependencies. Conventional attention mechanisms, while effective in many\napplications, exhibit limitations in maintaining coherent contextual\nrepresentations over extended sequences due to their reliance on discrete token\ninteractions. A novel approach is introduced through the formulation of\nIntrinsic Tensor Field Propagation (ITFP), which models contextual\nrelationships as continuous tensor fields distributed across token embeddings.\nThe propagation dynamics are governed through differential equations that\nenable a structured flow of contextual information, augmenting the standard\nattention mechanism to enhance coherence and recall. A series of experiments\nconducted on an open-source transformer-based model demonstrate that ITFP\nprovides measurable improvements in contextual retention, dependency\nresolution, and inference stability across various linguistic structures.\nComparisons with baseline models reveal a reduction in syntactic\ninconsistencies and factual errors, while ablation studies indicate that the\nchoice of propagation depth and integration strength significantly impacts\nmodel performance. Additional evaluations assessing domain generalization\nsuggest that ITFP effectively adapts across different text genres, reinforcing\nits applicability beyond conventional language modeling tasks. Although\ncomputational trade-offs are introduced through the inclusion of tensor field\ncomputations, empirical findings suggest that the benefits in accuracy and\ncoherence outweigh the increased processing demands.\n","authors":["Alfred Bexley","Lukas Radcliffe","Giles Weatherstone","Joseph Sakau"],"pdf_url":"https://arxiv.org/pdf/2501.18957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09447v3","updated":"2025-01-31T08:22:46Z","published":"2024-07-12T17:33:34Z","title":"ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to\n  Identify Low-Perplexity Toxic Prompts","summary":"  Conventional approaches for the automated red-teaming of large language\nmodels (LLMs) aim to identify prompts that elicit toxic outputs from a frozen\nlanguage model (the defender). This often results in the prompting model (the\nadversary) producing text that is unlikely to arise during autoregression. In\nresponse, we propose a reinforcement learning formulation of LLM red-teaming\ndesigned to discover prompts that both (1) elicit toxic outputs from a defender\nand (2) have low perplexity as scored by that defender. These prompts are the\nmost pertinent in a red-teaming setting because the defender generates them\nwith high probability. We solve this formulation with an online and weakly\nsupervised form of Identity Preference Optimization (IPO), attacking models\nranging from 137M to 7.8B parameters. Our policy performs competitively,\nproducing prompts that induce defender toxicity at a rate of 2-23 times higher\nthan baseline across model scales. Importantly, these prompts have lower\nperplexity than both automatically generated and human-written attacks.\nFurthermore, our method creates black-box attacks with 5.4-14 times increased\ntoxicity. To assess the downstream utility of our method, we use rollouts from\nour policy as negative examples for downstream toxicity tuning and demonstrate\nimproved safety.\n","authors":["Amelia F. Hardy","Houjun Liu","Bernard Lange","Duncan Eddy","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2407.09447v3.pdf","comment":"10 pages, 7 pages of appendix, 3 tables, 3 figures"},{"id":"http://arxiv.org/abs/2409.00358v2","updated":"2025-01-31T07:32:54Z","published":"2024-08-31T05:53:39Z","title":"Predicting the Target Word of Game-playing Conversations using a\n  Low-Rank Dialect Adapter for Decoder Models","summary":"  Dialect adapters that improve the performance of LLMs for NLU tasks on\ncertain sociolects/dialects/national varieties ('dialects' for the sake of\nbrevity) have been reported for encoder models. In this paper, we extend the\nidea of dialect adapters to decoder models in our architecture called LoRDD.\nUsing MD-3, a publicly available dataset of word game-playing conversations\nbetween dialectal speakers, our task is Target Word Prediction (TWP) from a\nmasked conversation. LoRDD combines task adapters and dialect adapters where\nthe latter employ contrastive learning on pseudo-parallel conversations from\nMD-3. Our experiments on Indian English and Nigerian English conversations with\ntwo models (Mistral and Gemma) demonstrate that LoRDD outperforms four\nbaselines on TWP. Additionally, it significantly reduces the performance gap\nwith American English, narrowing it to 12% and 5.8% for word similarity, and\n25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is\nin its promise for dialect adaptation of decoder models using TWP, a simplified\nversion of the commonly used next-word prediction task.\n","authors":["Dipankar Srirag","Aditya Joshi","Jacob Eisenstein"],"pdf_url":"https://arxiv.org/pdf/2409.00358v2.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2501.02448v2","updated":"2025-01-31T07:32:07Z","published":"2025-01-05T05:57:22Z","title":"Understand, Solve and Translate: Bridging the Multilingual Mathematical\n  Reasoning Gap","summary":"  Large language models (LLMs) demonstrate exceptional performance on complex\nreasoning tasks. However, despite their strong reasoning capabilities in\nhigh-resource languages (e.g., English and Chinese), a significant performance\ngap persists in other languages. To investigate this gap in Korean, we\nintroduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual\nmath problems. Through systematic analysis of model behaviors, we identify a\nkey finding: these performance disparities stem primarily from difficulties in\ncomprehending non-English inputs, rather than limitations in reasoning\ncapabilities. Based on these findings, we propose UST (Understand, Solve, and\nTranslate), a method that strategically uses English as an anchor for reasoning\nand solution generation. By fine-tuning the model on 130k synthetically\ngenerated data points, UST achieves a 10.91% improvement on the HRM8K benchmark\nand reduces the multilingual performance gap from 11.6% to 0.7%. Additionally,\nwe show that improvements from UST generalize effectively to different Korean\ndomains, demonstrating that capabilities acquired from machine-verifiable\ncontent can be generalized to other areas. We publicly release the benchmark,\ntraining dataset, and models.\n","authors":["Hyunwoo Ko","Guijin Son","Dasol Choi"],"pdf_url":"https://arxiv.org/pdf/2501.02448v2.pdf","comment":"18 pages, 14 figures, 9 tables"},{"id":"http://arxiv.org/abs/2501.18924v1","updated":"2025-01-31T07:10:40Z","published":"2025-01-31T07:10:40Z","title":"Language Games as the Pathway to Artificial Superhuman Intelligence","summary":"  The evolution of large language models (LLMs) toward artificial superhuman\nintelligence (ASI) hinges on data reproduction, a cyclical process in which\nmodels generate, curate and retrain on novel data to refine capabilities.\nCurrent methods, however, risk getting stuck in a data reproduction trap:\noptimizing outputs within fixed human-generated distributions in a closed loop\nleads to stagnation, as models merely recombine existing knowledge rather than\nexplore new frontiers. In this paper, we propose language games as a pathway to\nexpanded data reproduction, breaking this cycle through three mechanisms: (1)\n\\textit{role fluidity}, which enhances data diversity and coverage by enabling\nmulti-agent systems to dynamically shift roles across tasks; (2) \\textit{reward\nvariety}, embedding multiple feedback criteria that can drive complex\nintelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving\ninteraction constraints to foster learnability, thereby injecting continual\nnovelty. By scaling language games into global sociotechnical ecosystems,\nhuman-AI co-evolution generates unbounded data streams that drive open-ended\nexploration. This framework redefines data reproduction not as a closed loop\nbut as an engine for superhuman intelligence.\n","authors":["Ying Wen","Ziyu Wan","Shao Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18924v1.pdf","comment":"This position paper argues that language games provide robust\n  mechanism for achieving superhuman intelligence in large language models"},{"id":"http://arxiv.org/abs/2501.18922v1","updated":"2025-01-31T06:59:49Z","published":"2025-01-31T06:59:49Z","title":"KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree\n  Search","summary":"  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo.\n","authors":["Haoran Luo","Haihong E","Yikai Guo","Qika Lin","Xiaobao Wu","Xinyu Mu","Wenhao Liu","Meina Song","Yifan Zhu","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2501.18922v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.15625v2","updated":"2025-01-31T06:36:22Z","published":"2024-10-21T04:08:37Z","title":"Improving Parallel Program Performance with LLM Optimizers via\n  Agent-System Interface","summary":"  Modern scientific discovery increasingly relies on high-performance computing\nfor complex modeling and simulation. A key challenge in improving parallel\nprogram performance is efficiently mapping tasks to processors and data to\nmemory, a process dictated by intricate, low-level system code known as\nmappers. Developing high-performance mappers demands days of manual tuning,\nposing a significant barrier for domain scientists without systems expertise.\nWe introduce a framework that automates mapper development with generative\noptimization, leveraging richer feedback beyond scalar performance metrics. Our\napproach features the Agent-System Interface, which includes a Domain-Specific\nLanguage (DSL) to abstract away low-level complexity of system code and define\na structured search space, as well as AutoGuide, a mechanism that interprets\nraw execution output into actionable feedback. Unlike traditional reinforcement\nlearning methods such as OpenTuner, which rely solely on scalar feedback, our\nmethod finds superior mappers in far fewer iterations. With just 10 iterations,\nit outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster\nperformance. Our approach finds mappers that surpass expert-written mappers by\nup to 1.34X speedup across nine benchmarks while reducing tuning time from days\nto minutes.\n","authors":["Anjiang Wei","Allen Nie","Thiago S. F. X. Teixeira","Rohan Yadav","Wonchan Lee","Ke Wang","Alex Aiken"],"pdf_url":"https://arxiv.org/pdf/2410.15625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08458v2","updated":"2025-01-31T06:27:00Z","published":"2024-10-11T02:19:11Z","title":"Simultaneous Reward Distillation and Preference Learning: Get You a\n  Language Model Who Can Do Both","summary":"  Traditional RLHF-based LLM alignment methods explicitly maximize the expected\nrewards from a separate reward model. More recent supervised alignment methods\nlike Direct Preference Optimization (DPO) circumvent this phase to avoid\nproblems including model drift and reward overfitting. Although popular due to\nits simplicity, DPO and similar direct alignment methods which rely heavily on\nthe Bradley-Terry-based pairwise preference formulation can still lead to\ndegenerate policies when challenged by non-deterministic or noisy preference\nlabels, for example human scoring of two candidate outputs with low confidence.\nThis paper introduces DRDO (Direct Reward Distillation and\npolicy-Optimization), which simultaneously models rewards and preferences to\navoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while\nlearning human preferences with a novel preference likelihood formulation.\nResults on the Ultrafeedback and TL;DR datasets demonstrate that DRDO-trained\npolicies surpass methods such as DPO and e-DPO in terms of expected rewards and\nare more robust, on average, to noisy preference signals as well as\nout-of-distribution (OOD) settings.\n","authors":["Abhijnan Nath","Changsoo Jung","Ethan Seefried","Nikhil Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2410.08458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18895v1","updated":"2025-01-31T05:23:03Z","published":"2025-01-31T05:23:03Z","title":"Efficient Supernet Training with Orthogonal Softmax for Scalable ASR\n  Model Compression","summary":"  ASR systems are deployed across diverse environments, each with specific\nhardware constraints. We use supernet training to jointly train multiple\nencoders of varying sizes, enabling dynamic model size adjustment to fit\nhardware constraints without redundant training. Moreover, we introduce a novel\nmethod called OrthoSoftmax, which applies multiple orthogonal softmax functions\nto efficiently identify optimal subnets within the supernet, avoiding\nresource-intensive search. This approach also enables more flexible and precise\nsubnet selection by allowing selection based on various criteria and levels of\ngranularity. Our results with CTC on Librispeech and TED-LIUM-v2 show that\nFLOPs-aware component-wise selection achieves the best overall performance.\nWith the same number of training updates from one single job, WERs for all\nmodel sizes are comparable to or slightly better than those of individually\ntrained models. Furthermore, we analyze patterns in the selected components and\nreveal interesting insights.\n","authors":["Jingjing Xu","Eugen Beck","Zijian Yang","Ralf Schlüter"],"pdf_url":"https://arxiv.org/pdf/2501.18895v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2407.11384v2","updated":"2025-01-31T04:31:59Z","published":"2024-07-16T04:55:17Z","title":"InvAgent: A Large Language Model based Multi-Agent System for Inventory\n  Management in Supply Chains","summary":"  Supply chain management (SCM) involves coordinating the flow of goods,\ninformation, and finances across various entities to deliver products\nefficiently. Effective inventory management is crucial in today's volatile and\nuncertain world. Previous research has demonstrated the superiority of\nheuristic methods and reinforcement learning applications in inventory\nmanagement. However, the application of large language models (LLMs) as\nautonomous agents in multi-agent systems for inventory management remains\nunderexplored. This study introduces a novel approach using LLMs to manage\nmulti-agent inventory systems. Leveraging their zero-shot learning\ncapabilities, our model, InvAgent, enhances resilience and improves efficiency\nacross the supply chain network. Our contributions include utilizing LLMs for\nzero-shot learning to enable adaptive and informed decision-making without\nprior training, providing explainability and clarity through chain-of-thought,\nand demonstrating dynamic adaptability to varying demand scenarios while\nreducing costs and preventing stockouts. Extensive evaluations across different\nscenarios highlight the efficiency of our model in SCM.\n","authors":["Yinzhu Quan","Zefang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.11384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13397v3","updated":"2025-01-31T04:23:33Z","published":"2025-01-23T05:46:50Z","title":"ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models","summary":"  Masked Language Models (MLMs) have achieved remarkable success in many\nself-supervised representation learning tasks. MLMs are trained by randomly\nmasking portions of the input sequences with [MASK] tokens and learning to\nreconstruct the original content based on the remaining context. This paper\nexplores the impact of [MASK] tokens on MLMs. Analytical studies show that\nmasking tokens can introduce the corrupted semantics problem, wherein the\ncorrupted context may convey multiple, ambiguous meanings. This problem is also\na key factor affecting the performance of MLMs on downstream tasks. Based on\nthese findings, we propose a novel enhanced-context MLM, ExLM. Our approach\nexpands [MASK] tokens in the input context and models the dependencies between\nthese expanded states. This enhancement increases context capacity and enables\nthe model to capture richer semantic information, effectively mitigating the\ncorrupted semantics problem during pre-training. Experimental results\ndemonstrate that ExLM achieves significant performance improvements in both\ntext modeling and SMILES modeling tasks. Further analysis confirms that ExLM\nenriches semantic representations through context enhancement, and effectively\nreduces the semantic multimodality commonly observed in MLMs.\n","authors":["Kangjie Zheng","Junwei Yang","Siyue Liang","Bin Feng","Zequn Liu","Wei Ju","Zhiping Xiao","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.13397v3.pdf","comment":"30 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.15264v3","updated":"2025-01-31T03:15:34Z","published":"2024-12-17T02:07:33Z","title":"ReXTrust: A Model for Fine-Grained Hallucination Detection in\n  AI-Generated Radiology Reports","summary":"  The increasing adoption of AI-generated radiology reports necessitates robust\nmethods for detecting hallucinations--false or unfounded statements that could\nimpact patient care. We present ReXTrust, a novel framework for fine-grained\nhallucination detection in AI-generated radiology reports. Our approach\nleverages sequences of hidden states from large vision-language models to\nproduce finding-level hallucination risk scores. We evaluate ReXTrust on a\nsubset of the MIMIC-CXR dataset and demonstrate superior performance compared\nto existing approaches, achieving an AUROC of 0.8751 across all findings and\n0.8963 on clinically significant findings. Our results show that white-box\napproaches leveraging model hidden states can provide reliable hallucination\ndetection for medical AI systems, potentially improving the safety and\nreliability of automated radiology reporting.\n","authors":["Romain Hardy","Sung Eun Kim","Du Hyun Ro","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2412.15264v3.pdf","comment":"Accepted to AIMedHealth 10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.18858v1","updated":"2025-01-31T02:39:07Z","published":"2025-01-31T02:39:07Z","title":"BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language\n  Model Reasoning","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks, yet generating reliable reasoning processes remains a\nsignificant challenge. We present a unified probabilistic framework that\nformalizes LLM reasoning through a novel graphical model incorporating latent\nthinking processes and evaluation signals. Within this framework, we introduce\nthe Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in\ntwo steps. First, it generates high-quality rationales by approximating the\noptimal thinking process through reinforcement learning, using a novel reward\nshaping mechanism. Second, it enhances the base LLM by maximizing the joint\nprobability of rationale generation with respect to the model's parameters.\nTheoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$\nrepresenting the number of iterations. Empirical evaluations on math and coding\nbenchmarks demonstrate that our approach consistently improves performance\nacross different base models without requiring human-annotated thinking\nprocesses. In addition, BRiTE demonstrates superior performance compared to\nexisting algorithms that bootstrap thinking processes use alternative methods\nsuch as rejection sampling, and can even match or exceed the results achieved\nthrough supervised fine-tuning with human-annotated data.\n","authors":["Han Zhong","Yutong Yin","Shenao Zhang","Xiaojun Xu","Yuanxin Liu","Yifei Zuo","Zhihan Liu","Boyi Liu","Sirui Zheng","Hongyi Guo","Liwei Wang","Mingyi Hong","Zhaoran Wang"],"pdf_url":"https://arxiv.org/pdf/2501.18858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04343v3","updated":"2025-01-31T02:33:07Z","published":"2024-01-09T03:53:59Z","title":"Private Fine-tuning of Large Language Models with Zeroth-order\n  Optimization","summary":"  Differentially private stochastic gradient descent (DP-SGD) allows models to\nbe trained in a privacy-preserving manner, but has proven difficult to scale to\nthe era of foundation models. We introduce DP-ZO, a private fine-tuning\nframework for large language models by privatizing zeroth order optimization\nmethods. A key insight into the design of our method is that the direction of\nthe gradient in the zeroth-order optimization we use is random and the only\ninformation from training data is the step size, i.e., a scalar. Therefore, we\nonly need to privatize the scalar step size, which is memory-efficient. DP-ZO\nprovides a strong privacy-utility trade-off across different tasks, and model\nsizes that are comparable to DP-SGD in $(\\varepsilon,\\delta)$-DP. Notably,\nDP-ZO possesses significant advantages over DP-SGD in memory efficiency, and\nobtains higher utility in $\\varepsilon$-DP when using the Laplace mechanism.\n","authors":["Xinyu Tang","Ashwinee Panda","Milad Nasr","Saeed Mahloujifar","Prateek Mittal"],"pdf_url":"https://arxiv.org/pdf/2401.04343v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14288v2","updated":"2025-01-31T02:10:36Z","published":"2025-01-24T07:07:37Z","title":"A Comprehensive Framework for Semantic Similarity Analysis of Human and\n  AI-Generated Text Using Transformer Architectures and Ensemble Techniques","summary":"  The rapid advancement of large language models (LLMs) has made detecting\nAI-generated text an increasingly critical challenge. Traditional methods often\nfail to capture the nuanced semantic differences between human and\nmachine-generated content. We therefore propose a novel approach based on\nsemantic similarity analysis, leveraging a multi-layered architecture that\ncombines a pre-trained DeBERTa-v3-large model, Bi-directional LSTMs, and linear\nattention pooling to capture both local and global semantic patterns. To\nenhance performance, we employ advanced input and output augmentation\ntechniques such as sector-level context integration and wide output\nconfigurations. These techniques enable the model to learn more discriminative\nfeatures and generalize across diverse domains. Experimental results show that\nthis approach works better than traditional methods, proving its usefulness for\nAI-generated text detection and other text comparison tasks.\n","authors":["Lifu Gao","Ziwei Liu","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.14288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18845v1","updated":"2025-01-31T01:50:49Z","published":"2025-01-31T01:50:49Z","title":"Text Data Augmentation for Large Language Models: A Comprehensive Survey\n  of Methods, Challenges, and Opportunities","summary":"  The increasing size and complexity of pre-trained language models have\ndemonstrated superior performance in many applications, but they usually\nrequire large training datasets to be adequately trained. Insufficient training\nsets could unexpectedly make the model overfit and fail to cope with complex\ntasks. Large language models (LLMs) trained on extensive corpora have prominent\ntext generation capabilities, which improve the quality and quantity of data\nand play a crucial role in data augmentation. Specifically, distinctive prompt\ntemplates are given in personalised tasks to guide LLMs in generating the\nrequired content. Recent promising retrieval-based techniques further improve\nthe expressive performance of LLMs in data augmentation by introducing external\nknowledge to enable them to produce more grounded-truth data. This survey\nprovides an in-depth analysis of data augmentation in LLMs, classifying the\ntechniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based\nAugmentation and Hybrid Augmentation. We summarise the post-processing\napproaches in data augmentation, which contributes significantly to refining\nthe augmented data and enabling the model to filter out unfaithful content.\nThen, we provide the common tasks and evaluation metrics. Finally, we introduce\nexisting challenges and future opportunities that could bring further\nimprovement to data augmentation.\n","authors":["Yaping Chai","Haoran Xie","Joe S. Qin"],"pdf_url":"https://arxiv.org/pdf/2501.18845v1.pdf","comment":"20 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.14459v2","updated":"2025-01-31T01:37:22Z","published":"2024-09-22T14:14:05Z","title":"Exploring Multilingual Probing in Large Language Models: A\n  Cross-Language Analysis","summary":"  Probing techniques for large language models (LLMs) have primarily focused on\nEnglish, overlooking the vast majority of the world's languages. In this paper,\nwe extend these probing methods to a multilingual context, investigating the\nbehaviors of LLMs across diverse languages. We conduct experiments on several\nopen-source LLM models, analyzing probing accuracy, trends across layers, and\nsimilarities between probing vectors for multiple languages. Our key findings\nreveal: (1) a consistent performance gap between high-resource and low-resource\nlanguages, with high-resource languages achieving significantly higher probing\naccuracy; (2) divergent layer-wise accuracy trends, where high-resource\nlanguages show substantial improvement in deeper layers similar to English; and\n(3) higher representational similarities among high-resource languages, with\nlow-resource languages demonstrating lower similarities both among themselves\nand with high-resource languages. These results highlight significant\ndisparities in LLMs' multilingual capabilities and emphasize the need for\nimproved modeling of low-resource languages.\n","authors":["Daoyang Li","Haiyan Zhao","Qingcheng Zeng","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2409.14459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18838v1","updated":"2025-01-31T01:12:50Z","published":"2025-01-31T01:12:50Z","title":"Partially Rewriting a Transformer in Natural Language","summary":"  The greatest ambition of mechanistic interpretability is to completely\nrewrite deep neural networks in a format that is more amenable to human\nunderstanding, while preserving their behavior and performance. In this paper,\nwe attempt to partially rewrite a large language model using simple natural\nlanguage explanations. We first approximate one of the feedforward networks in\nthe LLM with a wider MLP with sparsely activating neurons - a transcoder - and\nuse an automated interpretability pipeline to generate explanations for these\nneurons. We then replace the first layer of this sparse MLP with an LLM-based\nsimulator, which predicts the activation of each neuron given its explanation\nand the surrounding context. Finally, we measure the degree to which these\nmodifications distort the model's final output. With our pipeline, the model's\nincrease in loss is statistically similar to entirely replacing the sparse MLP\noutput with the zero vector. We employ the same protocol, this time using a\nsparse autoencoder, on the residual stream of the same layer and obtain similar\nresults. These results suggest that more detailed explanations are needed to\nimprove performance substantially above the zero ablation baseline.\n","authors":["Gonçalo Paulo","Nora Belrose"],"pdf_url":"https://arxiv.org/pdf/2501.18838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18837v1","updated":"2025-01-31T01:09:32Z","published":"2025-01-31T01:09:32Z","title":"Constitutional Classifiers: Defending against Universal Jailbreaks\n  across Thousands of Hours of Red Teaming","summary":"  Large language models (LLMs) are vulnerable to universal jailbreaks-prompting\nstrategies that systematically bypass model safeguards and enable users to\ncarry out harmful processes that require many model interactions, like\nmanufacturing illegal substances at scale. To defend against these attacks, we\nintroduce Constitutional Classifiers: safeguards trained on synthetic data,\ngenerated by prompting LLMs with natural language rules (i.e., a constitution)\nspecifying permitted and restricted content. In over 3,000 estimated hours of\nred teaming, no red teamer found a universal jailbreak that could extract\ninformation from an early classifier-guarded LLM at a similar level of detail\nto an unguarded model across most target queries. On automated evaluations,\nenhanced classifiers demonstrated robust defense against held-out\ndomain-specific jailbreaks. These classifiers also maintain deployment\nviability, with an absolute 0.38% increase in production-traffic refusals and a\n23.7% inference overhead. Our work demonstrates that defending against\nuniversal jailbreaks while maintaining practical deployment viability is\ntractable.\n","authors":["Mrinank Sharma","Meg Tong","Jesse Mu","Jerry Wei","Jorrit Kruthoff","Scott Goodfriend","Euan Ong","Alwin Peng","Raj Agarwal","Cem Anil","Amanda Askell","Nathan Bailey","Joe Benton","Emma Bluemke","Samuel R. Bowman","Eric Christiansen","Hoagy Cunningham","Andy Dau","Anjali Gopal","Rob Gilson","Logan Graham","Logan Howard","Nimit Kalra","Taesung Lee","Kevin Lin","Peter Lofgren","Francesco Mosconi","Clare O'Hara","Catherine Olsson","Linda Petrini","Samir Rajani","Nikhil Saxena","Alex Silverstein","Tanya Singh","Theodore Sumers","Leonard Tang","Kevin K. Troy","Constantin Weisser","Ruiqi Zhong","Giulio Zhou","Jan Leike","Jared Kaplan","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2501.18837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18826v1","updated":"2025-01-31T00:46:21Z","published":"2025-01-31T00:46:21Z","title":"Structural Embedding Projection for Contextual Large Language Model\n  Inference","summary":"  Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.\n","authors":["Vincent Enoasmo","Cedric Featherstonehaugh","Xavier Konstantinopoulos","Zacharias Huntington"],"pdf_url":"https://arxiv.org/pdf/2501.18826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18824v1","updated":"2025-01-31T00:43:50Z","published":"2025-01-31T00:43:50Z","title":"Memory-Efficient Fine-Tuning of Transformers via Token Selection","summary":"  Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.\n","authors":["Antoine Simoulin","Namyong Park","Xiaoyi Liu","Grey Yang"],"pdf_url":"https://arxiv.org/pdf/2501.18824v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2501.18817v1","updated":"2025-01-31T00:28:29Z","published":"2025-01-31T00:28:29Z","title":"Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised\n  Strategies","summary":"  Recent advancements in the reasoning skills of Large Language Models (LLMs)\ndemonstrate an increase in the ability of LLMs to solve simple planning tasks.\nHowever, as long as the driving force behind improved reasoning capability is\nthe size and complexity of the model, the financial and computational costs\nassociated with running them will also increase. This trend raises questions\nabout continued accessibility and whether these improvements will increase at\nthe same pace as models continue to grow in size and expense. We propose two\napproaches to enhance the reasoning ability of less resource-intensive LLMs.\n(1) Provide them with a generalised strategy for solving tasks within a given\ndomain, generated by a more resource-intensive LLM. (2) Exploit their\ncost-effectiveness by iteratively prompting these models to correct errors in\ntheir proposed solutions. Our empirical results from planning and mathematical\nreasoning tasks demonstrate that these methods improve the performance of less\nresource-intensive LLMs to levels comparable with their more resource-intensive\ncounterparts, at a fraction of the cost. Additionally, we show that the\nutilisation of generalised strategies in our experiments reduced the cost of\nthe less resource-intensive model by nearly 30 percent on average.\n","authors":["Andrey Borro","Patricia J Riddle","Michael W Barley","Michael J Witbrock"],"pdf_url":"https://arxiv.org/pdf/2501.18817v1.pdf","comment":"7 page body, 2 page references, 16 page appendix (25 pages total); 2\n  figures; submitted to IJCAI2025"},{"id":"http://arxiv.org/abs/2501.18816v1","updated":"2025-01-31T00:26:38Z","published":"2025-01-31T00:26:38Z","title":"Large Language Models as Common-Sense Heuristics","summary":"  While systems designed for solving planning tasks vastly outperform Large\nLanguage Models (LLMs) in this domain, they usually discard the rich semantic\ninformation embedded within task descriptions. In contrast, LLMs possess\nparametrised knowledge across a wide range of topics, enabling them to leverage\nthe natural language descriptions of planning tasks in their solutions.\nHowever, current research in this direction faces challenges in generating\ncorrect and executable plans. Furthermore, these approaches depend on the LLM\nto output solutions in an intermediate language, which must be translated into\nthe representation language of the planning task. We introduce a novel planning\nmethod, which leverages the parametrised knowledge of LLMs by using their\noutput as a heuristic for Hill-Climbing Search. This approach is further\nenhanced by prompting the LLM to generate a solution estimate to guide the\nsearch. Our method outperforms the task success rate of similar systems within\na common household environment by 22 percentage points, with consistently\nexecutable plans. All actions are encoded in their original representation,\ndemonstrating that strong results can be achieved without an intermediate\nlanguage, thus eliminating the need for a translation step.\n","authors":["Andrey Borro","Patricia J Riddle","Michael W Barley","Michael J Witbrock"],"pdf_url":"https://arxiv.org/pdf/2501.18816v1.pdf","comment":"7 page body, 2 page references, 5 page appendix (14 page total); 1\n  figure; Submitted to IJCAI2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.19348v1","updated":"2025-01-31T17:52:03Z","published":"2025-01-31T17:52:03Z","title":"Characterizing User Behavior: The Interplay Between Mobility Patterns\n  and Mobile Traffic","summary":"  Mobile devices have become essential for capturing human activity, and\neXtended Data Records (XDRs) offer rich opportunities for detailed user\nbehavior modeling, which is useful for designing personalized digital services.\nPrevious studies have primarily focused on aggregated mobile traffic and\nmobility analyses, often neglecting individual-level insights. This paper\nintroduces a novel approach that explores the dependency between traffic and\nmobility behaviors at the user level. By analyzing 13 individual features that\nencompass traffic patterns and various mobility aspects, we enhance the\nunderstanding of how these behaviors interact. Our advanced user modeling\nframework integrates traffic and mobility behaviors over time, allowing for\nfine-grained dependencies while maintaining population heterogeneity through\nuser-specific signatures. Furthermore, we develop a Markov model that infers\ntraffic behavior from mobility and vice versa, prioritizing significant\ndependencies while addressing privacy concerns. Using a week-long XDR dataset\nfrom 1,337,719 users across several provinces in Chile, we validate our\napproach, demonstrating its robustness and applicability in accurately\ninferring user behavior and matching mobility and traffic profiles across\ndiverse urban contexts.\n","authors":["Anne Josiane Kouam","Aline Carneiro Viana","Mariano G. Beiró","Leo Ferres","Luca Pappalardo"],"pdf_url":"https://arxiv.org/pdf/2501.19348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19264v1","updated":"2025-01-31T16:24:46Z","published":"2025-01-31T16:24:46Z","title":"mFollowIR: a Multilingual Benchmark for Instruction Following in\n  Retrieval","summary":"  Retrieval systems generally focus on web-style queries that are short and\nunderspecified. However, advances in language models have facilitated the\nnascent rise of retrieval models that can understand more complex queries with\ndiverse intents. However, these efforts have focused exclusively on English;\ntherefore, we do not yet understand how they work across languages. We\nintroduce mFollowIR, a multilingual benchmark for measuring\ninstruction-following ability in retrieval models. mFollowIR builds upon the\nTREC NeuCLIR narratives (or instructions) that span three diverse languages\n(Russian, Chinese, Persian) giving both query and instruction to the retrieval\nmodels. We make small changes to the narratives and isolate how well retrieval\nmodels can follow these nuanced changes. We present results for both\nmultilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong\ncross-lingual performance with English-based retrievers that trained using\ninstructions, but find a notable drop in performance in the multilingual\nsetting, indicating that more work is needed in developing data for\ninstruction-based multilingual retrievers.\n","authors":["Orion Weller","Benjamin Chang","Eugene Yang","Mahsa Yarmohammadi","Sam Barham","Sean MacAvaney","Arman Cohan","Luca Soldaini","Benjamin Van Durme","Dawn Lawrie"],"pdf_url":"https://arxiv.org/pdf/2501.19264v1.pdf","comment":"Accepted to ECIR 2025"},{"id":"http://arxiv.org/abs/2501.19241v1","updated":"2025-01-31T15:55:14Z","published":"2025-01-31T15:55:14Z","title":"Emancipatory Information Retrieval","summary":"  Our world today is facing a confluence of several mutually reinforcing crises\neach of which intersects with concerns of social justice and emancipation. This\npaper is a provocation for the role of computer-mediated information access in\nour emancipatory struggles. We define emancipatory information retrieval as the\nstudy and development of information access methods that challenge various\nforms of human oppression, and situates its activities within broader\ncollective emancipatory praxis. The term \"emancipatory\" here signifies the\nmoral concerns of universal humanization of all peoples and the elimination of\noppression to create the conditions under which we can collectively flourish.\nTo develop an emancipatory research agenda for IR, in this paper we speculate\nabout the practices that the community can adopt, enumerate some of the\nprojects that the field should undertake, and discuss provocations to spark new\nideas and directions for research. We challenge the field of information\nretrieval (IR) research to embrace humanistic values and commit to universal\nemancipation and social justice as part of our research.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2501.19241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19232v1","updated":"2025-01-31T15:43:21Z","published":"2025-01-31T15:43:21Z","title":"A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain\n  Sequential Recommendation","summary":"  Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains.\n","authors":["Yunzhe Li","Junting Wang","Hari Sundaram","Zhining Liu"],"pdf_url":"https://arxiv.org/pdf/2501.19232v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2406.12433v3","updated":"2025-01-31T14:22:27Z","published":"2024-06-18T09:29:18Z","title":"LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations","summary":"  Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria. The code for this implementation is publicly\navailable.\n","authors":["Jingtong Gao","Bo Chen","Weiwen Liu","Xiangyang Li","Yichao Wang","Wanyu Wang","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.12433v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16125v2","updated":"2025-01-31T14:00:30Z","published":"2025-01-27T15:12:27Z","title":"SampleLLM: Optimizing Tabular Data Synthesis in Recommendations","summary":"  Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.\n","authors":["Jingtong Gao","Zhaocheng Du","Xiaopeng Li","Yichao Wang","Xiangyang Li","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.16125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11403v2","updated":"2025-01-31T10:08:36Z","published":"2025-01-20T11:06:05Z","title":"Verifying Cross-modal Entity Consistency in News using Vision-language\n  Models","summary":"  The web has become a crucial source of information, but it is also used to\nspread disinformation, often conveyed through multiple modalities like images\nand text. The identification of inconsistent cross-modal information, in\nparticular entities such as persons, locations, and events, is critical to\ndetect disinformation. Previous works either identify out-of-context\ndisinformation by assessing the consistency of images to the whole document,\nneglecting relations of individual entities, or focus on generic entities that\nare not relevant to news. So far, only few approaches have addressed the task\nof validating entity consistency between images and text in news. However, the\npotential of large vision-language models (LVLMs) has not been explored yet. In\nthis paper, we propose an LVLM-based framework for verifying Cross-modal Entity\nConsistency~(LVLM4CEC), to assess whether persons, locations and events in news\narticles are consistent across both modalities. We suggest effective prompting\nstrategies for LVLMs for entity verification that leverage reference images\ncrawled from web. Moreover, we extend three existing datasets for the task of\nentity verification in news providing manual ground-truth data. Our results\nshow the potential of LVLMs for automating cross-modal entity verification,\nshowing improved accuracy in identifying persons and events when using evidence\nimages. Moreover, our method outperforms a baseline for location and event\nverification in documents. The datasets and source code are available on GitHub\nat https://github.com/TIBHannover/LVLM4CEC.\n","authors":["Sahar Tahmasebi","David Ernst","Eric Müller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2501.11403v2.pdf","comment":"Accepted for publication in: European Conference on Information\n  Retrieval (ECIR) 2025"},{"id":"http://arxiv.org/abs/2501.18997v1","updated":"2025-01-31T10:05:01Z","published":"2025-01-31T10:05:01Z","title":"Collaborative Diffusion Model for Recommender System","summary":"  Diffusion-based recommender systems (DR) have gained increasing attention for\ntheir advanced generative and denoising capabilities. However, existing DR face\ntwo central limitations: (i) a trade-off between enhancing generative capacity\nvia noise injection and retaining the loss of personalized information. (ii)\nthe underutilization of rich item-side information. To address these\nchallenges, we present a Collaborative Diffusion model for Recommender System\n(CDiff4Rec). Specifically, CDiff4Rec generates pseudo-users from item features\nand leverages collaborative signals from both real and pseudo personalized\nneighbors identified through behavioral similarity, thereby effectively\nreconstructing nuanced user preferences. Experimental results on three public\ndatasets show that CDiff4Rec outperforms competitors by effectively mitigating\nthe loss of personalized information through the integration of item content\nand collaborative signals.\n","authors":["Gyuseok Lee","Yaochen Zhu","Hwanjo Yu","Yao Zhou","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2501.18997v1.pdf","comment":"WWW'25 short"},{"id":"http://arxiv.org/abs/2412.10381v4","updated":"2025-01-31T09:43:40Z","published":"2024-11-28T04:06:02Z","title":"Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream\n  Allocation in Feed","summary":"  In the context of a short video & live stream mixed recommendation scenario,\nthe live stream recommendation system (RS) decides whether to allocate at most\none live stream into the video feed for each user request. To maximize\nlong-term user engagement, it is crucial to determine an optimal live stream\npolicy for accurate live stream allocation. The inappropriate live stream\nallocation policy can significantly affect the duration of the usage app and\nuser retention, which ignores the long-term negative impact of live stream\nallocation. Recently, reinforcement learning (RL) has been widely applied in\nrecommendation systems to capture long-term user engagement. However,\ntraditional RL algorithms often face divergence and instability problems, which\nrestricts the application and deployment in the large-scale industrial\nrecommendation systems, especially in the aforementioned challenging scenario.\nTo address these challenges, we propose a novel Supervised Learning-enhanced\nMulti-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a\nsupervised learning-enhanced actor-critic framework that incorporates variance\nreduction techniques, where multi-task reward learning helps restrict\nbootstrapping error accumulation during critic learning. Additionally, we\ndesign a multi-group state decomposition module for both actor and critic\nnetworks to reduce prediction variance and improve model stability. We also\npropose a novel reward function to prevent overly greedy live stream\nallocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy\nevaluation (OPE) and online A/B testing. Experimental results demonstrate that\nthe proposed method not only outperforms baseline methods under the\nplatform-level constraints but also exhibits enhanced stability in online\nrecommendation scenarios.\n","authors":["Jingxin Liu","Xiang Gao","Yisha Li","Xin Li","Haiyang Lu","Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10381v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06877v2","updated":"2025-01-31T07:50:44Z","published":"2024-11-11T11:17:35Z","title":"LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?","summary":"  Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant effort in manual annotations, which often makes it very expensive\nand time-consuming. Thus, test collections could become too small when the\nbudget is limited, which may lead to unstable evaluations. As a cheaper\nalternative, recent studies have proposed the use of large language models\n(LLMs) to completely replace human assessors. However, while LLMs may seem to\nsomewhat correlate with human judgments, their predictions are not perfect and\noften show bias. Thus a complete replacement with LLMs is argued to be too\nrisky and not fully reliable.\n  Thus, in this paper, we propose LLM-Assisted Relevance Assessments (LARA), an\neffective method to balance manual annotations with LLM annotations, which\nhelps to build a rich and reliable test collection even under a low budget. We\nuse the LLM's predicted relevance probabilities to select the most profitable\ndocuments to manually annotate under a budget constraint. With theoretical\nreasoning, LARA effectively guides the human annotation process by actively\nlearning to calibrate the LLM's predicted relevance probabilities. Then, using\nthe calibration model learned from the limited manual annotations, LARA\ndebiases the LLM predictions to annotate the remaining non-assessed data.\nEmpirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and\nTREC-COVID datasets show that LARA outperforms alternative solutions under\nalmost any budget constraint.\n","authors":["Rikiya Takehi","Ellen M. Voorhees","Tetsuya Sakai","Ian Soboroff"],"pdf_url":"https://arxiv.org/pdf/2411.06877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11916v2","updated":"2025-01-31T06:57:29Z","published":"2025-01-21T06:43:16Z","title":"Generating with Fairness: A Modality-Diffused Counterfactual Framework\n  for Incomplete Multimodal Recommendations","summary":"  Incomplete scenario is a prevalent, practical, yet challenging setting in\nMultimodal Recommendations (MMRec), where some item modalities are missing due\nto various factors. Recently, a few efforts have sought to improve the\nrecommendation accuracy by exploring generic structures from incomplete data.\nHowever, two significant gaps persist: 1) the difficulty in accurately\ngenerating missing data due to the limited ability to capture modality\ndistributions; and 2) the critical but overlooked visibility bias, where items\nwith missing modalities are more likely to be disregarded due to the\nprioritization of items' multimodal data over user preference alignment. This\nbias raises serious concerns about the fair treatment of items. To bridge these\ntwo gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)\nframework for incomplete multimodal recommendations. MoDiCF features two key\nmodules: a novel modality-diffused data completion module and a new\ncounterfactual multimodal recommendation module. The former, equipped with a\nparticularly designed multimodal generative framework, accurately generates and\niteratively refines missing data from learned modality-specific distribution\nspaces. The latter, grounded in the causal perspective, effectively mitigates\nthe negative causal effects of visibility bias and thus assures fairness in\nrecommendations. Both modules work collaboratively to address the two\naforementioned significant gaps for generating more accurate and fair results.\nExtensive experiments on three real-world datasets demonstrate the superior\nperformance of MoDiCF in terms of both recommendation accuracy and fairness.\nThe code and processed datasets are released at\nhttps://github.com/JinLi-i/MoDiCF.\n","authors":["Jin Li","Shoujin Wang","Qi Zhang","Shui Yu","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2501.11916v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2501.19406v1","updated":"2025-01-31T18:59:16Z","published":"2025-01-31T18:59:16Z","title":"Low-Rank Adapting Models for Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) decompose language model representations into a\nsparse set of linear latent vectors. Recent works have improved SAEs using\nlanguage model gradients, but these techniques require many expensive backward\npasses during training and still cause a significant increase in cross entropy\nloss when SAE reconstructions are inserted into the model. In this work, we\nimprove on these limitations by taking a fundamentally different approach: we\nuse low-rank adaptation (LoRA) to finetune the language model itself around a\npreviously trained SAE. We analyze our method across SAE sparsity, SAE width,\nlanguage model size, LoRA rank, and model layer on the Gemma Scope family of\nSAEs. In these settings, our method reduces the cross entropy loss gap by 30%\nto 55% when SAEs are inserted during the forward pass. We also find that\ncompared to end-to-end (e2e) SAEs, our approach achieves the same downstream\ncross entropy loss 3$\\times$ to 20$\\times$ faster on Gemma-2-2B and 2$\\times$\nto 10$\\times$ faster on Llama-3.2-1B. We further show that our technique\nimproves downstream metrics and can adapt multiple SAEs at once. Our results\ndemonstrate that improving model interpretability is not limited to post-hoc\nSAE training; Pareto improvements can also be achieved by directly optimizing\nthe model itself.\n","authors":["Matthew Chen","Joshua Engels","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2501.19406v1.pdf","comment":"Code available at https://github.com/matchten/LoRA-Models-for-SAEs"},{"id":"http://arxiv.org/abs/2501.19403v1","updated":"2025-01-31T18:58:43Z","published":"2025-01-31T18:58:43Z","title":"Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach","summary":"  Machine unlearning seeks to systematically remove specified data from a\ntrained model, effectively achieving a state as though the data had never been\nencountered during training. While metrics such as Unlearning Accuracy (UA) and\nMembership Inference Attack (MIA) provide a baseline for assessing unlearning\nperformance, they fall short of evaluating the completeness and reliability of\nforgetting. This is because the ground truth labels remain potential candidates\nwithin the scope of uncertainty quantification, leaving gaps in the evaluation\nof true forgetting. In this paper, we identify critical limitations in existing\nunlearning metrics and propose enhanced evaluation metrics inspired by\nconformal prediction. Our metrics can effectively capture the extent to which\nground truth labels are excluded from the prediction set. Furthermore, we\nobserve that many existing machine unlearning methods do not achieve\nsatisfactory forgetting performance when evaluated with our new metrics. To\naddress this, we propose an unlearning framework that integrates conformal\nprediction insights into Carlini & Wagner adversarial attack loss. Extensive\nexperiments on the image classification task demonstrate that our enhanced\nmetrics offer deeper insights into unlearning effectiveness, and that our\nunlearning framework significantly improves the forgetting quality of\nunlearning methods.\n","authors":["Yingdan Shi","Ren Wang"],"pdf_url":"https://arxiv.org/pdf/2501.19403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05362v2","updated":"2025-01-31T18:58:24Z","published":"2024-10-07T17:45:00Z","title":"LLMs Are In-Context Bandit Reinforcement Learners","summary":"  Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.\n","authors":["Giovanni Monea","Antoine Bosselut","Kianté Brantley","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.05362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19401v1","updated":"2025-01-31T18:57:21Z","published":"2025-01-31T18:57:21Z","title":"Detection Is All You Need: A Feasible Optimal Prior-Free Black-Box\n  Approach For Piecewise Stationary Bandits","summary":"  We study the problem of piecewise stationary bandits without prior knowledge\nof the underlying non-stationarity. We propose the first $\\textit{feasible}$\nblack-box algorithm applicable to most common parametric bandit variants. Our\nprocedure, termed Detection Augmented Bandit (DAB), is modular, accepting any\nstationary bandit algorithm as input and augmenting it with a change detector.\nDAB achieves optimal regret in the piecewise stationary setting under mild\nassumptions. Specifically, we prove that DAB attains the order-optimal regret\nbound of $\\tilde{\\mathcal{O}}(\\sqrt{N_T T})$, where $N_T$ denotes the number of\nchanges over the horizon $T$, if its input stationary bandit algorithm has\norder-optimal stationary regret guarantees. Applying DAB to different\nparametric bandit settings, we recover recent state-of-the-art results.\nNotably, for self-concordant bandits, DAB achieves optimal dynamic regret,\nwhile previous works obtain suboptimal bounds and require knowledge on the\nnon-stationarity. In simulations on piecewise stationary environments, DAB\noutperforms existing approaches across varying number of changes.\nInterestingly, despite being theoretically designed for piecewise stationary\nenvironments, DAB is also effective in simulations in drifting environments,\noutperforming existing methods designed specifically for this scenario.\n","authors":["Argyrios Gerogiannis","Yu-Han Huang","Subhonmesh Bose","Venugopal V. Veeravalli"],"pdf_url":"https://arxiv.org/pdf/2501.19401v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.18580v2","updated":"2025-01-31T18:57:14Z","published":"2025-01-30T18:52:43Z","title":"Node Classification and Search on the Rubik's Cube Graph with GNNs","summary":"  This study focuses on the application of deep geometric models to solve the\n3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation and\ndefining distance as the model's optimization objective. The distance\napproximation task is reformulated as a node classification problem,\neffectively addressed using Graph Neural Networks (GNNs). After training the\nmodel on a random subgraph, the predicted classes are used to construct a\nheuristic for $A^*$ search. We conclude with experiments comparing our\nheuristic to that of the DeepCubeA model.\n","authors":["Alessandro Barro"],"pdf_url":"https://arxiv.org/pdf/2501.18580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19400v1","updated":"2025-01-31T18:57:08Z","published":"2025-01-31T18:57:08Z","title":"Vintix: Action Model via In-Context Reinforcement Learning","summary":"  In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code to be released at\nhttps://github.com/dunnolab/vintix\n","authors":["Andrey Polubarov","Nikita Lyubaykin","Alexander Derevyagin","Ilya Zisman","Denis Tarasov","Alexander Nikulin","Vladislav Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2501.19400v1.pdf","comment":"Preprint. In review"},{"id":"http://arxiv.org/abs/2501.19399v1","updated":"2025-01-31T18:55:35Z","published":"2025-01-31T18:55:35Z","title":"Scalable-Softmax Is Superior for Attention","summary":"  The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.\n","authors":["Ken M. Nakanishi"],"pdf_url":"https://arxiv.org/pdf/2501.19399v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2212.13669v2","updated":"2025-01-31T18:54:09Z","published":"2022-12-28T02:45:46Z","title":"Near-Optimal Algorithms for Group Distributionally Robust Optimization\n  and Beyond","summary":"  Distributionally robust optimization (DRO) can improve the robustness and\nfairness of learning methods. In this paper, we devise stochastic algorithms\nfor a class of DRO problems including group DRO, subpopulation fairness, and\nempirical conditional value at risk (CVaR) optimization. Our new algorithms\nachieve faster convergence rates than existing algorithms for multiple DRO\nsettings. We also provide a new information-theoretic lower bound that implies\nour bounds are tight for group DRO. Empirically, too, our algorithms outperform\nknown methods.\n","authors":["Tasuku Soma","Khashayar Gatmiry","Sharut Gupta","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2212.13669v2.pdf","comment":"4 tables, 2 figures"},{"id":"http://arxiv.org/abs/2501.19398v1","updated":"2025-01-31T18:53:43Z","published":"2025-01-31T18:53:43Z","title":"Do LLMs Strategically Reveal, Conceal, and Infer Information? A\n  Theoretical and Empirical Analysis in The Chameleon Game","summary":"  Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions.\n","authors":["Mustafa O. Karabag","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2501.19398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11321v2","updated":"2025-01-31T18:52:42Z","published":"2024-09-17T16:18:05Z","title":"SOAP: Improving and Stabilizing Shampoo using Adam","summary":"  There is growing evidence of the effectiveness of Shampoo, a higher-order\npreconditioning method, over Adam in deep learning optimization tasks. However,\nShampoo's drawbacks include additional hyperparameters and computational\noverhead when compared to Adam, which only updates running averages of first-\nand second-moment quantities. This work establishes a formal connection between\nShampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient\napproximation of Adam -- showing that Shampoo is equivalent to running\nAdafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to\nthe design of a simpler and computationally efficient algorithm:\n$\\textbf{S}$hampo$\\textbf{O}$ with $\\textbf{A}$dam in the\n$\\textbf{P}$reconditioner's eigenbasis (SOAP).\n  With regards to improving Shampoo's computational efficiency, the most\nstraightforward approach would be to simply compute Shampoo's\neigendecomposition less frequently. Unfortunately, as our empirical results\nshow, this leads to performance degradation that worsens with this frequency.\nSOAP mitigates this degradation by continually updating the running average of\nthe second moment, just as Adam does, but in the current (slowly changing)\ncoordinate basis. Furthermore, since SOAP is equivalent to running Adam in a\nrotated space, it introduces only one additional hyperparameter (the\npreconditioning frequency) compared to Adam. We empirically evaluate SOAP on\nlanguage model pre-training with 360m and 660m sized models. In the large batch\nregime, SOAP reduces the number of iterations by over 40% and wall clock time\nby over 35% compared to AdamW, with approximately 20% improvements in both\nmetrics compared to Shampoo. An implementation of SOAP is available at\nhttps://github.com/nikhilvyas/SOAP.\n","authors":["Nikhil Vyas","Depen Morwani","Rosie Zhao","Mujin Kwun","Itai Shapira","David Brandfonbrener","Lucas Janson","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2409.11321v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19393v1","updated":"2025-01-31T18:48:08Z","published":"2025-01-31T18:48:08Z","title":"s1: Simple test-time scaling","summary":"  Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1 exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling s1\nwith budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1.\n","authors":["Niklas Muennighoff","Zitong Yang","Weijia Shi","Xiang Lisa Li","Li Fei-Fei","Hannaneh Hajishirzi","Luke Zettlemoyer","Percy Liang","Emmanuel Candès","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2501.19393v1.pdf","comment":"46 pages (9 main), 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2501.19392v1","updated":"2025-01-31T18:47:42Z","published":"2025-01-31T18:47:42Z","title":"Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models","summary":"  Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.\n","authors":["Alina Shutova","Vladimir Malinovskii","Vage Egiazarian","Denis Kuznedelev","Denis Mazur","Nikita Surkov","Ivan Ermakov","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2501.19392v1.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2501.19389v1","updated":"2025-01-31T18:44:35Z","published":"2025-01-31T18:44:35Z","title":"Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) on devices is attracting increasing\ninterest. Recent works have fused low-rank adaptation (LoRA) techniques with\nfederated fine-tuning to mitigate challenges associated with device model sizes\nand data scarcity. Still, the heterogeneity of computational resources remains\na critical bottleneck: while higher-rank modules generally enhance performance,\nvarying device capabilities constrain LoRA's feasible rank range. Existing\napproaches attempting to resolve this issue either lack analytical\njustification or impose additional computational overhead, leaving a wide gap\nfor an efficient and theoretically-grounded solution. To address these\nchallenges, we propose federated sketching LoRA (FSLoRA), which leverages a\nsketching mechanism to enable devices to selectively update submatrices of\nglobal LoRA modules maintained by the server. By adjusting the sketching\nratios, which determine the ranks of the submatrices on the devices, FSLoRA\nflexibly adapts to device-specific communication and computational constraints.\nWe provide a rigorous convergence analysis of FSLoRA that characterizes how the\nsketching ratios affect the convergence rate. Through comprehensive experiments\non multiple datasets and LLM models, we demonstrate FSLoRA's superior\nperformance compared to various baselines.\n","authors":["Wenzhi Fang","Dong-Jun Han","Liangqi Yuan","Seyyedali Hosseinalipour","Christopher G. Brinton"],"pdf_url":"https://arxiv.org/pdf/2501.19389v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2411.12700v3","updated":"2025-01-31T18:41:29Z","published":"2024-11-19T18:08:01Z","title":"Learning multivariate Gaussians with imperfect advice","summary":"  We revisit the problem of distribution learning within the framework of\nlearning-augmented algorithms. In this setting, we explore the scenario where a\nprobability distribution is provided as potentially inaccurate advice on the\ntrue, unknown distribution. Our objective is to develop learning algorithms\nwhose sample complexity decreases as the quality of the advice improves,\nthereby surpassing standard learning lower bounds when the advice is\nsufficiently accurate.\n  Specifically, we demonstrate that this outcome is achievable for the problem\nof learning a multivariate Gaussian distribution $N(\\boldsymbol{\\mu},\n\\boldsymbol{\\Sigma})$ in the PAC learning setting. Classically, in the\nadvice-free setting, $\\tilde{\\Theta}(d^2/\\varepsilon^2)$ samples are sufficient\nand worst case necessary to learn $d$-dimensional Gaussians up to TV distance\n$\\varepsilon$ with constant probability. When we are additionally given a\nparameter $\\tilde{\\boldsymbol{\\Sigma}}$ as advice, we show that\n$\\tilde{O}(d^{2-\\beta}/\\varepsilon^2)$ samples suffices whenever $\\|\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} \\boldsymbol{\\Sigma}\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} - \\boldsymbol{I_d} \\|_1 \\leq \\varepsilon\nd^{1-\\beta}$ (where $\\|\\cdot\\|_1$ denotes the entrywise $\\ell_1$ norm) for any\n$\\beta > 0$, yielding a polynomial improvement over the advice-free setting.\n","authors":["Arnab Bhattacharyya","Davin Choo","Philips George John","Themis Gouleakis"],"pdf_url":"https://arxiv.org/pdf/2411.12700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18052v2","updated":"2025-01-31T18:39:23Z","published":"2025-01-29T23:29:47Z","title":"SAeUron: Interpretable Concept Unlearning in Diffusion Models with\n  Sparse Autoencoders","summary":"  Diffusion models, while powerful, can inadvertently generate harmful or\nundesirable content, raising significant ethical and safety concerns. Recent\nmachine unlearning approaches offer potential solutions but often lack\ntransparency, making it difficult to understand the changes they introduce to\nthe base model. In this work, we introduce SAeUron, a novel method leveraging\nfeatures learned by sparse autoencoders (SAEs) to remove unwanted concepts in\ntext-to-image diffusion models. First, we demonstrate that SAEs, trained in an\nunsupervised manner on activations from multiple denoising timesteps of the\ndiffusion model, capture sparse and interpretable features corresponding to\nspecific concepts. Building on this, we propose a feature selection method that\nenables precise interventions on model activations to block targeted content\nwhile preserving overall performance. Evaluation with the competitive\nUnlearnCanvas benchmark on object and style unlearning highlights SAeUron's\nstate-of-the-art performance. Moreover, we show that with a single SAE, we can\nremove multiple concepts simultaneously and that in contrast to other methods,\nSAeUron mitigates the possibility of generating unwanted content, even under\nadversarial attack. Code and checkpoints are available at:\nhttps://github.com/cywinski/SAeUron.\n","authors":["Bartosz Cywiński","Kamil Deja"],"pdf_url":"https://arxiv.org/pdf/2501.18052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19383v1","updated":"2025-01-31T18:37:42Z","published":"2025-01-31T18:37:42Z","title":"Decoding-based Regression","summary":"  Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.\n","authors":["Xingyou Song","Dara Bahri"],"pdf_url":"https://arxiv.org/pdf/2501.19383v1.pdf","comment":"Google DeepMind Technical Report, 25 pages. Code can be found at\n  https://github.com/google-research/optformer/tree/main/optformer/decoding_regression"},{"id":"http://arxiv.org/abs/2501.19381v1","updated":"2025-01-31T18:34:16Z","published":"2025-01-31T18:34:16Z","title":"Using gradient of Lagrangian function to compute efficient channels for\n  the ideal observer","summary":"  It is widely accepted that the Bayesian ideal observer (IO) should be used to\nguide the objective assessment and optimization of medical imaging systems. The\nIO employs complete task-specific information to compute test statistics for\nmaking inference decisions and performs optimally in signal detection tasks.\nHowever, the IO test statistic typically depends non-linearly on the image data\nand cannot be analytically determined. The ideal linear observer, known as the\nHotelling observer (HO), can sometimes be used as a surrogate for the IO.\nHowever, when image data are high dimensional, HO computation can be difficult.\nEfficient channels that can extract task-relevant features have been\ninvestigated to reduce the dimensionality of image data to approximate IO and\nHO performance. This work proposes a novel method for generating efficient\nchannels by use of the gradient of a Lagrangian-based loss function that was\ndesigned to learn the HO. The generated channels are referred to as the\nLagrangian-gradient (L-grad) channels. Numerical studies are conducted that\nconsider binary signal detection tasks involving various backgrounds and\nsignals. It is demonstrated that channelized HO (CHO) using L-grad channels can\nproduce significantly better signal detection performance compared to the CHO\nusing PLS channels. Moreover, it is shown that the proposed L-grad method can\nachieve significantly lower computation time compared to the PLS method.\n","authors":["Weimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.19381v1.pdf","comment":"SPIE Medical Imaging 2025"},{"id":"http://arxiv.org/abs/2501.19377v1","updated":"2025-01-31T18:30:36Z","published":"2025-01-31T18:30:36Z","title":"SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions","summary":"  In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.\n","authors":["Dominik Wagner","Alexander Churchill","Siddarth Sigtia","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2501.19377v1.pdf","comment":"Accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.19374v1","updated":"2025-01-31T18:23:45Z","published":"2025-01-31T18:23:45Z","title":"Fixing the Double Penalty in Data-Driven Weather Forecasting Through a\n  Modified Spherical Harmonic Loss Function","summary":"  Recent advancements in data-driven weather forecasting models have delivered\ndeterministic models that outperform the leading operational forecast systems\nbased on traditional, physics-based models. However, these data-driven models\nare typically trained with a mean squared error loss function, which causes\nsmoothing of fine scales through a \"double penalty\" effect. We develop a\nsimple, parameter-free modification to this loss function that avoids this\nproblem by separating the loss attributable to decorrelation from the loss\nattributable to spectral amplitude errors. Fine-tuning the GraphCast model with\nthis new loss function results in sharp deterministic weather forecasts, an\nincrease of the model's effective resolution from 1,250km to 160km,\nimprovements to ensemble spread, and improvements to predictions of tropical\ncyclone strength and surface wind extremes.\n","authors":["Christopher Subich","Syed Zahid Husain","Leo Separovic","Jing Yang"],"pdf_url":"https://arxiv.org/pdf/2501.19374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19373v1","updated":"2025-01-31T18:23:27Z","published":"2025-01-31T18:23:27Z","title":"Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising\n  Diffusions","summary":"  We introduce a new class of generative diffusion models that, unlike\nconventional denoising diffusion models, achieve a time-homogeneous structure\nfor both the noising and denoising processes, allowing the number of steps to\nadaptively adjust based on the noise level. This is accomplished by\nconditioning the forward process using Doob's $h$-transform, which terminates\nthe process at a suitable sampling distribution at a random time. The model is\nparticularly well suited for generating data with lower intrinsic dimensions,\nas the termination criterion simplifies to a first-hitting rule. A key feature\nof the model is its adaptability to the target data, enabling a variety of\ndownstream tasks using a pre-trained unconditional generative model. These\ntasks include natural conditioning through appropriate initialization of the\ndenoising process and classification of noisy data.\n","authors":["Sören Christensen","Claudia Strauch","Lukas Trottner"],"pdf_url":"https://arxiv.org/pdf/2501.19373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02755v3","updated":"2025-01-31T18:21:59Z","published":"2024-10-03T17:58:29Z","title":"GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data","summary":"  Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data.\n","authors":["Jifan Zhang","Ziyue Luo","Jia Liu","Ness Shroff","Robert Nowak"],"pdf_url":"https://arxiv.org/pdf/2410.02755v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19364v1","updated":"2025-01-31T18:14:28Z","published":"2025-01-31T18:14:28Z","title":"CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation","summary":"  Multivariate Time Series Imputation (MTSI) is crucial for many applications,\nsuch as healthcare monitoring and traffic management, where incomplete data can\ncompromise decision-making. Existing state-of-the-art methods, like Denoising\nDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;\nhowever, they suffer from significant computational costs and are notably\ntime-consuming due to their iterative nature. In this work, we propose CoSTI,\nan innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI\nemploys Consistency Training to achieve comparable imputation quality to DDPMs\nwhile drastically reducing inference times, making it more suitable for\nreal-time applications. We evaluate CoSTI across multiple datasets and missing\ndata scenarios, demonstrating up to a 98% reduction in imputation time with\nperformance on par with diffusion-based models. This work bridges the gap\nbetween efficiency and accuracy in generative imputation tasks, providing a\nscalable solution for handling missing data in critical spatio-temporal\nsystems.\n","authors":["Javier Solís-García","Belén Vega-Márquez","Juan A. Nepomuceno","Isabel A. Nepomuceno-Chamorro"],"pdf_url":"https://arxiv.org/pdf/2501.19364v1.pdf","comment":"20 pages, 5 figures, 13 tables"},{"id":"http://arxiv.org/abs/2501.01480v3","updated":"2025-01-31T18:13:14Z","published":"2025-01-02T15:09:00Z","title":"CORAL: Concept Drift Representation Learning for Co-evolving Time-series","summary":"  In the realm of time series analysis, tackling the phenomenon of concept\ndrift poses a significant challenge. Concept drift -- characterized by the\nevolving statistical properties of time series data, affects the reliability\nand accuracy of conventional analysis models. This is particularly evident in\nco-evolving scenarios where interactions among variables are crucial. This\npaper presents CORAL, a simple yet effective method that models time series as\nan evolving ecosystem to learn representations of concept drift. CORAL employs\na kernel-induced self-representation learning to generate a representation\nmatrix, encapsulating the inherent dynamics of co-evolving time series. This\nmatrix serves as a key tool for identification and adaptation to concept drift\nby observing its temporal variations. Furthermore, CORAL effectively identifies\nprevailing patterns and offers insights into emerging trends through pattern\nevolution analysis. Our empirical evaluation of CORAL across various datasets\ndemonstrates its effectiveness in handling the complexities of concept drift.\nThis approach introduces a novel perspective in the theoretical domain of\nco-evolving time series analysis, enhancing adaptability and accuracy in the\nface of dynamic data environments, and can be easily integrated into most deep\nlearning backbones.\n","authors":["Kunpeng Xu","Lifei Chen","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2501.01480v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.01232v3","updated":"2025-01-31T18:12:54Z","published":"2022-12-02T15:20:58Z","title":"Loss shaping enhances exact gradient learning with Eventprop in spiking\n  neural networks","summary":"  Event-based machine learning promises more energy-efficient AI on future\nneuromorphic hardware. Here, we investigate how the recently discovered\nEventprop algorithm for gradient descent on exact gradients in spiking neural\nnetworks can be scaled up to challenging keyword recognition benchmarks. We\nimplemented Eventprop in the GPU-enhanced Neural Networks framework and used it\nfor training recurrent spiking neural networks on the Spiking Heidelberg Digits\nand Spiking Speech Commands datasets. We found that learning depended strongly\non the loss function and extended Eventprop to a wider class of loss functions\nto enable effective training. We then tested a large number of data\naugmentations and regularisations as well as exploring different network\nstructures; and heterogeneous and trainable timescales. We found that when\ncombined with two specific augmentations, the right regularisation and a delay\nline input, Eventprop networks with one recurrent layer achieved\nstate-of-the-art performance on Spiking Heidelberg Digits and good accuracy on\nSpiking Speech Commands. In comparison to a leading surrogate-gradient-based\nSNN training method, our GeNN Eventprop implementation is 3X faster and uses 4X\nless memory. This work is a significant step towards a low-power neuromorphic\nalternative to current machine learning paradigms.\n","authors":["Thomas Nowotny","James P. Turner","James C. Knight"],"pdf_url":"https://arxiv.org/pdf/2212.01232v3.pdf","comment":"36 pages, 7 figures, 5 tables. Neuromorphic Computing and Engineering\n  (2022)"},{"id":"http://arxiv.org/abs/2501.19361v1","updated":"2025-01-31T18:12:41Z","published":"2025-01-31T18:12:41Z","title":"We're Different, We're the Same: Creative Homogeneity Across LLMs","summary":"  Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.\n","authors":["Emily Wenger","Yoed Kenett"],"pdf_url":"https://arxiv.org/pdf/2501.19361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09009v2","updated":"2025-01-31T18:12:36Z","published":"2025-01-15T18:50:52Z","title":"Towards Fast, Specialized Machine Learning Force Fields: Distilling\n  Foundation Models via Energy Hessians","summary":"  The foundation model (FM) paradigm is transforming Machine Learning Force\nFields (MLFFs), leveraging general-purpose representations and scalable\ntraining to perform a variety of computational chemistry tasks. Although MLFF\nFMs have begun to close the accuracy gap relative to first-principles methods,\nthere is still a strong need for faster inference speed. Additionally, while\nresearch is increasingly focused on general-purpose models which transfer\nacross chemical space, practitioners typically only study a small subset of\nsystems at a given time. This underscores the need for fast, specialized MLFFs\nrelevant to specific downstream applications, which preserve test-time physical\nsoundness while maintaining train-time scalability. In this work, we introduce\na method for transferring general-purpose representations from MLFF foundation\nmodels to smaller, faster MLFFs specialized to specific regions of chemical\nspace. We formulate our approach as a knowledge distillation procedure, where\nthe smaller \"student\" MLFF is trained to match the Hessians of the energy\npredictions of the \"teacher\" foundation model. Our specialized MLFFs can be up\nto 20 $\\times$ faster than the original foundation model, while retaining, and\nin some cases exceeding, its performance and that of undistilled models. We\nalso show that distilling from a teacher model with a direct force\nparameterization into a student model trained with conservative forces (i.e.,\ncomputed as derivatives of the potential energy) successfully leverages the\nrepresentations from the large-scale teacher for improved accuracy, while\nmaintaining energy conservation during test-time molecular dynamics\nsimulations. More broadly, our work suggests a new paradigm for MLFF\ndevelopment, in which foundation models are released along with smaller,\nspecialized simulation \"engines\" for common chemical subsets.\n","authors":["Ishan Amin","Sanjeev Raja","Aditi Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2501.09009v2.pdf","comment":"Accepted as a conference paper at ICLR 2025. The implementation of\n  our method is available at https://github.com/ASK-Berkeley/MLFF-distill"},{"id":"http://arxiv.org/abs/2501.19358v1","updated":"2025-01-31T18:10:53Z","published":"2025-01-31T18:10:53Z","title":"The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating\n  Reward Hacking","summary":"  This work identifies the Energy Loss Phenomenon in Reinforcement Learning\nfrom Human Feedback (RLHF) and its connection to reward hacking. Specifically,\nenergy loss in the final layer of a Large Language Model (LLM) gradually\nincreases during the RL process, with an excessive increase in energy loss\ncharacterizing reward hacking. Beyond empirical analysis, we further provide a\ntheoretical foundation by proving that, under mild conditions, the increased\nenergy loss reduces the upper bound of contextual relevance in LLMs, which is a\ncritical aspect of reward hacking as the reduced contextual relevance typically\nindicates overfitting to reward model-favored patterns in RL. To address this\nissue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the\nincrease in energy loss in the LLM's final layer during reward calculation to\nprevent excessive energy loss, thereby mitigating reward hacking. We\ntheoretically show that EPPO can be conceptually interpreted as an\nentropy-regularized RL algorithm, which provides deeper insights into its\neffectiveness. Extensive experiments across various LLMs and tasks demonstrate\nthe commonality of the energy loss phenomenon, as well as the effectiveness of\n\\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.\n","authors":["Yuchun Miao","Sen Zhang","Liang Ding","Yuqi Zhang","Lefei Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2501.19358v1.pdf","comment":"28 pages, 21 figures"},{"id":"http://arxiv.org/abs/2501.19351v1","updated":"2025-01-31T17:56:09Z","published":"2025-01-31T17:56:09Z","title":"Neural Implicit Solution Formula for Efficiently Solving Hamilton-Jacobi\n  Equations","summary":"  This paper presents an implicit solution formula for the Hamilton-Jacobi\npartial differential equation (HJ PDE). The formula is derived using the method\nof characteristics and is shown to coincide with the Hopf and Lax formulas in\nthe case where either the Hamiltonian or the initial function is convex. It\nprovides a simple and efficient numerical approach for computing the viscosity\nsolution of HJ PDEs, bypassing the need for the Legendre transform of the\nHamiltonian or the initial condition, and the explicit computation of\nindividual characteristic trajectories. A deep learning-based methodology is\nproposed to learn this implicit solution formula, leveraging the mesh-free\nnature of deep learning to ensure scalability for high-dimensional problems.\nBuilding upon this framework, an algorithm is developed that approximates the\ncharacteristic curves piecewise linearly for state-dependent Hamiltonians.\nExtensive experimental results demonstrate that the proposed method delivers\nhighly accurate solutions, even for nonconvex Hamiltonians, and exhibits\nremarkable scalability, achieving computational efficiency for problems up to\n40 dimensions.\n","authors":["Yesom Park","Stanley Osher"],"pdf_url":"https://arxiv.org/pdf/2501.19351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19347v1","updated":"2025-01-31T17:51:46Z","published":"2025-01-31T17:51:46Z","title":"An All-digital 65-nm Tsetlin Machine Image Classification Accelerator\n  with 8.6 nJ per MNIST Frame at 60.3k Frames per Second","summary":"  We present an all-digital programmable machine learning accelerator chip for\nimage classification, underpinning on the Tsetlin machine (TM) principles. The\nTM is a machine learning algorithm founded on propositional logic, utilizing\nsub-pattern recognition expressions called clauses. The accelerator implements\nthe coalesced TM version with convolution, and classifies booleanized images of\n28$\\times$28 pixels with 10 categories. A configuration with 128 clauses is\nused in a highly parallel architecture. Fast clause evaluation is obtained by\nkeeping all clause weights and Tsetlin automata (TA) action signals in\nregisters. The chip is implemented in a 65 nm low-leakage CMOS technology, and\noccupies an active area of 2.7mm$^2$. At a clock frequency of 27.8 MHz, the\naccelerator achieves 60.3k classifications per second, and consumes 8.6 nJ per\nclassification. The latency for classifying a single image is 25.4 $\\mu$s which\nincludes system timing overhead. The accelerator achieves 97.42%, 84.54% and\n82.55% test accuracies for the datasets MNIST, Fashion-MNIST and\nKuzushiji-MNIST, respectively, matching the TM software models.\n","authors":["Svein Anders Tunheim","Yujin Zheng","Lei Jiao","Rishad Shafik","Alex Yakovlev","Ole-Christoffer Granmo"],"pdf_url":"https://arxiv.org/pdf/2501.19347v1.pdf","comment":"10 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2501.19345v1","updated":"2025-01-31T17:47:32Z","published":"2025-01-31T17:47:32Z","title":"PUATE: Semiparametric Efficient Average Treatment Effect Estimation from\n  Treated (Positive) and Unlabeled Units","summary":"  The estimation of average treatment effects (ATEs), defined as the difference\nin expected outcomes between treatment and control groups, is a central topic\nin causal inference. This study develops semiparametric efficient estimators\nfor ATE estimation in a setting where only a treatment group and an unknown\ngroup-comprising units for which it is unclear whether they received the\ntreatment or control-are observable. This scenario represents a variant of\nlearning from positive and unlabeled data (PU learning) and can be regarded as\na special case of ATE estimation with missing data. For this setting, we derive\nsemiparametric efficiency bounds, which provide lower bounds on the asymptotic\nvariance of regular estimators. We then propose semiparametric efficient ATE\nestimators whose asymptotic variance aligns with these efficiency bounds. Our\nfindings contribute to causal inference with missing data and weakly supervised\nlearning.\n","authors":["Masahiro Kato","Fumiaki Kozai","Ryo Inokuchi"],"pdf_url":"https://arxiv.org/pdf/2501.19345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19342v1","updated":"2025-01-31T17:43:30Z","published":"2025-01-31T17:43:30Z","title":"Covering Multiple Objectives with a Small Set of Solutions Using\n  Bayesian Optimization","summary":"  In multi-objective black-box optimization, the goal is typically to find\nsolutions that optimize a set of T black-box objective functions, $f_1$, ...,\n$f_T$, simultaneously. Traditional approaches often seek a single\nPareto-optimal set that balances trade-offs among all objectives. In this work,\nwe introduce a novel problem setting that departs from this paradigm: finding a\nsmaller set of K solutions, where K < T, that collectively \"covers\" the T\nobjectives. A set of solutions is defined as \"covering\" if, for each objective\n$f_1$, ..., $f_T$, there is at least one good solution. A motivating example\nfor this problem setting occurs in drug design. For example, we may have T\npathogens and aim to identify a set of K < T antibiotics such that at least one\nantibiotic can be used to treat each pathogen. To address this problem, we\npropose Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principled\nalgorithm designed to efficiently find a covering set. We validate our approach\nthrough extensive experiments on challenging high-dimensional tasks, including\napplications in peptide and molecular design. Experiments demonstrate MOCOBO's\nability to find high-performing covering sets of solutions. Additionally, we\nshow that the small sets of K < T solutions found by MOCOBO can match or nearly\nmatch the performance of T individually optimized solutions for the same\nobjectives. Our results highlight MOCOBO's potential to tackle complex\nmulti-objective problems in domains where finding at least one high-performing\nsolution for each objective is critical.\n","authors":["Natalie Maus","Kyurae Kim","Yimeng Zeng","Haydn Thomas Jones","Fangping Wan","Marcelo Der Torossian Torres","Cesar de la Fuente-Nunez","Jacob R. Gardner"],"pdf_url":"https://arxiv.org/pdf/2501.19342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14713v2","updated":"2025-01-31T17:38:07Z","published":"2025-01-24T18:46:37Z","title":"FlexiGPT: Pruning and Extending Large Language Models with Low-Rank\n  Weight Sharing","summary":"  The rapid proliferation of large language models (LLMs) in natural language\nprocessing (NLP) has created a critical need for techniques that enable\nefficient deployment on memory-constrained devices without compromising\nperformance. We present a method to prune LLMs that selectively prunes model\nblocks based on an importance score and replaces them with a low-parameter\nreplacement strategy. Specifically, we propose a principled metric to replace\neach pruned block using a weight-sharing mechanism that leverages unpruned\ncounterparts from the model and block-specific low-rank adapters. Furthermore,\nwe facilitate the learning of these replacement blocks with output feature\nnormalization and an adapter initialization scheme built on low-rank SVD\nreconstructions. Empirical evaluations demonstrate substantial performance\ngains over existing methods, achieving state-of-the-art performance on 5/6\nbenchmarks for a compression rate of 30% and 6/6 benchmarks for a compression\nrate of 40%. We also demonstrate that our approach can extend smaller models,\nboosting performance on 6/6 benchmarks using only ~0.3% tokens of extended\ntraining with minimal additional parameter costs.\n","authors":["James Seale Smith","Chi-Heng Lin","Shikhar Tuli","Haris Jeelani","Shangqian Gao","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"pdf_url":"https://arxiv.org/pdf/2501.14713v2.pdf","comment":"Accepted to NAACL 2025 - Main Conference"},{"id":"http://arxiv.org/abs/2501.19335v1","updated":"2025-01-31T17:35:21Z","published":"2025-01-31T17:35:21Z","title":"What is causal about causal models and representations?","summary":"  Causal Bayesian networks are 'causal' models since they make predictions\nabout interventional distributions. To connect such causal model predictions to\nreal-world outcomes, we must determine which actions in the world correspond to\nwhich interventions in the model. For example, to interpret an action as an\nintervention on a treatment variable, the action will presumably have to a)\nchange the distribution of treatment in a way that corresponds to the\nintervention, and b) not change other aspects, such as how the outcome depends\non the treatment; while the marginal distributions of some variables may change\nas an effect. We introduce a formal framework to make such requirements for\ndifferent interpretations of actions as interventions precise. We prove that\nthe seemingly natural interpretation of actions as interventions is circular:\nUnder this interpretation, every causal Bayesian network that correctly models\nthe observational distribution is trivially also interventionally valid, and no\naction yields empirical data that could possibly falsify such a model. We prove\nan impossibility result: No interpretation exists that is non-circular and\nsimultaneously satisfies a set of natural desiderata. Instead, we examine\nnon-circular interpretations that may violate some desiderata and show how this\nmay in turn enable the falsification of causal models. By rigorously examining\nhow a causal Bayesian network could be a 'causal' model of the world instead of\nmerely a mathematical object, our formal framework contributes to the\nconceptual foundations of causal representation learning, causal discovery, and\ncausal abstraction, while also highlighting some limitations of existing\napproaches.\n","authors":["Frederik Hytting Jørgensen","Luigi Gresele","Sebastian Weichwald"],"pdf_url":"https://arxiv.org/pdf/2501.19335v1.pdf","comment":"50 pages"},{"id":"http://arxiv.org/abs/2412.11276v2","updated":"2025-01-31T17:35:20Z","published":"2024-12-15T18:48:14Z","title":"Wearable Accelerometer Foundation Models for Health via Knowledge\n  Distillation","summary":"  Modern wearable devices can conveniently record various biosignals in the\nmany different environments of daily living, enabling a rich view of individual\nhealth. However, not all biosignals are the same: high-fidelity biosignals,\nsuch as photoplethysmogram (PPG), contain more physiological information, but\nrequire optical sensors with a high power footprint. Alternatively, a\nlower-fidelity biosignal such as accelerometry has a significantly smaller\npower footprint and is available in almost any wearable device. While\naccelerometry is widely used for activity recognition and fitness, it is less\nexplored for health biomarkers and diagnosis. Here, we show that an\naccelerometry foundation model can predict a wide variety of health targets. To\nachieve improved performance, we distill representational knowledge from PPG\nencoders to accelerometery encoders using 20 million minutes of unlabeled data,\ncollected from ~172K participants in the Apple Heart and Movement Study under\ninformed consent. We observe strong cross-modal alignment on unseen data, e.g.,\n99.2% top-1 accuracy for retrieving PPG embeddings from accelerometry\nembeddings. We show that distilled accelerometry encoders have significantly\nmore informative representations compared to self-supervised or supervised\nencoders trained directly on accelerometry data, observed by at least 23%-49%\nimproved performance for predicting heart rate and heart rate variability. We\nalso show that distilled accelerometry encoders are readily predictive of a\nwide array of downstream health targets, i.e., they are generalist foundation\nmodels. We believe accelerometry foundation models for health may unlock new\nopportunities for developing digital biomarkers from any wearable device.\n","authors":["Salar Abbaspourazad","Anshuman Mishra","Joseph Futoma","Andrew C. Miller","Ian Shapiro"],"pdf_url":"https://arxiv.org/pdf/2412.11276v2.pdf","comment":"updated format"},{"id":"http://arxiv.org/abs/2501.19334v1","updated":"2025-01-31T17:34:53Z","published":"2025-01-31T17:34:53Z","title":"The Value of Prediction in Identifying the Worst-Off","summary":"  Machine learning is increasingly used in government programs to identify and\nsupport the most vulnerable individuals, prioritizing assistance for those at\ngreatest risk over optimizing aggregate outcomes. This paper examines the\nwelfare impacts of prediction in equity-driven contexts, and how they compare\nto other policy levers, such as expanding bureaucratic capacity. Through\nmathematical models and a real-world case study on long-term unemployment\namongst German residents, we develop a comprehensive understanding of the\nrelative effectiveness of prediction in surfacing the worst-off. Our findings\nprovide clear analytical frameworks and practical, data-driven tools that\nempower policymakers to make principled decisions when designing these systems.\n","authors":["Unai Fischer-Abaigar","Christoph Kern","Juan Carlos Perdomo"],"pdf_url":"https://arxiv.org/pdf/2501.19334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08893v2","updated":"2025-01-31T17:27:39Z","published":"2024-10-11T15:10:40Z","title":"Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and\n  Parameter Efficient","summary":"  Model-based reinforcement learning (RL) offers a solution to the data\ninefficiency that plagues most model-free RL algorithms. However, learning a\nrobust world model often demands complex and deep architectures, which are\nexpensive to compute and train. Within the world model, dynamics models are\nparticularly crucial for accurate predictions, and various dynamics-model\narchitectures have been explored, each with its own set of challenges.\nCurrently, recurrent neural network (RNN) based world models face issues such\nas vanishing gradients and difficulty in capturing long-term dependencies\neffectively. In contrast, use of transformers suffers from the well-known\nissues of self-attention mechanisms, where both memory and computational\ncomplexity scale as $O(n^2)$, with $n$ representing the sequence length.\n  To address these challenges we propose a state space model (SSM) based world\nmodel, specifically based on Mamba, that achieves $O(n)$ memory and\ncomputational complexity while effectively capturing long-term dependencies and\nfacilitating the use of longer training sequences efficiently. We also\nintroduce a novel sampling method to mitigate the suboptimality caused by an\nincorrect world model in the early stages of training, combining it with the\naforementioned technique to achieve a normalised score comparable to other\nstate-of-the-art model-based RL algorithms using only a 7 million trainable\nparameter world model. This model is accessible and can be trained on an\noff-the-shelf laptop. Our code is available at\nhttps://github.com/realwenlongwang/Drama.git\n","authors":["Wenlong Wang","Ivana Dusparic","Yucheng Shi","Ke Zhang","Vinny Cahill"],"pdf_url":"https://arxiv.org/pdf/2410.08893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19328v1","updated":"2025-01-31T17:26:06Z","published":"2025-01-31T17:26:06Z","title":"Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation","summary":"  With the rise in global greenhouse gas emissions, accurate large-scale tree\ncanopy height maps are essential for understanding forest structure, estimating\nabove-ground biomass, and monitoring ecological disruptions. To this end, we\npresent a novel approach to generate large-scale, high-resolution canopy height\nmaps over time. Our model accurately predicts canopy height over multiple years\ngiven Sentinel-2 time series satellite data. Using GEDI LiDAR data as the\nground truth for training the model, we present the first 10m resolution\ntemporal canopy height map of the European continent for the period 2019-2022.\nAs part of this product, we also offer a detailed canopy height map for 2020,\nproviding more precise estimates than previous studies. Our pipeline and the\nresulting temporal height map are publicly available, enabling comprehensive\nlarge-scale monitoring of forests and, hence, facilitating future research and\necological analyses. For an interactive viewer, see\nhttps://europetreemap.projects.earthengine.app/view/temporalcanopyheight.\n","authors":["Jan Pauls","Max Zimmer","Berkant Turan","Sassan Saatchi","Philippe Ciais","Sebastian Pokutta","Fabian Gieseke"],"pdf_url":"https://arxiv.org/pdf/2501.19328v1.pdf","comment":"9 pages main paper, 5 pages references and appendix, 8 figures, 5\n  tables"},{"id":"http://arxiv.org/abs/2501.19321v1","updated":"2025-01-31T17:16:45Z","published":"2025-01-31T17:16:45Z","title":"Language Bias in Self-Supervised Learning For Automatic Speech\n  Recognition","summary":"  Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.\n","authors":["Edward Storey","Naomi Harte","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2501.19321v1.pdf","comment":"Accepted to Speech and Language Technology Workshop (SLT) 2024\n  accessible on IEEE Xplore"},{"id":"http://arxiv.org/abs/2402.10164v2","updated":"2025-01-31T17:16:17Z","published":"2024-02-15T18:09:41Z","title":"Random features and polynomial rules","summary":"  Random features models play a distinguished role in the theory of deep\nlearning, describing the behavior of neural networks close to their\ninfinite-width limit. In this work, we present a thorough analysis of the\ngeneralization performance of random features models for generic supervised\nlearning problems with Gaussian data. Our approach, built with tools from the\nstatistical mechanics of disordered systems, maps the random features model to\nan equivalent polynomial model, and allows us to plot average generalization\ncurves as functions of the two main control parameters of the problem: the\nnumber of random features $N$ and the size $P$ of the training set, both\nassumed to scale as powers in the input dimension $D$. Our results extend the\ncase of proportional scaling between $N$, $P$ and $D$. They are in accordance\nwith rigorous bounds known for certain particular learning tasks and are in\nquantitative agreement with numerical experiments performed over many order of\nmagnitudes of $N$ and $P$. We find good agreement also far from the asymptotic\nlimits where $D\\to \\infty$ and at least one between $P/D^K$, $N/D^L$ remains\nfinite.\n","authors":["Fabián Aguirre-López","Silvio Franz","Mauro Pastore"],"pdf_url":"https://arxiv.org/pdf/2402.10164v2.pdf","comment":"11 pages + appendix, 4 figures"},{"id":"http://arxiv.org/abs/2410.03000v2","updated":"2025-01-31T17:12:39Z","published":"2024-10-03T21:20:46Z","title":"Towards Universal Certified Robustness with Multi-Norm Training","summary":"  Existing certified training methods can only train models to be robust\nagainst a certain perturbation type (e.g. $l_\\infty$ or $l_2$). However, an\n$l_\\infty$ certifiably robust model may not be certifiably robust against $l_2$\nperturbation (and vice versa) and also has low robustness against other\nperturbations (e.g. geometric and patch transformation). By constructing a\ntheoretical framework to analyze and mitigate the tradeoff, we propose the\nfirst multi-norm certified training framework \\textbf{CURE}, consisting of\nseveral multi-norm certified training methods, to attain better \\emph{union\nrobustness} when training from scratch or fine-tuning a pre-trained certified\nmodel. Inspired by our theoretical findings, we devise bound alignment and\nconnect natural training with certified training for better union robustness.\nCompared with SOTA-certified training, \\textbf{CURE} improves union robustness\nto $32.0\\%$ on MNIST, $25.8\\%$ on CIFAR-10, and $10.6\\%$ on TinyImagenet across\ndifferent epsilon values. It leads to better generalization on a diverse set of\nchallenging unseen geometric and patch perturbations to $6.8\\%$ and $16.0\\%$ on\nCIFAR-10. Overall, our contributions pave a path towards \\textit{universal\ncertified robustness}.\n","authors":["Enyi Jiang","David S. Cheung","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2410.03000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19309v1","updated":"2025-01-31T17:09:53Z","published":"2025-01-31T17:09:53Z","title":"Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model\n  Alignment","summary":"  The performance of large language models (LLMs) is closely linked to their\nunderlying size, leading to ever-growing networks and hence slower inference.\nSpeculative decoding has been proposed as a technique to accelerate\nautoregressive generation, leveraging a fast draft model to propose candidate\ntokens, which are then verified in parallel based on their likelihood under the\ntarget model. While this approach guarantees to reproduce the target output, it\nincurs a substantial penalty: many high-quality draft tokens are rejected, even\nwhen they represent objectively valid continuations. Indeed, we show that even\npowerful draft models such as GPT-4o, as well as human text cannot achieve high\nacceptance rates under the standard verification scheme. This severely limits\nthe speedup potential of current speculative decoding methods, as an early\nrejection becomes overwhelmingly likely when solely relying on alignment of\ndraft and target.\n  We thus ask the following question: Can we adapt verification to recognize\ncorrect, but non-aligned replies? To this end, we draw inspiration from the\nLLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers\nin a versatile way. We carefully design a dataset to elicit the same capability\nin the target model by training a compact module on top of the embeddings to\nproduce ``judgements\" of the current continuation. We showcase our strategy on\nthe Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over\nLlama-405B, while maintaining its quality on a large range of benchmarks. These\nbenefits remain present even in optimized inference frameworks, where our\nmethod reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B\non 2 and 8 H100s respectively.\n","authors":["Gregor Bachmann","Sotiris Anagnostidis","Albert Pumarola","Markos Georgopoulos","Artsiom Sanakoyeu","Yuming Du","Edgar Schönfeld","Ali Thabet","Jonas Kohler"],"pdf_url":"https://arxiv.org/pdf/2501.19309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11647v2","updated":"2025-01-31T17:02:52Z","published":"2023-09-20T21:23:52Z","title":"Potential and limitations of random Fourier features for dequantizing\n  quantum machine learning","summary":"  Quantum machine learning is arguably one of the most explored applications of\nnear-term quantum devices. Much focus has been put on notions of variational\nquantum machine learning where parameterized quantum circuits (PQCs) are used\nas learning models. These PQC models have a rich structure which suggests that\nthey might be amenable to efficient dequantization via random Fourier features\n(RFF). In this work, we establish necessary and sufficient conditions under\nwhich RFF does indeed provide an efficient dequantization of variational\nquantum machine learning for regression. We build on these insights to make\nconcrete suggestions for PQC architecture design, and to identify structures\nwhich are necessary for a regression problem to admit a potential quantum\nadvantage via PQC based optimization.\n","authors":["Ryan Sweke","Erik Recio","Sofiene Jerbi","Elies Gil-Fuster","Bryce Fuller","Jens Eisert","Johannes Jakob Meyer"],"pdf_url":"https://arxiv.org/pdf/2309.11647v2.pdf","comment":"38 pages, 5 figures. Many clarifying figures added to this version.\n  Comments and feedback welcome. Now accepted in Quantum"},{"id":"http://arxiv.org/abs/2410.02675v3","updated":"2025-01-31T17:00:03Z","published":"2024-10-03T17:02:21Z","title":"FAN: Fourier Analysis Networks","summary":"  Despite the remarkable successes of general-purpose neural networks, such as\nMLPs and Transformers, we find that they exhibit notable shortcomings in\nmodeling and reasoning about periodic phenomena, achieving only marginal\nperformance within the training domain and failing to generalize effectively to\nout-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature and\nscience. Therefore, neural networks should be equipped with the essential\nability to model and handle periodicity. In this work, we propose FAN, a novel\ngeneral-purpose neural network that offers broad applicability similar to MLP\nwhile effectively addressing periodicity modeling challenges. Periodicity is\nnaturally integrated into FAN's structure and computational processes by\nintroducing the Fourier Principle. Unlike existing Fourier-based networks,\nwhich possess particular periodicity modeling abilities but are typically\ndesigned for specific tasks, our approach maintains the general-purpose\nmodeling capability. Therefore, FAN can seamlessly replace MLP in various model\narchitectures with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the superiority of FAN in periodicity modeling tasks and the\neffectiveness and generalizability of FAN across a range of real-world tasks,\ne.g., symbolic formula representation, time series forecasting, language\nmodeling, and image recognition.\n","authors":["Yihong Dong","Ge Li","Yongding Tao","Xue Jiang","Kechi Zhang","Jia Li","Jinliang Deng","Jing Su","Jun Zhang","Jingjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.02675v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19300v1","updated":"2025-01-31T16:56:18Z","published":"2025-01-31T16:56:18Z","title":"Offline Learning for Combinatorial Multi-armed Bandits","summary":"  The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.\n","authors":["Xutong Liu","Xiangxiang Dai","Jinhang Zuo","Siwei Wang","Carlee-Joe Wong","John C. S. Lui","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.19300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19298v1","updated":"2025-01-31T16:55:43Z","published":"2025-01-31T16:55:43Z","title":"Synthetic User Behavior Sequence Generation with Large Language Models\n  for Smart Homes","summary":"  In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance.\n","authors":["Zhiyao Xu","Dan Zhao","Qingsong Zou","Jingyu Xiao","Yong Jiang","Zhenhui Yuan","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2501.19298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19287v1","updated":"2025-01-31T16:48:38Z","published":"2025-01-31T16:48:38Z","title":"Differentially Private In-context Learning via Sampling Few-shot Mixed\n  with Zero-shot Outputs","summary":"  In-context learning (ICL) has shown promising improvement in downstream task\nadaptation of LLMs by augmenting prompts with relevant input-output examples\n(demonstrations). However, the ICL demonstrations can contain privacy-sensitive\ninformation, which can be leaked and/or regurgitated by the LLM output.\nDifferential Privacy (DP), a widely adopted privacy safeguard, has emerged to\nmitigate this privacy leakage, with recent work demonstrating strong\nprivacy-utility tradeoffs in classification tasks for ICL. However, generation\ntasks for ICL are challenging due to the high-dimensional output space of\nopen-ended generation. To this end, we propose $\\texttt{dps-mozo}$,\nDifferentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a\ndecoding framework that generates DP text by sampling from the product of\nmultiple one-shot outputs mixed with a zero-shot output. This mixing\neffectively reduces the amount of information that can be leaked by each\ndemonstration. By utilizing the inherent randomness in sampling from the mixed\ndistributions, we can achieve DP without adding noise, thereby improving the\nprivacy-utility tradeoff. Our experimental evaluations show $\\texttt{dps-mozo}$\ncan achieve a strong privacy guarantee, $\\epsilon=2$, with minimal utility\ndegradation compared to non-private few-shot learning, $\\textbf{0.3}$% ROUGE-L\nF1 score decrease on the SAMSum dataset with Gemma 2 2B.\n","authors":["James Flemings","Haosheng Gan","Hongyi Li","Meisam Razaviyayn","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2501.19287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09133v2","updated":"2025-01-31T16:48:37Z","published":"2024-10-11T15:10:20Z","title":"MVG-CRPS: A Robust Loss Function for Multivariate Probabilistic\n  Forecasting","summary":"  Multivariate Gaussian (MVG) distributions are central to modeling correlated\ncontinuous variables in probabilistic forecasting. Neural forecasting models\ntypically parameterize the mean vector and covariance matrix of the\ndistribution using neural networks, optimizing with the log-score (negative\nlog-likelihood) as the loss function. However, the sensitivity of the log-score\nto outliers can lead to significant errors in the presence of anomalies.\nDrawing on the continuous ranked probability score (CRPS) for univariate\ndistributions, we propose MVG-CRPS, a strictly proper scoring rule for MVG\ndistributions. MVG-CRPS admits a closed-form expression in terms of neural\nnetwork outputs, thereby integrating seamlessly into deep learning frameworks.\nExperiments on real-world datasets across multivariate autoregressive and\nunivariate sequence-to-sequence (Seq2Seq) forecasting tasks show that MVG-CRPS\nimproves robustness, accuracy, and uncertainty quantification in probabilistic\nforecasting.\n","authors":["Vincent Zhihao Zheng","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.09133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18651v2","updated":"2025-01-31T16:48:19Z","published":"2024-06-26T18:00:03Z","title":"Contraction of Private Quantum Channels and Private Quantum Hypothesis\n  Testing","summary":"  A quantum generalized divergence by definition satisfies the data-processing\ninequality; as such, the relative decrease in such a divergence under the\naction of a quantum channel is at most one. This relative decrease is formally\nknown as the contraction coefficient of the channel and the divergence.\nInterestingly, there exist combinations of channels and divergences for which\nthe contraction coefficient is strictly less than one. Furthermore,\nunderstanding the contraction coefficient is fundamental for the study of\nstatistical tasks under privacy constraints. To this end, here we establish\nupper bounds on contraction coefficients for the hockey-stick divergence under\nprivacy constraints, where privacy is quantified with respect to the quantum\nlocal differential privacy (QLDP) framework, and we fully characterize the\ncontraction coefficient for the trace distance under privacy constraints. With\nthe machinery developed, we also determine an upper bound on the contraction of\nboth the Bures distance and quantum relative entropy relative to the normalized\ntrace distance, under QLDP constraints. Next, we apply our findings to\nestablish bounds on the sample complexity of quantum hypothesis testing under\nprivacy constraints. Furthermore, we study various scenarios in which the\nsample complexity bounds are tight, while providing order-optimal quantum\nchannels that achieve those bounds. Lastly, we show how private quantum\nchannels provide fairness and Holevo information stability in quantum learning\nsettings.\n","authors":["Theshani Nuradha","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2406.18651v2.pdf","comment":"36 pages; See independent work titled \"Sample Complexity of Locally\n  Differentially Private Quantum Hypothesis Testing\" by Hao-Chung Cheng,\n  Christoph Hirche, and Cambyse Rouz\\'e"},{"id":"http://arxiv.org/abs/2501.19285v1","updated":"2025-01-31T16:48:16Z","published":"2025-01-31T16:48:16Z","title":"OneBatchPAM: A Fast and Frugal K-Medoids Algorithm","summary":"  This paper proposes a novel k-medoids approximation algorithm to handle\nlarge-scale datasets with reasonable computational time and memory complexity.\nWe develop a local-search algorithm that iteratively improves the medoid\nselection based on the estimation of the k-medoids objective. A single batch of\nsize m << n provides the estimation, which reduces the required memory size and\nthe number of pairwise dissimilarities computations to O(mn), instead of O(n^2)\ncompared to most k-medoids baselines. We obtain theoretical results\nhighlighting that a batch of size m = O(log(n)) is sufficient to guarantee,\nwith strong probability, the same performance as the original local-search\nalgorithm. Multiple experiments conducted on real datasets of various sizes and\ndimensions show that our algorithm provides similar performances as\nstate-of-the-art methods such as FasterPAM and BanditPAM++ with a drastically\nreduced running time.\n","authors":["Antoine de Mathelin","Nicolas Enrique Cecchi","François Deheeger","Mathilde Mougeot","Nicolas Vayatis"],"pdf_url":"https://arxiv.org/pdf/2501.19285v1.pdf","comment":"Paper accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.19283v1","updated":"2025-01-31T16:47:22Z","published":"2025-01-31T16:47:22Z","title":"Application of Generative Adversarial Network (GAN) for Synthetic\n  Training Data Creation to improve performance of ANN Classifier for\n  extracting Built-Up pixels from Landsat Satellite Imagery","summary":"  Training a neural network for pixel based classification task using low\nresolution Landsat images is difficult as the size of the training data is\nusually small due to less number of available pixels that represent a single\nclass without any mixing with other classes. Due to this scarcity of training\ndata, neural network may not be able to attain expected level of accuracy. This\nlimitation could be overcome using a generative network that aims to generate\nsynthetic data having the same distribution as the sample data with which it is\ntrained. In this work, we have proposed a methodology for improving the\nperformance of ANN classifier to identify built-up pixels in the Landsat$7$\nimage with the help of developing a simple GAN architecture that could generate\nsynthetic training pixels when trained using original set of sample built-up\npixels. To ensure that the marginal and joint distributions of all the bands\ncorresponding to the generated and original set of pixels are\nindistinguishable, non-parametric Kolmogorov Smirnov Test and Ball Divergence\nbased Equality of Distributions Test have been performed respectively. It has\nbeen observed that the overall accuracy and kappa coefficient of the ANN model\nfor built-up classification have continuously improved from $0.9331$ to\n$0.9983$ and $0.8277$ to $0.9958$ respectively, with the inclusion of generated\nsets of built-up pixels to the original one.\n","authors":["Amritendu Mukherjee","Dipanwita Sinha Mukherjee","Parthasarathy Ramachandran"],"pdf_url":"https://arxiv.org/pdf/2501.19283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01703v2","updated":"2025-01-31T16:47:16Z","published":"2024-11-03T22:19:20Z","title":"UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have revolutionized vision-language\nunderstanding but remain vulnerable to multimodal jailbreak attacks, where\nadversarial inputs are meticulously crafted to elicit harmful or inappropriate\nresponses. We propose UniGuard, a novel multimodal safety guardrail that\njointly considers the unimodal and cross-modal harmful signals. UniGuard trains\na multimodal guardrail to minimize the likelihood of generating harmful\nresponses in a toxic corpus. The guardrail can be seamlessly applied to any\ninput prompt during inference with minimal computational costs. Extensive\nexperiments demonstrate the generalizability of UniGuard across multiple\nmodalities, attack strategies, and multiple state-of-the-art MLLMs, including\nLLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust\ndefense mechanism maintains the models' overall vision-language understanding\ncapabilities.\n","authors":["Sejoon Oh","Yiqiao Jin","Megha Sharma","Donghyun Kim","Eric Ma","Gaurav Verma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2411.01703v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.19281v1","updated":"2025-01-31T16:43:57Z","published":"2025-01-31T16:43:57Z","title":"Statistical Physics of Deep Neural Networks: Generalization Capability,\n  Beyond the Infinite Width, and Feature Learning","summary":"  Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassing\nhuman performance. Yet their internal processes remain elusive, frequently\ndescribed as \"black boxes.\" While performance can be refined experimentally,\nachieving a fundamental grasp of their inner workings is still a challenge.\n  Statistical Mechanics has long tackled computational problems, and this\nthesis applies physics-based insights to understand DNNs via three\ncomplementary approaches.\n  First, by averaging over data, we derive an asymptotic bound on\ngeneralization that depends solely on the size of the last layer, rather than\non the total number of parameters -- revealing how deep architectures process\ninformation differently across layers.\n  Second, adopting a data-dependent viewpoint, we explore a finite-width\nthermodynamic limit beyond the infinite-width regime. This leads to: (i) a\nclosed-form expression for the generalization error in a finite-width\none-hidden-layer network (regression task); (ii) an approximate partition\nfunction for deeper architectures; and (iii) a link between deep networks in\nthis thermodynamic limit and Student's t-processes.\n  Finally, from a task-explicit perspective, we present a preliminary analysis\nof how DNNs interact with a controlled dataset, investigating whether they\ntruly internalize its structure -- collapsing to the teacher -- or merely\nmemorize it. By understanding when a network must learn data structure rather\nthan just memorize, it sheds light on fostering meaningful internal\nrepresentations.\n  In essence, this thesis leverages the synergy between Statistical Physics and\nMachine Learning to illuminate the inner behavior of DNNs.\n","authors":["Sebastiano Ariosto"],"pdf_url":"https://arxiv.org/pdf/2501.19281v1.pdf","comment":"PhD thesis (200 pages), divided into four separate chapters, each of\n  which can be read independently. Some of the material presented has\n  previously appeared in works available on arXiv under the following\n  identifiers: 2209.04882 and 2201.11022"},{"id":"http://arxiv.org/abs/2302.08913v6","updated":"2025-01-31T16:43:25Z","published":"2023-02-04T15:55:23Z","title":"Referential communication in heterogeneous communities of pre-trained\n  visual deep networks","summary":"  As large pre-trained image-processing neural networks are being embedded in\nautonomous agents such as self-driving cars or robots, the question arises of\nhow such systems can communicate with each other about the surrounding world,\ndespite their different architectures and training regimes. As a first step in\nthis direction, we systematically explore the task of referential communication\nin a community of heterogeneous state-of-the-art pre-trained visual networks,\nshowing that they can develop, in a self-supervised way, a shared protocol to\nrefer to a target object among a set of candidates. This shared protocol can\nalso be used, to some extent, to communicate about previously unseen object\ncategories of different granularity. Moreover, a visual network that was not\ninitially part of an existing community can learn the community's protocol with\nremarkable ease. Finally, we study, both qualitatively and quantitatively, the\nproperties of the emergent protocol, providing some evidence that it is\ncapturing high-level semantic features of objects.\n","authors":["Matéo Mahaut","Francesca Franzon","Roberto Dessì","Marco Baroni"],"pdf_url":"https://arxiv.org/pdf/2302.08913v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19279v1","updated":"2025-01-31T16:43:02Z","published":"2025-01-31T16:43:02Z","title":"S-VOTE: Similarity-based Voting for Client Selection in Decentralized\n  Federated Learning","summary":"  Decentralized Federated Learning (DFL) enables collaborative,\nprivacy-preserving model training without relying on a central server. This\ndecentralized approach reduces bottlenecks and eliminates single points of\nfailure, enhancing scalability and resilience. However, DFL also introduces\nchallenges such as suboptimal models with non-IID data distributions, increased\ncommunication overhead, and resource usage. Thus, this work proposes S-VOTE, a\nvoting-based client selection mechanism that optimizes resource usage and\nenhances model performance in federations with non-IID data conditions. S-VOTE\nconsiders an adaptive strategy for spontaneous local training that addresses\nparticipation imbalance, allowing underutilized clients to contribute without\nsignificantly increasing resource costs. Extensive experiments on benchmark\ndatasets demonstrate the S-VOTE effectiveness. More in detail, it achieves\nlower communication costs by up to 21%, 4-6% faster convergence, and improves\nlocal performance by 9-17% compared to baseline methods in some configurations,\nall while achieving a 14-24% energy consumption reduction. These results\nhighlight the potential of S-VOTE to address DFL challenges in heterogeneous\nenvironments.\n","authors":["Pedro Miguel Sánchez Sánchez","Enrique Tomás Martínez Beltrán","Chao Feng","Gérôme Bovet","Gregorio Martínez Pérez","Alberto Huertas Celdrán"],"pdf_url":"https://arxiv.org/pdf/2501.19279v1.pdf","comment":"Submitted to IJCNN"},{"id":"http://arxiv.org/abs/2501.19277v1","updated":"2025-01-31T16:42:29Z","published":"2025-01-31T16:42:29Z","title":"On Pareto Optimality for the Multinomial Logistic Bandit","summary":"  We provide a new online learning algorithm for tackling the Multinomial Logit\nBandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorial\nnature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-based\nmethod that achieves Pareto optimality by balancing regret minimization and\nestimation error of the assortment revenues and the MNL parameters. We develop\ntheoretical guarantees characterizing the tradeoff between regret and\nestimation error for the MNL-Bandit problem through information-theoretic\nbounds, and propose a modified UCB algorithm that incorporates forced\nexploration to improve parameter estimation accuracy while maintaining low\nregret. Our analysis sheds critical insights into how to optimally balance the\ncollected revenues and the treatment estimation in dynamic assortment\noptimization.\n","authors":["Jierui Zuo","Hanzhang Qin"],"pdf_url":"https://arxiv.org/pdf/2501.19277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09535v2","updated":"2025-01-31T16:41:21Z","published":"2024-05-15T17:45:34Z","title":"Restoring balance: principled under/oversampling of data for optimal\n  classification","summary":"  Class imbalance in real-world data poses a common bottleneck for machine\nlearning tasks, since achieving good generalization on under-represented\nexamples is often challenging. Mitigation strategies, such as under or\noversampling the data depending on their abundances, are routinely proposed and\ntested empirically, but how they should adapt to the data statistics remains\npoorly understood. In this work, we determine exact analytical expressions of\nthe generalization curves in the high-dimensional regime for linear classifiers\n(Support Vector Machines). We also provide a sharp prediction of the effects of\nunder/oversampling strategies depending on class imbalance, first and second\nmoments of the data, and the metrics of performance considered. We show that\nmixed strategies involving under and oversampling of data lead to performance\nimprovement. Through numerical experiments, we show the relevance of our\ntheoretical predictions on real datasets, on deeper architectures and with\nsampling strategies based on unsupervised probabilistic models.\n","authors":["Emanuele Loffredo","Mauro Pastore","Simona Cocco","Rémi Monasson"],"pdf_url":"https://arxiv.org/pdf/2405.09535v2.pdf","comment":"9 pages + appendix, 3 figures. Presented at ICML'24"},{"id":"http://arxiv.org/abs/2501.00701v2","updated":"2025-01-31T16:40:16Z","published":"2025-01-01T02:19:42Z","title":"ResKoopNet: Learning Koopman Representations for Complex Dynamics with\n  Spectral Residuals","summary":"  Analyzing long-term behaviors in high-dimensional nonlinear dynamical systems\nremains challenging, with the Koopman operator framework providing a powerful\nglobal linearization approach, though existing methods for approximating its\nspectral components often suffer from theoretical limitations and reliance on\npredefined dictionaries. While Residual Dynamic Mode Decomposition (ResDMD)\nintroduced the spectral residual to assess the accuracy of Koopman operator\napproximation, its only filters precomputed spectra, which prevents it from\nfully discovering the Koopman operator's complete spectral information (a\nlimitation sometimes referred to as the 'spectral inclusion' problem). We\nintroduce ResKoopNet (Residual-based Koopman-learning Network), a novel method\nthat addresses this limitation by explicitly minimizing the spectral residual\nto compute Koopman eigenpairs, which can identify a more precise and complete\nspectrum of the Koopman operator. This approach provides theoretical guarantees\nwhile maintaining computational adaptability through a neural network\nimplementation. Experiments on physical and biological systems demonstrate\nResKoopNet's superior accuracy in spectral approximation compared to existing\nmethods, particularly for systems with continuous spectra and high dimensional,\nwhich makes it as an effective tool for analyzing complex dynamical systems.\n","authors":["Yuanchao Xu","Kaidi Shao","Nikos Logothetis","Zhongwei Shen"],"pdf_url":"https://arxiv.org/pdf/2501.00701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15704v2","updated":"2025-01-31T16:40:09Z","published":"2023-09-27T14:46:10Z","title":"Deep Out-of-Distribution Uncertainty Quantification via Weight Entropy\n  Maximization","summary":"  This paper deals with uncertainty quantification and out-of-distribution\ndetection in deep learning using Bayesian and ensemble methods. It proposes a\npractical solution to the lack of prediction diversity observed recently for\nstandard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et\nal., 2021). Considering that this issue is mainly related to a lack of weight\ndiversity, we claim that standard methods sample in \"over-restricted\" regions\nof the weight space due to the use of \"over-regularization\" processes, such as\nweight decay and zero-mean centered Gaussian priors. We propose to solve the\nproblem by adopting the maximum entropy principle for the weight distribution,\nwith the underlying idea to maximize the weight diversity. Under this paradigm,\nthe epistemic uncertainty is described by the weight distribution of maximal\nentropy that produces neural networks \"consistent\" with the training\nobservations. Considering stochastic neural networks, a practical optimization\nis derived to build such a distribution, defined as a trade-off between the\naverage empirical risk and the weight distribution entropy. We develop a novel\nweight parameterization for the stochastic model, based on the singular value\ndecomposition of the neural network's hidden representations, which enables a\nlarge increase of the weight entropy for a small empirical risk penalization.\nWe provide both theoretical and numerical results to assess the efficiency of\nthe approach. In particular, the proposed algorithm appears in the top three\nbest methods in all configurations of an extensive out-of-distribution\ndetection benchmark including more than thirty competitors.\n","authors":["Antoine de Mathelin","François Deheeger","Mathilde Mougeot","Nicolas Vayatis"],"pdf_url":"https://arxiv.org/pdf/2309.15704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19271v1","updated":"2025-01-31T16:32:36Z","published":"2025-01-31T16:32:36Z","title":"Concept-Based Explainable Artificial Intelligence: Metrics and\n  Benchmarks","summary":"  Concept-based explanation methods, such as concept bottleneck models (CBMs),\naim to improve the interpretability of machine learning models by linking their\ndecisions to human-understandable concepts, under the critical assumption that\nsuch concepts can be accurately attributed to the network's feature space.\nHowever, this foundational assumption has not been rigorously validated, mainly\nbecause the field lacks standardised metrics and benchmarks to assess the\nexistence and spatial alignment of such concepts. To address this, we propose\nthree metrics: the concept global importance metric, the concept existence\nmetric, and the concept location metric, including a technique for visualising\nconcept activations, i.e., concept activation mapping. We benchmark post-hoc\nCBMs to illustrate their capabilities and challenges. Through qualitative and\nquantitative experiments, we demonstrate that, in many cases, even the most\nimportant concepts determined by post-hoc CBMs are not present in input images;\nmoreover, when they are present, their saliency maps fail to align with the\nexpected regions by either activating across an entire object or misidentifying\nrelevant concept-specific regions. We analyse the root causes of these\nlimitations, such as the natural correlation of concepts. Our findings\nunderscore the need for more careful application of concept-based explanation\ntechniques especially in settings where spatial interpretability is critical.\n","authors":["Halil Ibrahim Aysel","Xiaohao Cai","Adam Prugel-Bennett"],"pdf_url":"https://arxiv.org/pdf/2501.19271v1.pdf","comment":"17 pages it total, 8 main pages"},{"id":"http://arxiv.org/abs/2501.19266v1","updated":"2025-01-31T16:26:28Z","published":"2025-01-31T16:26:28Z","title":"Jackpot! Alignment as a Maximal Lottery","summary":"  Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions.\n","authors":["Roberto-Rafael Maura-Rivero","Marc Lanctot","Francesco Visin","Kate Larson"],"pdf_url":"https://arxiv.org/pdf/2501.19266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19265v1","updated":"2025-01-31T16:25:49Z","published":"2025-01-31T16:25:49Z","title":"Medical Semantic Segmentation with Diffusion Pretrain","summary":"  Recent advances in deep learning have shown that learning robust feature\nrepresentations is critical for the success of many computer vision tasks,\nincluding medical image segmentation. In particular, both transformer and\nconvolutional-based architectures have benefit from leveraging pretext tasks\nfor pretraining. However, the adoption of pretext tasks in 3D medical imaging\nhas been less explored and remains a challenge, especially in the context of\nlearning generalizable feature representations.\n  We propose a novel pretraining strategy using diffusion models with\nanatomical guidance, tailored to the intricacies of 3D medical image data. We\nintroduce an auxiliary diffusion process to pretrain a model that produce\ngeneralizable feature representations, useful for a variety of downstream\nsegmentation tasks. We employ an additional model that predicts 3D universal\nbody-part coordinates, providing guidance during the diffusion process and\nimproving spatial awareness in generated representations. This approach not\nonly aids in resolving localization inaccuracies but also enriches the model's\nability to understand complex anatomical structures.\n  Empirical validation on a 13-class organ segmentation task demonstrate the\neffectiveness of our pretraining technique. It surpasses existing restorative\npretraining methods in 3D medical image segmentation by $7.5\\%$, and is\ncompetitive with the state-of-the-art contrastive pretraining approach,\nachieving an average Dice coefficient of 67.8 in a non-linear evaluation\nscenario.\n","authors":["David Li","Anvar Kurmukov","Mikhail Goncharov","Roman Sokolov","Mikhail Belyaev"],"pdf_url":"https://arxiv.org/pdf/2501.19265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19264v1","updated":"2025-01-31T16:24:46Z","published":"2025-01-31T16:24:46Z","title":"mFollowIR: a Multilingual Benchmark for Instruction Following in\n  Retrieval","summary":"  Retrieval systems generally focus on web-style queries that are short and\nunderspecified. However, advances in language models have facilitated the\nnascent rise of retrieval models that can understand more complex queries with\ndiverse intents. However, these efforts have focused exclusively on English;\ntherefore, we do not yet understand how they work across languages. We\nintroduce mFollowIR, a multilingual benchmark for measuring\ninstruction-following ability in retrieval models. mFollowIR builds upon the\nTREC NeuCLIR narratives (or instructions) that span three diverse languages\n(Russian, Chinese, Persian) giving both query and instruction to the retrieval\nmodels. We make small changes to the narratives and isolate how well retrieval\nmodels can follow these nuanced changes. We present results for both\nmultilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong\ncross-lingual performance with English-based retrievers that trained using\ninstructions, but find a notable drop in performance in the multilingual\nsetting, indicating that more work is needed in developing data for\ninstruction-based multilingual retrievers.\n","authors":["Orion Weller","Benjamin Chang","Eugene Yang","Mahsa Yarmohammadi","Sam Barham","Sean MacAvaney","Arman Cohan","Luca Soldaini","Benjamin Van Durme","Dawn Lawrie"],"pdf_url":"https://arxiv.org/pdf/2501.19264v1.pdf","comment":"Accepted to ECIR 2025"},{"id":"http://arxiv.org/abs/2405.19420v3","updated":"2025-01-31T16:19:24Z","published":"2024-05-29T18:01:58Z","title":"Learning Human-Aligned Representations with Contrastive Learning and\n  Generative Similarity","summary":"  Humans rely on effective representations to learn from few examples and\nabstract useful information from sensory data. Inducing such representations in\nmachine learning models has been shown to improve their performance on various\nbenchmarks such as few-shot learning and robustness. However, finding effective\ntraining procedures to achieve that goal can be challenging as psychologically\nrich training data such as human similarity judgments are expensive to scale,\nand Bayesian models of human inductive biases are often intractable for\ncomplex, realistic domains. Here, we address this challenge by leveraging a\nBayesian notion of generative similarity whereby two data points are considered\nsimilar if they are likely to have been sampled from the same distribution.\nThis measure can be applied to complex generative processes, including\nprobabilistic programs. We incorporate generative similarity into a contrastive\nlearning objective to enable learning of embeddings that express human\ncognitive representations. We demonstrate the utility of our approach by\nshowing that it can be used to capture human-like representations of shape\nregularity, abstract Euclidean geometric concepts, and semantic hierarchies for\nnatural images.\n","authors":["Raja Marjieh","Sreejan Kumar","Declan Campbell","Liyi Zhang","Gianluca Bencomo","Jake Snell","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2405.19420v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19259v1","updated":"2025-01-31T16:17:03Z","published":"2025-01-31T16:17:03Z","title":"Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for\n  Autonomous Drone FlighT at the Edge","summary":"  The integration of human-intuitive interactions into autonomous systems has\nbeen limited. Traditional Natural Language Processing (NLP) systems struggle\nwith context and intent understanding, severely restricting human-robot\ninteraction. Recent advancements in Large Language Models (LLMs) have\ntransformed this dynamic, allowing for intuitive and high-level communication\nthrough speech and text, and bridging the gap between human commands and\nrobotic actions. Additionally, autonomous navigation has emerged as a central\nfocus in robotics research, with artificial intelligence (AI) increasingly\nbeing leveraged to enhance these systems. However, existing AI-based navigation\nalgorithms face significant challenges in latency-critical tasks where rapid\ndecision-making is critical. Traditional frame-based vision systems, while\neffective for high-level decision-making, suffer from high energy consumption\nand latency, limiting their applicability in real-time scenarios. Neuromorphic\nvision systems, combining event-based cameras and spiking neural networks\n(SNNs), offer a promising alternative by enabling energy-efficient, low-latency\nnavigation. Despite their potential, real-world implementations of these\nsystems, particularly on physical platforms such as drones, remain scarce. In\nthis work, we present Neuro-LIFT, a real-time neuromorphic navigation framework\nimplemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural\nlanguage processing, Neuro-LIFT translates human speech into high-level\nplanning commands which are then autonomously executed using event-based\nneuromorphic vision and physics-driven planning. Our framework demonstrates its\ncapabilities in navigating in a dynamic environment, avoiding obstacles, and\nadapting to human instructions in real-time.\n","authors":["Amogh Joshi","Sourav Sanyal","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2501.19259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01640v2","updated":"2025-01-31T16:12:56Z","published":"2024-06-30T20:47:15Z","title":"BADM: Batch ADMM for Deep Learning","summary":"  Stochastic gradient descent-based algorithms are widely used for training\ndeep neural networks but often suffer from slow convergence. To address the\nchallenge, we leverage the framework of the alternating direction method of\nmultipliers (ADMM) to develop a novel data-driven algorithm, called batch ADMM\n(BADM). The fundamental idea of the proposed algorithm is to split the training\ndata into batches, which is further divided into sub-batches where primal and\ndual variables are updated to generate global parameters through aggregation.\nWe evaluate the performance of BADM across various deep learning tasks,\nincluding graph modelling, computer vision, image generation, and natural\nlanguage processing. Extensive numerical experiments demonstrate that BADM\nachieves faster convergence and superior testing accuracy compared to other\nstate-of-the-art optimizers.\n","authors":["Ouya Wang","Shenglong Zhou","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2407.01640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19254v1","updated":"2025-01-31T16:10:50Z","published":"2025-01-31T16:10:50Z","title":"Linear $Q$-Learning Does Not Diverge: Convergence Rates to a Bounded Set","summary":"  $Q$-learning is one of the most fundamental reinforcement learning\nalgorithms. Previously, it is widely believed that $Q$-learning with linear\nfunction approximation (i.e., linear $Q$-learning) suffers from possible\ndivergence. This paper instead establishes the first $L^2$ convergence rate of\nlinear $Q$-learning to a bounded set. Notably, we do not make any modification\nto the original linear $Q$-learning algorithm, do not make any Bellman\ncompleteness assumption, and do not make any near-optimality assumption on the\nbehavior policy. All we need is an $\\epsilon$-softmax behavior policy with an\nadaptive temperature. The key to our analysis is the general result of\nstochastic approximations under Markovian noise with fast-changing transition\nfunctions. As a side product, we also use this general result to establish the\n$L^2$ convergence rate of tabular $Q$-learning with an $\\epsilon$-softmax\nbehavior policy, for which we rely on a novel pseudo-contraction property of\nthe weighted Bellman optimality operator.\n","authors":["Xinyu Liu","Zixuan Xie","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.19254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09536v2","updated":"2025-01-31T16:09:12Z","published":"2024-10-12T13:55:26Z","title":"TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning","summary":"  This work introduces Transformer-based Off-Policy Episodic Reinforcement\nLearning (TOP-ERL), a novel algorithm that enables off-policy updates in the\nERL framework. In ERL, policies predict entire action trajectories over\nmultiple time steps instead of single actions at every time step. These\ntrajectories are typically parameterized by trajectory generators such as\nMovement Primitives (MP), allowing for smooth and efficient exploration over\nlong horizons while capturing high-level temporal correlations. However, ERL\nmethods are often constrained to on-policy frameworks due to the difficulty of\nevaluating state-action values for entire action sequences, limiting their\nsample efficiency and preventing the use of more efficient off-policy\narchitectures. TOP-ERL addresses this shortcoming by segmenting long action\nsequences and estimating the state-action values for each segment using a\ntransformer-based critic architecture alongside an n-step return estimation.\nThese contributions result in efficient and stable training that is reflected\nin the empirical results conducted on sophisticated robot learning\nenvironments. TOP-ERL significantly outperforms state-of-the-art RL methods.\nThorough ablation studies additionally show the impact of key design choices on\nthe model performance.\n","authors":["Ge Li","Dong Tian","Hongyi Zhou","Xinkai Jiang","Rudolf Lioutikov","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2410.09536v2.pdf","comment":"Codebase: https://github.com/BruceGeLi/TOP_ERL_ICLR25. arXiv admin\n  note: text overlap with arXiv:2401.11437"},{"id":"http://arxiv.org/abs/2403.06833v3","updated":"2025-01-31T16:06:52Z","published":"2024-03-11T15:48:56Z","title":"Can LLMs Separate Instructions From Data? And What Do We Even Mean By\n  That?","summary":"  Instruction-tuned Large Language Models (LLMs) show impressive results in\nnumerous practical applications, but they lack essential safety features that\nare common in other areas of computer science, particularly an explicit\nseparation of instructions and data. This makes them vulnerable to\nmanipulations such as indirect prompt injections and generally unsuitable for\nsafety-critical tasks. Surprisingly, there is currently no established\ndefinition or benchmark to quantify this phenomenon. In this work, we close\nthis gap by introducing a formal measure for instruction-data separation and an\nempirical variant that is calculable from a model's outputs. We also present a\nnew dataset, SEP, that allows estimating the measure for real-world models. Our\nresults on various LLMs show that the problem of instruction-data separation is\nreal: all models fail to achieve high separation, and canonical mitigation\ntechniques, such as prompt engineering and fine-tuning, either fail to\nsubstantially improve separation or reduce model utility. The source code and\nSEP dataset are openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.\n","authors":["Egor Zverev","Sahar Abdelnabi","Soroush Tabesh","Mario Fritz","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2403.06833v3.pdf","comment":"Published as a conference paper at ICLR 2025, GitHub:\n  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 10 pages main\n  text, 30 pages in total"},{"id":"http://arxiv.org/abs/2405.13682v4","updated":"2025-01-31T16:05:32Z","published":"2024-05-22T14:25:02Z","title":"Unified Universality Theorem for Deep and Shallow\n  Joint-Group-Equivariant Machines","summary":"  We present a constructive universal approximation theorem for learning\nmachines equipped with joint-group-equivariant feature maps, called the\njoint-equivariant machines, based on the group representation theory.\n\"Constructive\" here indicates that the distribution of parameters is given in a\nclosed-form expression known as the ridgelet transform.\nJoint-group-equivariance encompasses a broad class of feature maps that\ngeneralize classical group-equivariance. Particularly, fully-connected networks\nare not group-equivariant but are joint-group-equivariant. Our main theorem\nalso unifies the universal approximation theorems for both shallow and deep\nnetworks. Until this study, the universality of deep networks has been shown in\na different manner from the universality of shallow networks, but our results\ndiscuss them on common ground. Now we can understand the approximation schemes\nof various learning machines in a unified manner. As applications, we show the\nconstructive universal approximation properties of four examples: depth-$n$\njoint-equivariant machine, depth-$n$ fully-connected network, depth-$n$\ngroup-convolutional network, and a new depth-$2$ network with quadratic forms\nwhose universality has not been known.\n","authors":["Sho Sonoda","Yuka Hashimoto","Isao Ishikawa","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2405.13682v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19247v1","updated":"2025-01-31T16:03:06Z","published":"2025-01-31T16:03:06Z","title":"Clustering in hyperbolic balls","summary":"  The idea of representations of the data in negatively curved manifolds\nrecently attracted a lot of attention and gave a rise to the new research\ndirection named {\\it hyperbolic machine learning} (ML). In order to unveil the\nfull potential of this new paradigm, efficient techniques for data analysis and\nstatistical modeling in hyperbolic spaces are necessary. In the present paper\nrigorous mathematical framework for clustering in hyperbolic spaces is\nestablished. First, we introduce the $k$-means clustering in hyperbolic balls,\nbased on the novel definition of barycenter. Second, we present the\nexpectation-maximization (EM) algorithm for learning mixtures of novel\nprobability distributions in hyperbolic balls. In such a way we lay the\nfoundation of unsupervised learning in hyperbolic spaces.\n","authors":["Vladimir Jaćimović","Aladin Crnkić"],"pdf_url":"https://arxiv.org/pdf/2501.19247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05113v2","updated":"2025-01-31T15:57:48Z","published":"2024-06-07T17:44:32Z","title":"LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets\n  and Models","summary":"  This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that\naddress the critical need for reliable guardrails in the era of large-scale\ndata and models. To this end, we establish a novel open framework, describing a\ncustomizable safety taxonomy, data preprocessing, augmentation, and training\nsetup. For teaching a VLM safeguard on safety, we further create a multimodal\nsafety dataset with high-quality human expert annotations, where each image is\nlabeled with a safety rating, category and rationale. We also employ advanced\naugmentations to support context-specific assessments. The resulting LlavaGuard\nmodels, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the\nsafety compliance of visual content against flexible policies. In comprehensive\nexperiments, LlavaGuard outperforms both state-of-the-art safeguards and VLMs\nin accuracy and in flexibly handling different policies. Additionally, we\ndemonstrate LlavaGuard's performance in two real-world applications:\nlarge-scale dataset annotation and moderation of text-to-image models. We make\nour entire framework publicly available, including the dataset and model\nweights.\n","authors":["Lukas Helff","Felix Friedrich","Manuel Brack","Kristian Kersting","Patrick Schramowski"],"pdf_url":"https://arxiv.org/pdf/2406.05113v2.pdf","comment":"Project page at\n  https://ml-research.github.io/human-centered-genai/projects/llavaguard/index.html"},{"id":"http://arxiv.org/abs/2501.19239v1","updated":"2025-01-31T15:53:14Z","published":"2025-01-31T15:53:14Z","title":"Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics","summary":"  We study decentralized multi-agent multi-armed bandits in fully heavy-tailed\nsettings, where clients communicate over sparse random graphs with heavy-tailed\ndegree distributions and observe heavy-tailed (homogeneous or heterogeneous)\nreward distributions with potentially infinite variance. The objective is to\nmaximize system performance by pulling the globally optimal arm with the\nhighest global reward mean across all clients. We are the first to address such\nfully heavy-tailed scenarios, which capture the dynamics and challenges in\ncommunication and inference among multiple clients in real-world systems. In\nhomogeneous settings, our algorithmic framework exploits hub-like structures\nunique to heavy-tailed graphs, allowing clients to aggregate rewards and reduce\nnoises via hub estimators when constructing UCB indices; under $M$ clients and\ndegree distributions with power-law index $\\alpha > 1$, our algorithm attains a\nregret bound (almost) of order $O(M^{1 -\\frac{1}{\\alpha}} \\log{T})$. Under\nheterogeneous rewards, clients synchronize by communicating with neighbors,\naggregating exchanged estimators in UCB indices; With our newly established\ninformation delay bounds on sparse random graphs, we prove a regret bound of\n$O(M \\log{T})$. Our results improve upon existing work, which only address\ntime-invariant connected graphs, or light-tailed dynamics in dense graphs and\nrewards.\n","authors":["Xingyu Wang","Mengfan Xu"],"pdf_url":"https://arxiv.org/pdf/2501.19239v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2405.14749v2","updated":"2025-01-31T15:53:01Z","published":"2024-05-23T16:16:58Z","title":"Policy Gradient Methods for Risk-Sensitive Distributional Reinforcement\n  Learning with Provable Convergence","summary":"  Risk-sensitive reinforcement learning (RL) is crucial for maintaining\nreliable performance in high-stakes applications. While traditional RL methods\naim to learn a point estimate of the random cumulative cost, distributional RL\n(DRL) seeks to estimate the entire distribution of it, which leads to a unified\nframework for handling different risk measures. However, developing policy\ngradient methods for risk-sensitive DRL is inherently more complex as it\ninvolves finding the gradient of a probability measure. This paper introduces a\nnew policy gradient method for risk-sensitive DRL with general coherent risk\nmeasures, where we provide an analytical form of the probability measure's\ngradient for any distribution. For practical use, we design a categorical\ndistributional policy gradient algorithm (CDPG) that approximates any\ndistribution by a categorical family supported on some fixed points. We further\nprovide a finite-support optimality guarantee and a finite-iteration\nconvergence guarantee under inexact policy evaluation and gradient estimation.\nThrough experiments on stochastic Cliffwalk and CartPole environments, we\nillustrate the benefits of considering a risk-sensitive setting in DRL.\n","authors":["Minheng Xiao","Xian Yu","Lei Ying"],"pdf_url":"https://arxiv.org/pdf/2405.14749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19237v1","updated":"2025-01-31T15:51:41Z","published":"2025-01-31T15:51:41Z","title":"DINAMO: Dynamic and INterpretable Anomaly MOnitoring for Large-Scale\n  Particle Physics Experiments","summary":"  Ensuring reliable data collection in large-scale particle physics experiments\ndemands Data Quality Monitoring (DQM) procedures to detect possible detector\nmalfunctions and preserve data integrity. Traditionally, this\nresource-intensive task has been handled by human shifters that struggle with\nfrequent changes in operational conditions. We present novel, interpretable,\nrobust, and scalable DQM algorithms designed to automate anomaly detection in\ntime-dependent settings. Our approach constructs evolving histogram templates\nwith built-in uncertainties, featuring both a statistical variant - extending\nthe classical Exponentially Weighted Moving Average (EWMA) - and a machine\nlearning (ML)-enhanced version that leverages a transformer encoder for\nimproved adaptability. Experimental validations on synthetic datasets\ndemonstrate the high accuracy, adaptability, and interpretability of these\nmethods, with the statistical variant being commissioned in the LHCb experiment\nat the Large Hadron Collider, underscoring its real-world impact. The code used\nin this study is available at https://github.com/ArseniiGav/DINAMO.\n","authors":["Arsenii Gavrikov","Julián García Pardiñas","Alberto Garfagnini"],"pdf_url":"https://arxiv.org/pdf/2501.19237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19234v1","updated":"2025-01-31T15:49:09Z","published":"2025-01-31T15:49:09Z","title":"Hourly Short Term Load Forecasting for Residential Buildings and Energy\n  Communities","summary":"  Electricity load consumption may be extremely complex in terms of profile\npatterns, as it depends on a wide range of human factors, and it is often\ncorrelated with several exogenous factors, such as the availability of\nrenewable energy and the weather conditions. The first goal of this paper is to\ninvestigate the performance of a large selection of different types of\nforecasting models in predicting the electricity load consumption within the\nshort time horizon of a day or few hours ahead. Such forecasts may be rather\nuseful for the energy management of individual residential buildings or small\nenergy communities. In particular, we introduce persistence models, standard\nauto-regressive-based machine learning models, and more advanced deep learning\nmodels. The second goal of this paper is to introduce two alternative modeling\napproaches that are simpler in structure while they take into account domain\nspecific knowledge, as compared to the previously mentioned black-box modeling\ntechniques. In particular, we consider the persistence-based auto-regressive\nmodel (PAR) and the seasonal persistence-based regressive model (SPR), priorly\nintroduced by the authors. In this paper, we specifically tailor these models\nto accommodate the generation of hourly forecasts. The introduced models and\nthe induced comparative analysis extend prior work of the authors which was\nrestricted to day-ahead forecasts. We observed a 15-30% increase in the\nprediction accuracy of the newly introduced hourly-based forecasting models\nover existing approaches.\n","authors":["Aleksei Kychkin","Georgios C. Chasparis"],"pdf_url":"https://arxiv.org/pdf/2501.19234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08355v2","updated":"2025-01-31T15:41:43Z","published":"2024-10-10T20:19:35Z","title":"Metalic: Meta-Learning In-Context with Protein Language Models","summary":"  Predicting the biophysical and functional properties of proteins is essential\nfor in silico protein design. Machine learning has emerged as a promising\ntechnique for such prediction tasks. However, the relative scarcity of in vitro\nannotations means that these models often have little, or no, specific data on\nthe desired fitness prediction task. As a result of limited data, protein\nlanguage models (PLMs) are typically trained on general protein sequence\nmodeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness\nprediction. When no task data is available, the models make strong assumptions\nabout the correlation between the protein sequence likelihood and fitness\nscores. In contrast, we propose meta-learning over a distribution of standard\nfitness prediction tasks, and demonstrate positive transfer to unseen fitness\nprediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses\nin-context learning and fine-tuning, when data is available, to adapt to new\ntasks. Crucially, fine-tuning enables considerable generalization, even though\nit is not accounted for during meta-training. Our fine-tuned models achieve\nstrong results with 18 times fewer parameters than state-of-the-art models.\nMoreover, our method sets a new state-of-the-art in low-data settings on\nProteinGym, an established fitness-prediction benchmark. Due to data scarcity,\nwe believe meta-learning will play a pivotal role in advancing protein\nengineering.\n","authors":["Jacob Beck","Shikha Surana","Manus McAuliffe","Oliver Bent","Thomas D. Barrett","Juan Jose Garau Luis","Paul Duckworth"],"pdf_url":"https://arxiv.org/pdf/2410.08355v2.pdf","comment":"Published at The Thirteenth International Conference on Learning\n  Representations (ICLR 2025). Code is provided at\n  https://github.com/instadeepai/metalic"},{"id":"http://arxiv.org/abs/2501.19224v1","updated":"2025-01-31T15:31:01Z","published":"2025-01-31T15:31:01Z","title":"Fast exact recovery of noisy matrix from few entries: the infinity norm\n  approach","summary":"  The matrix recovery (completion) problem, a central problem in data science\nand theoretical computer science, is to recover a matrix $A$ from a relatively\nsmall sample of entries.\n  While such a task is impossible in general, it has been shown that one can\nrecover $A$ exactly in polynomial time, with high probability, from a random\nsubset of entries, under three (basic and necessary) assumptions: (1) the rank\nof $A$ is very small compared to its dimensions (low rank), (2) $A$ has\ndelocalized singular vectors (incoherence), and (3) the sample size is\nsufficiently large.\n  There are many different algorithms for the task, including convex\noptimization by Candes, Tao and Recht (2009), alternating projection by Hardt\nand Wooters (2014) and low rank approximation with gradient descent by\nKeshavan, Montanari and Oh (2009, 2010).\n  In applications, it is more realistic to assume that data is noisy. In this\ncase, these approaches provide an approximate recovery with small root mean\nsquare error. However, it is hard to transform such approximate recovery to an\nexact one.\n  Recently, results by Abbe et al. (2017) and Bhardwaj et al. (2023) concerning\napproximation in the infinity norm showed that we can achieve exact recovery\neven in the noisy case, given that the ground matrix has bounded precision.\nBeyond the three basic assumptions above, they required either the condition\nnumber of $A$ is small (Abbe et al.) or the gap between consecutive singular\nvalues is large (Bhardwaj et al.).\n  In this paper, we remove these extra spectral assumptions. As a result, we\nobtain a simple algorithm for exact recovery in the noisy case, under only\nthree basic assumptions. This is the first such algorithm. To analyse the\nalgorithm, we introduce a contour integration argument which is totally\ndifferent from all previous methods and may be of independent interest.\n","authors":["BaoLinh Tran","Van Vu"],"pdf_url":"https://arxiv.org/pdf/2501.19224v1.pdf","comment":"56 pages, 1 figure"},{"id":"http://arxiv.org/abs/2501.19223v1","updated":"2025-01-31T15:30:14Z","published":"2025-01-31T15:30:14Z","title":"Through the Looking Glass: LLM-Based Analysis of AR/VR Android\n  Applications Privacy Policies","summary":"  \\begin{abstract} This paper comprehensively analyzes privacy policies in\nAR/VR applications, leveraging BERT, a state-of-the-art text classification\nmodel, to evaluate the clarity and thoroughness of these policies. By comparing\nthe privacy policies of AR/VR applications with those of free and premium\nwebsites, this study provides a broad perspective on the current state of\nprivacy practices within the AR/VR industry. Our findings indicate that AR/VR\napplications generally offer a higher percentage of positive segments than free\ncontent but lower than premium websites. The analysis of highlighted segments\nand words revealed that AR/VR applications strategically emphasize critical\nprivacy practices and key terms. This enhances privacy policies' clarity and\neffectiveness.\n","authors":["Abdulaziz Alghamdi","David Mohaisen"],"pdf_url":"https://arxiv.org/pdf/2501.19223v1.pdf","comment":"7 pages; appeared in ICMLA 2024"},{"id":"http://arxiv.org/abs/2412.20553v2","updated":"2025-01-31T15:29:13Z","published":"2024-12-29T18:59:01Z","title":"Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD","summary":"  Recent findings by Cohen et al., 2021, demonstrate that when training neural\nnetworks with full-batch gradient descent with a step size of $\\eta$, the\nlargest eigenvalue $\\lambda_{\\max}$ of the full-batch Hessian consistently\nstabilizes at $\\lambda_{\\max} = 2/\\eta$. These results have significant\nimplications for convergence and generalization. This, however, is not the case\nof mini-batch stochastic gradient descent (SGD), limiting the broader\napplicability of its consequences. We show that SGD trains in a different\nregime we term Edge of Stochastic Stability (EoSS). In this regime, what\nstabilizes at $2/\\eta$ is *Batch Sharpness*: the expected directional curvature\nof mini-batch Hessians along their corresponding stochastic gradients. As a\nconsequence $\\lambda_{\\max}$--which is generally smaller than Batch\nSharpness--is suppressed, aligning with the long-standing empirical observation\nthat smaller batches and larger step sizes favor flatter minima. We further\ndiscuss implications for mathematical modeling of SGD trajectories.\n","authors":["Arseniy Andreyev","Pierfrancesco Beneventano"],"pdf_url":"https://arxiv.org/pdf/2412.20553v2.pdf","comment":"35 pages, 26 figures"},{"id":"http://arxiv.org/abs/2410.06981v2","updated":"2025-01-31T15:27:10Z","published":"2024-10-09T15:18:57Z","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","summary":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.\n","authors":["Michael Lan","Philip Torr","Austin Meek","Ashkan Khakzar","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.06981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19216v1","updated":"2025-01-31T15:22:58Z","published":"2025-01-31T15:22:58Z","title":"\\underline{E2}Former: A Linear-time \\underline{E}fficient and\n  \\underline{E}quivariant Trans\\underline{former} for Scalable Molecular\n  Modeling","summary":"  Equivariant Graph Neural Networks (EGNNs) have demonstrated significant\nsuccess in modeling microscale systems, including those in chemistry, biology\nand materials science. However, EGNNs face substantial computational challenges\ndue to the high cost of constructing edge features via spherical tensor\nproducts, making them impractical for large-scale systems. To address this\nlimitation, we introduce E2Former, an equivariant and efficient transformer\narchitecture that incorporates the Wigner $6j$ convolution (Wigner $6j$ Conv).\nBy shifting the computational burden from edges to nodes, the Wigner $6j$ Conv\nreduces the complexity from $O(|\\mathcal{E}|)$ to $ O(| \\mathcal{V}|)$ while\npreserving both the model's expressive power and rotational equivariance. We\nshow that this approach achieves a 7x-30x speedup compared to conventional\n$\\mathrm{SO}(3)$ convolutions. Furthermore, our empirical results demonstrate\nthat the derived E2Former mitigates the computational challenges of existing\napproaches without compromising the ability to capture detailed geometric\ninformation. This development could suggest a promising direction for scalable\nand efficient molecular modeling.\n","authors":["Yunyang Li","Lin Huang","Zhihao Ding","Chu Wang","Xinran Wei","Han Yang","Zun Wang","Chang Liu","Yu Shi","Peiran Jin","Jia Zhang","Mark Gerstein","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2501.19216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19215v1","updated":"2025-01-31T15:21:54Z","published":"2025-01-31T15:21:54Z","title":"Strassen Attention: Unlocking Compositional Abilities in Transformers\n  Based on a New Lower Bound Method","summary":"  We propose a novel method to evaluate the theoretical limits of Transformers,\nallowing us to prove the first lower bounds against one-layer softmax\nTransformers with infinite precision. We establish those bounds for three tasks\nthat require advanced reasoning. The first task, Match3 (Sanford et al., 2023),\nrequires looking at all triples of positions. The second and third tasks\naddress compositionality-based reasoning: one is composition of functions (Peng\net al., 2024) and the other is composition of binary relations. We formally\nprove the inability of one-layer softmax Transformers to solve any of these\ntasks. In an attempt to overcome these limitations, we introduce Strassen\nattention and prove that with this mechanism a one-layer Transformer can in\nprinciple solve all these tasks. We also show that it enjoys sub-cubic\nrunning-time complexity, making it more scalable than similar previously\nproposed mechanisms, such as higher-order attention (Sanford et al., 2023). To\ncomplement our theoretical findings, we experimentally studied Strassen\nattention and compared it against standard (Vaswani et al, 2017), higher-order\nattention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021).\nOur results help to disentangle all these attention mechanisms, highlighting\ntheir strengths and limitations. In particular, Strassen attention outperforms\nstandard attention significantly on all the tasks. Altogether, understanding\nthe theoretical limitations can guide research towards scalable attention\nmechanisms that improve the reasoning abilities of Transformers.\n","authors":["Alexander Kozachinskiy","Felipe Urrutia","Hector Jimenez","Tomasz Steifer","Germán Pizarro","Matías Fuentes","Francisco Meza","Cristian Buc","Cristóbal Rojas"],"pdf_url":"https://arxiv.org/pdf/2501.19215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19214v1","updated":"2025-01-31T15:18:52Z","published":"2025-01-31T15:18:52Z","title":"A single-loop SPIDER-type stochastic subgradient method for\n  expectation-constrained nonconvex nonsmooth optimization","summary":"  Many real-world problems, such as those with fairness constraints, involve\ncomplex expectation constraints and large datasets, necessitating the design of\nefficient stochastic methods to solve them. Most existing research focuses on\ncases with no {constraint} or easy-to-project constraints or deterministic\nconstraints. In this paper, we consider nonconvex nonsmooth stochastic\noptimization problems with expectation constraints, for which we build a novel\nexact penalty model. We first show the relationship between the penalty model\nand the original problem. Then on solving the penalty problem, we present a\nsingle-loop SPIDER-type stochastic subgradient method, which utilizes the\nsubgradients of both the objective and constraint functions, as well as the\nconstraint function value at each iteration. Under certain regularity\nconditions (weaker than Slater-type constraint qualification or strong\nfeasibility assumed in existing works), we establish an iteration complexity\nresult of $O(\\epsilon^{-4})$ to reach a near-$\\epsilon$ stationary point of the\npenalized problem in expectation, matching the lower bound for such tasks.\nBuilding on the exact penalization, an $(\\epsilon,\\epsilon)$-KKT point of the\noriginal problem is obtained. For a few scenarios, our complexity of either the\n{objective} sample subgradient or the constraint sample function values can be\nlower than the state-of-the-art results by a factor of $\\epsilon^{-2}$.\nMoreover, on solving two fairness-constrained problems, our method is\nsignificantly (up to 466 times) faster than the state-of-the-art algorithms,\nincluding switching subgradient method and inexact proximal point methods.\n","authors":["Wei Liu","Yangyang Xu"],"pdf_url":"https://arxiv.org/pdf/2501.19214v1.pdf","comment":"Key word: stochastic, subgradient, expectation constraints, weakly\n  convex, fairness constrained classification"},{"id":"http://arxiv.org/abs/2501.19208v1","updated":"2025-01-31T15:16:02Z","published":"2025-01-31T15:16:02Z","title":"Learning While Repositioning in On-Demand Vehicle Sharing Networks","summary":"  We consider a network inventory problem motivated by one-way, on-demand\nvehicle sharing services. Due to uncertainties in both demand and returns, as\nwell as a fixed number of rental units across an $n$-location network, the\nservice provider must periodically reposition vehicles to match supply with\ndemand spatially while minimizing costs. The optimal repositioning policy under\na general $n$-location network is intractable without knowing the optimal value\nfunction. We introduce the best base-stock repositioning policy as a\ngeneralization of the classical inventory control policy to $n$ dimensions, and\nestablish its asymptotic optimality in two distinct limiting regimes under\ngeneral network structures. We present reformulations to efficiently compute\nthis best base-stock policy in an offline setting with pre-collected data.\n  In the online setting, we show that a natural Lipschitz-bandit approach\nachieves a regret guarantee of $\\widetilde{O}(T^{\\frac{n}{n+1}})$, which\nsuffers from the exponential dependence on $n$. We illustrate the challenges of\nlearning with censored data in networked systems through a regret lower bound\nanalysis and by demonstrating the suboptimality of alternative algorithmic\napproaches. Motivated by these challenges, we propose an Online Gradient\nRepositioning algorithm that relies solely on censored demand. Under a mild\ncost-structure assumption, we prove that it attains an optimal regret of\n$O(n^{2.5} \\sqrt{T})$, which matches the regret lower bound in $T$ and achieves\nonly polynomial dependence on $n$. The key algorithmic innovation involves\nproposing surrogate costs to disentangle intertemporal dependencies and\nleveraging dual solutions to find the gradient of policy change. Numerical\nexperiments demonstrate the effectiveness of our proposed methods.\n","authors":["Hansheng Jiang","Chunlin Sun","Zuo-Jun Max Shen","Shunan Jiang"],"pdf_url":"https://arxiv.org/pdf/2501.19208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19207v1","updated":"2025-01-31T15:15:08Z","published":"2025-01-31T15:15:08Z","title":"Learning Sheaf Laplacian Optimizing Restriction Maps","summary":"  The aim of this paper is to propose a novel framework to infer the sheaf\nLaplacian, including the topology of a graph and the restriction maps, from a\nset of data observed over the nodes of a graph. The proposed method is based on\nsheaf theory, which represents an important generalization of graph signal\nprocessing. The learning problem aims to find the sheaf Laplacian that\nminimizes the total variation of the observed data, where the variation over\neach edge is also locally minimized by optimizing the associated restriction\nmaps. Compared to alternative methods based on semidefinite programming, our\nsolution is significantly more numerically efficient, as all its fundamental\nsteps are resolved in closed form. The method is numerically tested on data\nconsisting of vectors defined over subspaces of varying dimensions at each\nnode. We demonstrate how the resulting graph is influenced by two key factors:\nthe cross-correlation and the dimensionality difference of the data residing on\nthe graph's nodes.\n","authors":["Leonardo Di Nino","Sergio Barbarossa","Paolo Di Lorenzo"],"pdf_url":"https://arxiv.org/pdf/2501.19207v1.pdf","comment":"Proc. 58th Annual Asilomar Conference on Signals, Systems, and\n  Computers (Asilomar), Pacific Grove, CA, Oct. 27 - Oct. 30, 2024"},{"id":"http://arxiv.org/abs/2501.19205v1","updated":"2025-01-31T15:14:25Z","published":"2025-01-31T15:14:25Z","title":"RIGNO: A Graph-based framework for robust and accurate operator learning\n  for PDEs on arbitrary domains","summary":"  Learning the solution operators of PDEs on arbitrary domains is challenging\ndue to the diversity of possible domain shapes, in addition to the often\nintricate underlying physics. We propose an end-to-end graph neural network\n(GNN) based neural operator to learn PDE solution operators from data on point\nclouds in arbitrary domains. Our multi-scale model maps data between\ninput/output point clouds by passing it through a downsampled regional mesh.\nMany novel elements are also incorporated to ensure resolution invariance and\ntemporal continuity. Our model, termed RIGNO, is tested on a challenging suite\nof benchmarks, composed of various time-dependent and steady PDEs defined on a\ndiverse set of domains. We demonstrate that RIGNO is significantly more\naccurate than neural operator baselines and robustly generalizes to unseen\nspatial resolutions and time instances.\n","authors":["Sepehr Mousavi","Shizheng Wen","Levi Lingsch","Maximilian Herde","Bogdan Raonić","Siddhartha Mishra"],"pdf_url":"https://arxiv.org/pdf/2501.19205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05072v2","updated":"2025-01-31T15:13:00Z","published":"2024-06-07T16:43:54Z","title":"Linearization Turns Neural Operators into Function-Valued Gaussian\n  Processes","summary":"  Neural operators generalize neural networks to learn mappings between\nfunction spaces from data. They are commonly used to learn solution operators\nof parametric partial differential equations (PDEs) or propagators of\ntime-dependent PDEs. However, to make them useful in high-stakes simulation\nscenarios, their inherent predictive error must be quantified reliably. We\nintroduce LUNO, a novel framework for approximate Bayesian uncertainty\nquantification in trained neural operators. Our approach leverages model\nlinearization to push (Gaussian) weight-space uncertainty forward to the neural\noperator's predictions. We show that this can be interpreted as a probabilistic\nversion of the concept of currying from functional programming, yielding a\nfunction-valued (Gaussian) random process belief. Our framework provides a\npractical yet theoretically sound way to apply existing Bayesian deep learning\nmethods such as the linearized Laplace approximation to neural operators. Just\nas the underlying neural operator, our approach is resolution-agnostic by\ndesign. The method adds minimal prediction overhead, can be applied post-hoc\nwithout retraining the network, and scales to large models and datasets. We\nevaluate these aspects in a case study on Fourier neural operators.\n","authors":["Emilia Magnani","Marvin Pförtner","Tobias Weber","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2406.05072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19201v1","updated":"2025-01-31T15:10:29Z","published":"2025-01-31T15:10:29Z","title":"Efficient Reasoning with Hidden Thinking","summary":"  Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.\n","authors":["Xuan Shen","Yizhou Wang","Xiangxi Shi","Yanzhi Wang","Pu Zhao","Jiuxiang Gu"],"pdf_url":"https://arxiv.org/pdf/2501.19201v1.pdf","comment":"Preprint version"},{"id":"http://arxiv.org/abs/2401.01879v2","updated":"2025-01-31T15:10:21Z","published":"2024-01-03T18:39:13Z","title":"Theoretical guarantees on the best-of-n alignment policy","summary":"  A simple and effective method for the inference-time alignment of generative\nmodels is the best-of-$n$ policy, where $n$ samples are drawn from a reference\npolicy, ranked based on a reward function, and the highest ranking one is\nselected. A commonly used analytical expression in the literature claims that\nthe KL divergence between the best-of-$n$ policy and the reference policy is\nequal to $\\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show\nthat it is an upper bound on the actual KL divergence. We also explore the\ntightness of this upper bound in different regimes, and propose a new estimator\nfor the KL divergence and empirically show that it provides a tight\napproximation. We also show that the win rate of the best-of-$n$ policy against\nthe reference policy is upper bounded by $n/(n+1)$ and derive bounds on the\ntightness of this characterization. We conclude with analyzing the tradeoffs\nbetween win rate and KL divergence of the best-of-$n$ alignment policy, which\ndemonstrate that very good tradeoffs are achievable with $n < 1000$.\n","authors":["Ahmad Beirami","Alekh Agarwal","Jonathan Berant","Alexander D'Amour","Jacob Eisenstein","Chirag Nagpal","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2401.01879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01643v3","updated":"2025-01-31T15:10:00Z","published":"2024-10-02T15:13:25Z","title":"Stable Offline Value Function Learning with Bisimulation-based\n  Representations","summary":"  In reinforcement learning, offline value function learning is the procedure\nof using an offline dataset to estimate the expected discounted return from\neach state when taking actions according to a fixed target policy. The\nstability of this procedure, i.e., whether it converges to its fixed-point,\ncritically depends on the representations of the state-action pairs. Poorly\nlearned representations can make value function learning unstable, or even\ndivergent. Therefore, it is critical to stabilize value function learning by\nexplicitly shaping the state-action representations. Recently, the class of\nbisimulation-based algorithms have shown promise in shaping representations for\ncontrol. However, it is still unclear if this class of methods can stabilize\nvalue function learning. In this work, we investigate this question and answer\nit affirmatively. We introduce a bisimulation-based algorithm called kernel\nrepresentations for offline policy evaluation (KROPE). KROPE uses a kernel to\nshape state-action representations such that state-action pairs that have\nsimilar immediate rewards and lead to similar next state-action pairs under the\ntarget policy also have similar representations. We show that KROPE: 1) learns\nstable representations and 2) leads to lower value error than baselines. Our\nanalysis provides new theoretical insight into the stability properties of\nbisimulation-based methods and suggests that practitioners can use these\nmethods for stable and accurate evaluation of offline reinforcement learning\nagents.\n","authors":["Brahma S. Pavse","Yudong Chen","Qiaomin Xie","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2410.01643v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.08272v3","updated":"2025-01-31T15:09:20Z","published":"2024-06-12T14:37:29Z","title":"Learning positional encodings in transformers depends on initialization","summary":"  The attention mechanism is central to the transformer's ability to capture\ncomplex dependencies between tokens of an input sequence. Key to the successful\napplication of the attention mechanism in transformers is its choice of\npositional encoding (PE). The PE provides essential information that\ndistinguishes the position and order amongst tokens in a sequence. Most prior\ninvestigations of PE effects on generalization were tailored to 1D input\nsequences, such as those presented in natural language, where adjacent tokens\n(e.g., words) are highly related. In contrast, many real world tasks involve\ndatasets with highly non-trivial positional arrangements, such as datasets\norganized in multiple spatial dimensions, or datasets for which ground truth\npositions are not known, such as in biological data. Here we study the\nimportance of learning accurate PE for problems which rely on a non-trivial\narrangement of input tokens. Critically, we find that the choice of\ninitialization of a learnable PE greatly influences its ability to learn\naccurate PEs that lead to enhanced generalization. We empirically demonstrate\nour findings in three experiments: 1) A 2D relational reasoning task; 2) A\nnonlinear stochastic network simulation; 3) A real world 3D neuroscience\ndataset, applying interpretability analyses to verify the learning of accurate\nPEs. Overall, we find that a learned PE initialized from a small-norm\ndistribution can 1) uncover interpretable PEs that mirror ground truth\npositions in multiple dimensions, and 2) lead to improved downstream\ngeneralization in empirical evaluations. Importantly, choosing an ill-suited PE\ncan be detrimental to both model interpretability and generalization. Together,\nour results illustrate the feasibility of learning identifiable and\ninterpretable PEs for enhanced generalization.\n","authors":["Takuya Ito","Luca Cocchi","Tim Klinger","Parikshit Ram","Murray Campbell","Luke Hearne"],"pdf_url":"https://arxiv.org/pdf/2406.08272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19200v1","updated":"2025-01-31T15:07:26Z","published":"2025-01-31T15:07:26Z","title":"A Variational Perspective on Generative Protein Fitness Optimization","summary":"  The goal of protein fitness optimization is to discover new protein variants\nwith enhanced fitness for a given use. The vast search space and the sparsely\npopulated fitness landscape, along with the discrete nature of protein\nsequences, pose significant challenges when trying to determine the gradient\ntowards configurations with higher fitness. We introduce Variational Latent\nGenerative Protein Optimization (VLGPO), a variational perspective on fitness\noptimization. Our method embeds protein sequences in a continuous latent space\nto enable efficient sampling from the fitness distribution and combines a\n(learned) flow matching prior over sequence mutations with a fitness predictor\nto guide optimization towards sequences with high fitness. VLGPO achieves\nstate-of-the-art results on two different protein benchmarks of varying\ncomplexity. Moreover, the variational design with explicit prior and likelihood\nfunctions offers a flexible plug-and-play framework that can be easily\ncustomized to suit various protein design tasks.\n","authors":["Lea Bogensperger","Dominik Narnhofer","Ahmed Allam","Konrad Schindler","Michael Krauthammer"],"pdf_url":"https://arxiv.org/pdf/2501.19200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17284v4","updated":"2025-01-31T15:04:34Z","published":"2024-11-26T10:13:39Z","title":"AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling","summary":"  Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.\n","authors":["Alexander Capstick","Rahul G. Krishnan","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2411.17284v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15844v2","updated":"2025-01-31T15:04:08Z","published":"2024-09-24T08:14:26Z","title":"Adaptive Learn-then-Test: Statistically Valid and Efficient\n  Hyperparameter Selection","summary":"  We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter\nselection procedure that provides finite-sample statistical guarantees on the\npopulation risk of AI models. Unlike the existing learn-then-test (LTT)\ntechnique, which relies on conventional p-value-based multiple hypothesis\ntesting (MHT), aLTT implements sequential data-dependent MHT with early\ntermination by leveraging e-processes. As a result, aLTT can reduce the number\nof testing rounds, making it particularly well-suited for scenarios in which\ntesting is costly or presents safety risks. Apart from maintaining statistical\nvalidity, in applications such as online policy selection for offline\nreinforcement learning and prompt engineering, aLTT is shown to achieve the\nsame performance as LTT while requiring only a fraction of the testing rounds.\n","authors":["Matteo Zecchin","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2409.15844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19195v1","updated":"2025-01-31T15:03:54Z","published":"2025-01-31T15:03:54Z","title":"Rethinking Early Stopping: Refine, Then Calibrate","summary":"  Machine learning classifiers often produce probabilistic predictions that are\ncritical for accurate and interpretable decision-making in various domains. The\nquality of these predictions is generally evaluated with proper losses like\ncross-entropy, which decompose into two components: calibration error assesses\ngeneral under/overconfidence, while refinement error measures the ability to\ndistinguish different classes. In this paper, we provide theoretical and\nempirical evidence that these two errors are not minimized simultaneously\nduring training. Selecting the best training epoch based on validation loss\nthus leads to a compromise point that is suboptimal for both calibration error\nand, most importantly, refinement error. To address this, we introduce a new\nmetric for early stopping and hyperparameter tuning that makes it possible to\nminimize refinement error during training. The calibration error is minimized\nafter training, using standard techniques. Our method integrates seamlessly\nwith any architecture and consistently improves performance across diverse\nclassification tasks.\n","authors":["Eugène Berta","David Holzmüller","Michael I. Jordan","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2501.19195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06504v2","updated":"2025-01-31T14:56:51Z","published":"2024-06-10T17:43:13Z","title":"Equivariant Neural Tangent Kernels","summary":"  Little is known about the training dynamics of equivariant neural networks,\nin particular how it compares to data augmented training of their\nnon-equivariant counterparts. Recently, neural tangent kernels (NTKs) have\nemerged as a powerful tool to analytically study the training dynamics of wide\nneural networks. In this work, we take an important step towards a theoretical\nunderstanding of training dynamics of equivariant models by deriving neural\ntangent kernels for a broad class of equivariant architectures based on group\nconvolutions. As a demonstration of the capabilities of our framework, we show\nan interesting relationship between data augmentation and group convolutional\nnetworks. Specifically, we prove that they share the same expected prediction\nat all training times and even off-manifold. In this sense, they have the same\ntraining dynamics. We demonstrate in numerical experiments that this still\nholds approximately for finite-width ensembles. By implementing equivariant\nNTKs for roto-translations in the plane ($G=C_{n}\\ltimes\\mathbb{R}^{2}$) and 3d\nrotations ($G=\\mathrm{SO}(3)$), we show that equivariant NTKs outperform their\nnon-equivariant counterparts as kernel predictors for histological image\nclassification and quantum mechanical property prediction.\n","authors":["Philipp Misof","Pan Kessel","Jan E. Gerken"],"pdf_url":"https://arxiv.org/pdf/2406.06504v2.pdf","comment":"16 pages + 20 pages appendices"},{"id":"http://arxiv.org/abs/2405.13846v3","updated":"2025-01-31T14:52:47Z","published":"2024-05-22T17:14:03Z","title":"Regression Trees Know Calculus","summary":"  Regression trees have emerged as a preeminent tool for solving real-world\nregression problems due to their ability to deal with nonlinearities,\ninteraction effects and sharp discontinuities. In this article, we rather study\nregression trees applied to well-behaved, differentiable functions, and\ndetermine the relationship between node parameters and the local gradient of\nthe function being approximated. We find a simple estimate of the gradient\nwhich can be efficiently computed using quantities exposed by popular tree\nlearning libraries. This allows the tools developed in the context of\ndifferentiable algorithms, like neural nets and Gaussian processes, to be\ndeployed to tree-based models. To demonstrate this, we study measures of model\nsensitivity defined in terms of integrals of gradients and demonstrate how to\ncompute them for regression trees using the proposed gradient estimates.\nQuantitative and qualitative numerical experiments reveal the capability of\ngradients estimated by regression trees to improve predictive analysis, solve\ntasks in uncertainty quantification, and provide interpretation of model\nbehavior.\n","authors":["Nathan Wycoff"],"pdf_url":"https://arxiv.org/pdf/2405.13846v3.pdf","comment":"Better math (asymptotic rate instead of just consistency) and\n  reorganization of the exposition"},{"id":"http://arxiv.org/abs/2410.07299v2","updated":"2025-01-31T14:50:26Z","published":"2024-10-09T17:09:30Z","title":"Towards Generalisable Time Series Understanding Across Domains","summary":"  Recent breakthroughs in natural language processing and computer vision,\ndriven by efficient pre-training on large datasets, have enabled foundation\nmodels to excel on a wide range of tasks. However, this potential has not yet\nbeen fully realised in time series analysis, as existing methods fail to\naddress the heterogeneity in large time series corpora. Prevalent in domains\nranging from medicine to finance, time series vary substantially in\ncharacteristics such as variate count, inter-variate relationships, temporal\npatterns, and sampling frequency. To address this, we introduce a novel\npre-training paradigm specifically designed to handle time series\nheterogeneity. We propose a tokeniser with learnable domain signatures, a dual\nmasking strategy, and a normalised cross-correlation loss, enabling our open\nmodel for general time series analysis (OTiS) to efficiently learn from large\ntime series corpora. Extensive benchmarking on diverse tasks, such as\nclassification, regression, and forecasting, demonstrates that OTiS outperforms\nstate-of-the-art baselines. Our code and pre-trained weights are available at\nhttps://github.com/oetu/otis.\n","authors":["Özgün Turgut","Philip Müller","Martin J. Menten","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2410.07299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19183v1","updated":"2025-01-31T14:46:30Z","published":"2025-01-31T14:46:30Z","title":"Position: Curvature Matrices Should Be Democratized via Linear Operators","summary":"  Structured large matrices are prevalent in machine learning. A particularly\nimportant class is curvature matrices like the Hessian, which are central to\nunderstanding the loss landscape of neural nets (NNs), and enable second-order\noptimization, uncertainty quantification, model pruning, data attribution, and\nmore. However, curvature computations can be challenging due to the complexity\nof automatic differentiation, and the variety and structural assumptions of\ncurvature proxies, like sparsity and Kronecker factorization. In this position\npaper, we argue that linear operators -- an interface for performing\nmatrix-vector products -- provide a general, scalable, and user-friendly\nabstraction to handle curvature matrices. To support this position, we\ndeveloped $\\textit{curvlinops}$, a library that provides curvature matrices\nthrough a unified linear operator interface. We demonstrate with\n$\\textit{curvlinops}$ how this interface can hide complexity, simplify\napplications, be extensible and interoperable with other libraries, and scale\nto large NNs.\n","authors":["Felix Dangel","Runa Eschenhagen","Weronika Ormaniec","Andres Fernandez","Lukas Tatzel","Agustinus Kristiadi"],"pdf_url":"https://arxiv.org/pdf/2501.19183v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2501.19182v1","updated":"2025-01-31T14:46:11Z","published":"2025-01-31T14:46:11Z","title":"A Comunication Framework for Compositional Generation","summary":"  Compositionality and compositional generalization--the ability to understand\nnovel combinations of known concepts--are central characteristics of human\nlanguage and are hypothesized to be essential for human cognition. In machine\nlearning, the emergence of this property has been studied in a communication\ngame setting, where independent agents (a sender and a receiver) converge to a\nshared encoding policy from a set of states to a space of discrete messages,\nwhere the receiver can correctly reconstruct the states observed by the sender\nusing only the sender's messages. The use of communication games in generation\ntasks is still largely unexplored, with recent methods for compositional\ngeneration focusing mainly on the use of supervised guidance (either through\nclass labels or text). In this work, we take the first steps to fill this gap,\nand we present a self-supervised generative communication game-based framework\nfor creating compositional encodings in learned representations from\npre-trained encoder-decoder models. In an Iterated Learning (IL) protocol\ninvolving a sender and a receiver, we apply alternating pressures for\ncompression and diversity of encoded discrete messages, so that the protocol\nconverges to an efficient but unambiguous encoding. Approximate message entropy\nregularization is used to favor compositional encodings. Our framework is based\non rigorous justifications and proofs of defining and balancing the concepts of\nEficiency, Unambiguity and Non-Holisticity in encoding. We test our method on\nthe compositional image dataset Shapes3D, demonstrating robust performance in\nboth reconstruction and compositionality metrics, surpassing other tested\ndiscrete message frameworks.\n","authors":["Rafael Elberg","Mircea Petrache","Denis Parra"],"pdf_url":"https://arxiv.org/pdf/2501.19182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19179v1","updated":"2025-01-31T14:43:22Z","published":"2025-01-31T14:43:22Z","title":"Learning Non-Local Molecular Interactions via Equivariant Local\n  Representations and Charge Equilibration","summary":"  Graph Neural Network (GNN) potentials relying on chemical locality offer\nnear-quantum mechanical accuracy at significantly reduced computational costs.\nBy propagating local information to distance particles, Message-passing neural\nnetworks (MPNNs) extend the locality concept to model interactions beyond their\nlocal neighborhood. Still, this locality precludes modeling long-range effects,\nsuch as charge transfer, electrostatic interactions, and dispersion effects,\nwhich are critical to adequately describe many real-world systems. In this\nwork, we propose the Charge Equilibration Layer for Long-range Interactions\n(CELLI) to address the challenging modeling of non-local interactions and the\nhigh computational cost of MPNNs. This novel architecture generalizes the\nfourth-generation high-dimensional neural network (4GHDNN) concept, integrating\nthe charge equilibration (Qeq) method into a model-agnostic building block for\nmodern equivariant GNN potentials. A series of benchmarks show that CELLI can\nextend the strictly local Allegro architecture to model highly non-local\ninteractions and charge transfer. Our architecture generalizes to diverse\ndatasets and large structures, achieving an accuracy comparable to MPNNs at\nabout twice the computational efficiency.\n","authors":["Paul Fuchs","Michał Sanocki","Julija Zavadlav"],"pdf_url":"https://arxiv.org/pdf/2501.19179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19178v1","updated":"2025-01-31T14:43:16Z","published":"2025-01-31T14:43:16Z","title":"No Foundations without Foundations -- Why semi-mechanistic models are\n  essential for regulatory biology","summary":"  Despite substantial efforts, deep learning has not yet delivered a\ntransformative impact on elucidating regulatory biology, particularly in the\nrealm of predicting gene expression profiles. Here, we argue that genuine\n\"foundation models\" of regulatory biology will remain out of reach unless\nguided by frameworks that integrate mechanistic insight with principled\nexperimental design. We present one such ground-up, semi-mechanistic framework\nthat unifies perturbation-based experimental designs across both in vitro and\nin vivo CRISPR screens, accounting for differentiating and non-differentiating\ncellular systems. By revealing previously unrecognised assumptions in published\nmachine learning methods, our approach clarifies links with popular techniques\nsuch as variational autoencoders and structural causal models. In practice,\nthis framework suggests a modified loss function that we demonstrate can\nimprove predictive performance, and further suggests an error analysis that\ninforms batching strategies. Ultimately, since cellular regulation emerges from\ninnumerable interactions amongst largely uncharted molecular components, we\ncontend that systems-level understanding cannot be achieved through structural\nbiology alone. Instead, we argue that real progress will require a\nfirst-principles perspective on how experiments capture biological phenomena,\nhow data are generated, and how these processes can be reflected in more\nfaithful modelling architectures.\n","authors":["Luka Kovačević","Thomas Gaudelet","James Opzoomer","Hagen Triendl","John Whittaker","Caroline Uhler","Lindsay Edwards","Jake P. Taylor-King"],"pdf_url":"https://arxiv.org/pdf/2501.19178v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.19172v1","updated":"2025-01-31T14:39:12Z","published":"2025-01-31T14:39:12Z","title":"PSyDUCK: Training-Free Steganography for Latent Diffusion","summary":"  Recent advances in AI-generated steganography highlight its potential for\nsafeguarding the privacy of vulnerable democratic actors, including aid\nworkers, journalists, and whistleblowers operating in oppressive regimes. In\nthis work, we address current limitations and establish the foundations for\nlarge-throughput generative steganography. We introduce a novel approach that\nenables secure and efficient steganography within latent diffusion models. We\nshow empirically that our methods perform well across a variety of open-source\nlatent diffusion models, particularly in generative image and video tasks.\n","authors":["Georgia Channing","Aqib Mahfuz","Mark van der Wilk","Philip Torr","Fabio Pizzati","Christian Schroeder de Witt"],"pdf_url":"https://arxiv.org/pdf/2501.19172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11182v2","updated":"2025-01-31T14:36:14Z","published":"2024-10-15T02:00:36Z","title":"Position: On-Premises LLM Deployment Demands a Middle Path: Preserving\n  Privacy Without Sacrificing Model Confidentiality","summary":"  Current LLM customization typically relies on two deployment strategies:\nclosed-source APIs, which require users to upload private data to external\nservers, and open-weight models, which allow local fine-tuning but pose misuse\nrisks. In this position paper, we argue that (1) deploying closed-source LLMs\nwithin user-controlled infrastructure (\\textit{on-premises deployment})\nenhances data privacy and mitigates misuse risks, and (2) a well-designed\non-premises deployment must ensure model confidentiality -- by preventing model\ntheft -- and offer privacy-preserving customization. Prior research on small\nmodels has explored securing only the output layer within hardware-secured\ndevices to balance confidentiality and customization efficiency. However, we\nshow that this approach is insufficient for defending large-scale LLMs against\ndistillation attacks. We therefore introduce a {semi-open deployment framework}\nthat secures only a few, carefully chosen layers, achieving distillation\nresistance comparable to fully secured models while preserving fine-tuning\nflexibility. Through extensive experiments, we show that securing bottom layers\nsignificantly reduces functional extraction risks. Our findings demonstrate\nthat privacy and confidentiality can coexist, paving the way for secure\non-premises AI deployment that balances usability and protection.\n","authors":["Hanbo Huang","Yihan Li","Bowen Jiang","Lin Liu","Bo Jiang","Ruoyu Sun","Zhuotao Liu","Shiyu Liang"],"pdf_url":"https://arxiv.org/pdf/2410.11182v2.pdf","comment":"8 pages for main content of the paper"},{"id":"http://arxiv.org/abs/2410.03756v2","updated":"2025-01-31T14:29:42Z","published":"2024-10-02T06:30:07Z","title":"The Smart Buildings Control Suite: A Diverse Open Source Benchmark to\n  Evaluate and Scale HVAC Control Policies for Sustainability","summary":"  Commercial buildings account for 17% of U.S. carbon emissions, with roughly\nhalf of that from Heating, Ventilation, and Air Conditioning (HVAC). HVAC\ndevices form a complex thermodynamic system, and while Model Predictive Control\nand Reinforcement Learning have been used to optimize control policies, scaling\nto thousands of buildings remains a significant unsolved challenge. Most\ncurrent algorithms are over-optimized for specific buildings and rely on\nproprietary data or hard-to-configure simulations. We present the Smart\nBuildings Control Suite, the first open source interactive HVAC control\nbenchmark with a focus on solutions that scale. It consists of 3 components:\nreal-world telemetric data extracted from 11 buildings over 6 years, a\nlightweight data-driven simulator for each building, and a modular Physically\nInformed Neural Network (PINN) building model as a simulator alternative. The\nbuildings span a variety of climates, management systems, and sizes, and both\nthe simulator and PINN easily scale to new buildings, ensuring solutions using\nthis benchmark are robust to these factors and only reliant on fully scalable\nbuilding models. This represents a major step towards scaling HVAC optimization\nfrom the lab to buildings everywhere. To facilitate use, our benchmark is\ncompatible with the Gym standard, and our data is part of TensorFlow Datasets.\n","authors":["Judah Goldfeder","Victoria Dean","Zixin Jiang","Xuezheng Wang","Bing dong","Hod Lipson","John Sipple"],"pdf_url":"https://arxiv.org/pdf/2410.03756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19161v1","updated":"2025-01-31T14:28:47Z","published":"2025-01-31T14:28:47Z","title":"Locality-aware Surrogates for Gradient-based Black-box Optimization","summary":"  In physics and engineering, many processes are modeled using\nnon-differentiable black-box simulators, making the optimization of such\nfunctions particularly challenging. To address such cases, inspired by the\nGradient Theorem, we propose locality-aware surrogate models for active\nmodel-based black-box optimization. We first establish a theoretical connection\nbetween gradient alignment and the minimization of a Gradient Path Integral\nEquation (GradPIE) loss, which enforces consistency of the surrogate's\ngradients in local regions of the design space. Leveraging this theoretical\ninsight, we develop a scalable training algorithm that minimizes the GradPIE\nloss, enabling both offline and online learning while maintaining computational\nefficiency. We evaluate our approach on three real-world tasks - spanning\nautomated in silico experiments such as coupled nonlinear oscillators, analog\ncircuits, and optical systems - and demonstrate consistent improvements in\noptimization efficiency under limited query budgets. Our results offer\ndependable solutions for both offline and online optimization tasks where\nreliable gradient estimation is needed.\n","authors":["Ali Momeni","Stefan Uhlich","Arun Venkitaraman","Chia-Yu Hsieh","Andrea Bonetti","Ryoga Matsuo","Eisaku Ohbuchi","Lorenzo Servadei"],"pdf_url":"https://arxiv.org/pdf/2501.19161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09760v3","updated":"2025-01-31T14:26:27Z","published":"2024-10-13T07:36:45Z","title":"Targeted Vaccine: Safety Alignment for Large Language Models against\n  Harmful Fine-Tuning via Layer-wise Perturbation","summary":"  Harmful fine-tuning attack poses a serious threat to the online fine-tuning\nservice. Vaccine, a recent alignment-stage defense, applies uniform\nperturbation to all layers of embedding to make the model robust to the\nsimulated embedding drift. However, applying layer-wise uniform perturbation\nmay lead to excess perturbations for some particular safety-irrelevant layers,\nresulting in defense performance degradation and unnecessary memory\nconsumption. To address this limitation, we propose Targeted Vaccine\n(T-Vaccine), a memory-efficient safety alignment method that applies\nperturbation to only selected layers of the model. T-Vaccine follows two core\nsteps: First, it uses gradient norm as a statistical metric to identify the\nsafety-critical layers. Second, instead of applying uniform perturbation across\nall layers, T-Vaccine only applies perturbation to the safety-critical layers\nwhile keeping other layers frozen during training. Results show that T-Vaccine\noutperforms Vaccine in terms of both defense effectiveness and resource\nefficiency. Comparison with other defense baselines, e.g., RepNoise and TAR\nalso demonstrate the superiority of T-Vaccine. Notably, T-Vaccine is the first\ndefense that can address harmful fine-tuning issues for a 7B pre-trained models\ntrained on consumer GPUs with limited memory (e.g., RTX 4090). Our code is\navailable at https://github.com/Lslland/T-Vaccine.\n","authors":["Guozhi Liu","Weiwei Lin","Tiansheng Huang","Ruichao Mo","Qi Mu","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2410.09760v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19158v1","updated":"2025-01-31T14:21:02Z","published":"2025-01-31T14:21:02Z","title":"A theoretical framework for overfitting in energy-based modeling","summary":"  We investigate the impact of limited data on training pairwise energy-based\nmodels for inverse problems aimed at identifying interaction networks.\nUtilizing the Gaussian model as testbed, we dissect training trajectories\nacross the eigenbasis of the coupling matrix, exploiting the independent\nevolution of eigenmodes and revealing that the learning timescales are tied to\nthe spectral decomposition of the empirical covariance matrix. We see that\noptimal points for early stopping arise from the interplay between these\ntimescales and the initial conditions of training. Moreover, we show that\nfinite data corrections can be accurately modeled through asymptotic random\nmatrix theory calculations and provide the counterpart of generalized\ncross-validation in the energy based model context. Our analytical framework\nextends to binary-variable maximum-entropy pairwise models with minimal\nvariations. These findings offer strategies to control overfitting in\ndiscrete-variable models through empirical shrinkage corrections, improving the\nmanagement of overfitting in energy-based generative models.\n","authors":["Giovanni Catania","Aurélien Decelle","Cyril Furtlehner","Beatriz Seoane"],"pdf_url":"https://arxiv.org/pdf/2501.19158v1.pdf","comment":"23 pages, 13 figures (including appendix)"},{"id":"http://arxiv.org/abs/2406.04914v2","updated":"2025-01-31T14:20:10Z","published":"2024-06-07T13:11:04Z","title":"Submodular Framework for Structured-Sparse Optimal Transport","summary":"  Unbalanced optimal transport (UOT) has recently gained much attention due to\nits flexible framework for handling un-normalized measures and its robustness\nproperties. In this work, we explore learning (structured) sparse transport\nplans in the UOT setting, i.e., transport plans have an upper bound on the\nnumber of non-sparse entries in each column (structured sparse pattern) or in\nthe whole plan (general sparse pattern). We propose novel sparsity-constrained\nUOT formulations building on the recently explored maximum mean discrepancy\nbased UOT. We show that the proposed optimization problem is equivalent to the\nmaximization of a weakly submodular function over a uniform matroid or a\npartition matroid. We develop efficient gradient-based discrete greedy\nalgorithms and provide the corresponding theoretical guarantees. Empirically,\nwe observe that our proposed greedy algorithms select a diverse support set and\nwe illustrate the efficacy of the proposed approach in various applications.\n","authors":["Piyushi Manupriya","Pratik Jawanpuria","Karthik S. Gurumoorthy","SakethaNath Jagarlapudi","Bamdev Mishra"],"pdf_url":"https://arxiv.org/pdf/2406.04914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17426v3","updated":"2025-01-31T14:13:49Z","published":"2024-11-26T13:34:02Z","title":"CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning","summary":"  Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.\n","authors":["Fanxu Meng","Pingzhi Tang","Fan jiang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.17426v3.pdf","comment":"https://github.com/GraphPKU/PiSSA"},{"id":"http://arxiv.org/abs/2501.19153v1","updated":"2025-01-31T14:11:10Z","published":"2025-01-31T14:11:10Z","title":"Test-Time Training Scaling for Chemical Exploration in Drug Design","summary":"  Chemical language models for molecular design have the potential to find\nsolutions to multi-parameter optimization problems in drug discovery via\nreinforcement learning (RL). A key requirement to achieve this is the capacity\nto \"search\" chemical space to identify all molecules of interest. Here, we\npropose a challenging new benchmark to discover dissimilar molecules that\npossess similar bioactivity, a common scenario in drug discovery, but a hard\nproblem to optimize. We show that a population of RL agents can solve the\nbenchmark, while a single agent cannot. We also find that cooperative\nstrategies are not significantly better than independent agents. Moreover, the\nperformance on the benchmark scales log-linearly with the number of independent\nagents, showing a test-time training scaling law for chemical language models.\n","authors":["Morgan Thomas","Albert Bou","Gianni De Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2501.19153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19149v1","updated":"2025-01-31T14:06:13Z","published":"2025-01-31T14:06:13Z","title":"On the inductive bias of infinite-depth ResNets and the bottleneck rank","summary":"  We compute the minimum-norm weights of a deep linear ResNet, and find that\nthe inductive bias of this architecture lies between minimizing nuclear norm\nand rank. This implies that, with appropriate hyperparameters, deep nonlinear\nResNets have an inductive bias towards minimizing bottleneck rank.\n","authors":["Enric Boix-Adsera"],"pdf_url":"https://arxiv.org/pdf/2501.19149v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.13439v3","updated":"2025-01-31T14:05:16Z","published":"2024-12-18T02:20:51Z","title":"Rare Event Detection in Imbalanced Multi-Class Datasets Using an Optimal\n  MIP-Based Ensemble Weighting Approach","summary":"  To address the challenges of imbalanced multi-class datasets typically used\nfor rare event detection in critical cyber-physical systems, we propose an\noptimal, efficient, and adaptable mixed integer programming (MIP) ensemble\nweighting scheme. Our approach leverages the diverse capabilities of the\nclassifier ensemble on a granular per class basis, while optimizing the weights\nof classifier-class pairs using elastic net regularization for improved\nrobustness and generalization. Additionally, it seamlessly and optimally\nselects a predefined number of classifiers from a given set. We evaluate and\ncompare our MIP-based method against six well-established weighting schemes,\nusing representative datasets and suitable metrics, under various ensemble\nsizes. The experimental results reveal that MIP outperforms all existing\napproaches, achieving an improvement in balanced accuracy ranging from 0.99% to\n7.31%, with an overall average of 4.53% across all datasets and ensemble sizes.\nFurthermore, it attains an overall average increase of 4.63%, 4.60%, and 4.61%\nin macro-averaged precision, recall, and F1-score, respectively, while\nmaintaining computational efficiency.\n","authors":["Georgios Tertytchny","Georgios L. Stavrinides","Maria K. Michael"],"pdf_url":"https://arxiv.org/pdf/2412.13439v3.pdf","comment":"To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25). This version includes the supplementary\n  material"},{"id":"http://arxiv.org/abs/2410.06816v2","updated":"2025-01-31T14:04:29Z","published":"2024-10-09T12:14:24Z","title":"On the Expressiveness of Multi-Neuron Convex Relaxations","summary":"  To provide robustness guarantees, neural network certification methods\nheavily rely on convex relaxations. The imprecision of these convex\nrelaxations, however, is a major obstacle: even the most precise single-neuron\nrelaxation is incomplete for general ReLU networks, a phenomenon referred to as\nthe single-neuron convex barrier. While heuristic instantiations of\nmulti-neuron relaxations have been proposed to circumvent this barrier in\npractice, their theoretical properties remain largely unknown. In this work, we\nconduct the first rigorous study of the expressiveness of multi-neuron\nrelaxations. We first show that the ``$\\max$'' function in $\\mathbb{R}^d$ can\nbe encoded by a ReLU network and exactly bounded by a multi-neuron relaxation,\nwhich is impossible for any single-neuron relaxation. Further, we prove that\nmulti-neuron relaxations can be turned into complete verifiers by\nsemantic-preserving structural transformations or by input space partitioning\nthat enjoys improved worst-case partition complexity. We also show that without\nthese augmentations, the completeness guarantee can no longer be obtained, and\nthe relaxation error of every multi-neuron relaxation can be unbounded. To the\nbest of our knowledge, this is the first work to provide an extensive\ncharacterization of multi-neuron relaxations and their expressiveness in neural\nnetwork certification.\n","authors":["Yuhao Mao","Yani Zhang","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.06816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02175v4","updated":"2025-01-31T14:03:02Z","published":"2024-06-04T10:11:46Z","title":"Branches: Efficiently Seeking Optimal Sparse Decision Trees with AO*","summary":"  Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine\nLearning, yet it poses a formidable optimisation challenge. Practical\nalgorithms have recently emerged, primarily leveraging Dynamic Programming and\nBranch & Bound. However, most of these approaches rely on a Depth-First-Search\nstrategy, which is inefficient when searching for DTs at high depths and\nrequires the definition of a maximum depth hyperparameter. Best-First-Search\nwas also employed by other methods to circumvent these issues. The downside of\nthis strategy is its higher memory consumption, as such, it has to be designed\nin a fully efficient manner that takes full advantage of the problem's\nstructure. We formulate the problem as an AND/OR graph search which we solve\nwith a novel AO*-type algorithm called Branches. We prove both optimality and\ncomplexity guarantees for Branches and we show that it is more efficient than\nthe state of the art theoretically and on a variety of experiments.\nFurthermore, Branches supports non-binary features unlike the other methods, we\nshow that this property can further induce larger gains in computational\nefficiency.\n","authors":["Ayman Chaouki","Jesse Read","Albert Bifet"],"pdf_url":"https://arxiv.org/pdf/2406.02175v4.pdf","comment":"This preprint is currently under review"},{"id":"http://arxiv.org/abs/2501.19145v1","updated":"2025-01-31T14:00:02Z","published":"2025-01-31T14:00:02Z","title":"Improving Multi-Label Contrastive Learning by Leveraging Label\n  Distribution","summary":"  In multi-label learning, leveraging contrastive learning to learn better\nrepresentations faces a key challenge: selecting positive and negative samples\nand effectively utilizing label information. Previous studies selected positive\nand negative samples based on the overlap between labels and used them for\nlabel-wise loss balancing. However, these methods suffer from a complex\nselection process and fail to account for the varying importance of different\nlabels. To address these problems, we propose a novel method that improves\nmulti-label contrastive learning through label distribution. Specifically, when\nselecting positive and negative samples, we only need to consider whether there\nis an intersection between labels. To model the relationships between labels,\nwe introduce two methods to recover label distributions from logical labels,\nbased on Radial Basis Function (RBF) and contrastive loss, respectively. We\nevaluate our method on nine widely used multi-label datasets, including image\nand vector datasets. The results demonstrate that our method outperforms\nstate-of-the-art methods in six evaluation metrics.\n","authors":["Ning Chen","Shen-Huan Lyu","Tian-Shuang Wu","Yanyan Wang","Bin Tang"],"pdf_url":"https://arxiv.org/pdf/2501.19145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02300v2","updated":"2025-01-31T13:57:25Z","published":"2024-06-04T13:29:12Z","title":"Point-Level Topological Representation Learning on Point Clouds","summary":"  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise and heterogeneous sampling.\n","authors":["Vincent P. Grande","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2406.02300v2.pdf","comment":"46 pages, 18 figures, comments welcome"},{"id":"http://arxiv.org/abs/2411.16085v3","updated":"2025-01-31T13:56:58Z","published":"2024-11-25T04:36:01Z","title":"Cautious Optimizers: Improving Training with One Line of Code","summary":"  AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searched for faster and more stable optimizers with only\nconstrained positive outcomes. In this work, we propose a single-line\nmodification in Pytorch to any momentum-based optimizer, which we rename\ncautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing not only speed-up on\nLlama and MAE pretraining up to $1.47$ times, but also better results in LLM\npost-training tasks. Code is available at\nhttps://github.com/kyleliang919/C-Optim.\n","authors":["Kaizhao Liang","Lizhang Chen","Bo Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19137v1","updated":"2025-01-31T13:46:42Z","published":"2025-01-31T13:46:42Z","title":"A Metric for the Balance of Information in Graph Learning","summary":"  Graph learning on molecules makes use of information from both the molecular\nstructure and the features attached to that structure. Much work has been\nconducted on biasing either towards structure or features, with the aim that\nbias bolsters performance. Identifying which information source a dataset\nfavours, and therefore how to approach learning that dataset, is an open issue.\nHere we propose Noise-Noise Ratio Difference (NNRD), a quantitative metric for\nwhether there is more useful information in structure or features. By employing\niterative noising on features and structure independently, leaving the other\nintact, NNRD measures the degradation of information in each. We employ NNRD\nover a range of molecular tasks, and show that it corresponds well to a loss of\ninformation, with intuitive results that are more expressive than simple\nperformance aggregates. Our future work will focus on expanding data domains,\ntasks and types, as well as refining our choice of baseline model.\n","authors":["Alex O. Davies","Nirav S. Ajmeri","Telmo de Menezes e Silva Filho"],"pdf_url":"https://arxiv.org/pdf/2501.19137v1.pdf","comment":"In proceedings of the 4th Annual AAAI Workshop on AI to Accelerate\n  Science and Engineering (AI2ASE)"},{"id":"http://arxiv.org/abs/2501.19133v1","updated":"2025-01-31T13:38:57Z","published":"2025-01-31T13:38:57Z","title":"Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning","summary":"  The effectiveness of credit assignment in reinforcement learning (RL) when\ndealing with high-dimensional data is influenced by the success of\nrepresentation learning via deep neural networks, and has implications for the\nsample efficiency of deep RL algorithms. Input decorrelation has been\npreviously introduced as a method to speed up optimization in neural networks,\nand has proven impactful in both efficient deep learning and as a method for\neffective representation learning for deep RL algorithms. We propose a novel\napproach to online decorrelation in deep RL based on the decorrelated\nbackpropagation algorithm that seamlessly integrates the decorrelation process\ninto the RL training pipeline. Decorrelation matrices are added to each layer,\nwhich are updated using a separate decorrelation learning rule that minimizes\nthe total decorrelation loss across all layers, in parallel to minimizing the\nusual RL loss. We used our approach in combination with the soft actor-critic\n(SAC) method, which we refer to as decorrelated soft actor-critic (DSAC).\nExperiments on the Atari 100k benchmark with DSAC shows, compared to the\nregular SAC baseline, faster training in five out of the seven games tested and\nimproved reward performance in two games with around 50% reduction in\nwall-clock time, while maintaining performance levels on the other games. These\nresults demonstrate the positive impact of network-wide decorrelation in deep\nRL for speeding up its sample efficiency through more effective credit\nassignment.\n","authors":["Burcu Küçükoğlu","Sander Dalm","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2501.19133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19128v1","updated":"2025-01-31T13:35:19Z","published":"2025-01-31T13:35:19Z","title":"Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised\n  Approach","summary":"  In many real-world scenarios, reward signal for agents are exceedingly\nsparse, making it challenging to learn an effective reward function for reward\nshaping. To address this issue, our approach performs reward shaping not only\nby utilizing non-zero-reward transitions but also by employing the\nSemi-Supervised Learning (SSL) technique combined with a novel data\naugmentation to learn trajectory space representations from the majority of\ntransitions, zero-reward transitions, thereby improving the efficacy of reward\nshaping. Experimental results in Atari and robotic manipulation demonstrate\nthat our method effectively generalizes reward shaping to sparse reward\nscenarios, achieving up to four times better performance in reaching higher\nbest scores compared to curiosity-driven methods. The proposed double entropy\ndata augmentation enhances performance, showcasing a 15.8\\% increase in best\nscore over other augmentation methods.\n","authors":["Wenyun Li","Wenjie Huang"],"pdf_url":"https://arxiv.org/pdf/2501.19128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19122v1","updated":"2025-01-31T13:26:22Z","published":"2025-01-31T13:26:22Z","title":"FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling","summary":"  Federated Learning (FL) enables collaborative model training across\ndistributed clients without data sharing, but its high computational and\ncommunication demands strain resource-constrained devices. While existing\nmethods use dynamic pruning to improve efficiency by periodically adjusting\nsparse model topologies while maintaining sparsity, these approaches suffer\nfrom issues such as greedy adjustments, unstable topologies, and communication\ninefficiency, resulting in less robust models and suboptimal performance under\ndata heterogeneity and partial client availability. To address these\nchallenges, we propose Federated Robust pruning via combinatorial Thompson\nSampling (FedRTS), a novel framework designed to develop robust sparse models.\nFedRTS enhances robustness and performance through its Thompson Sampling-based\nAdjustment (TSAdj) mechanism, which uses probabilistic decisions informed by\nstable, farsighted information instead of deterministic decisions reliant on\nunstable and myopic information in previous methods. Extensive experiments\ndemonstrate that FedRTS achieves state-of-the-art performance in computer\nvision and natural language processing tasks while reducing communication\ncosts, particularly excelling in scenarios with heterogeneous data\ndistributions and partial client participation. Our codes are available at:\nhttps://github.com/Little0o0/FedRTS\n","authors":["Hong Huang","Hai Yang","Yuan Chen","Jiaxun Ye","Dapeng Wu"],"pdf_url":"https://arxiv.org/pdf/2501.19122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19116v1","updated":"2025-01-31T13:20:05Z","published":"2025-01-31T13:20:05Z","title":"A Theoretical Justification for Asymmetric Actor-Critic Algorithms","summary":"  In reinforcement learning for partially observable environments, many\nsuccessful algorithms were developed within the asymmetric learning paradigm.\nThis paradigm leverages additional state information available at training time\nfor faster learning. Although the proposed learning objectives are usually\ntheoretically sound, these methods still lack a theoretical justification for\ntheir potential benefits. We propose such a justification for asymmetric\nactor-critic algorithms with linear function approximators by adapting a\nfinite-time convergence analysis to this setting. The resulting finite-time\nbound reveals that the asymmetric critic eliminates an error term arising from\naliasing in the agent state.\n","authors":["Gaspard Lambrechts","Damien Ernst","Aditya Mahajan"],"pdf_url":"https://arxiv.org/pdf/2501.19116v1.pdf","comment":"7 pages, 29 pages total"},{"id":"http://arxiv.org/abs/2501.19114v1","updated":"2025-01-31T13:18:10Z","published":"2025-01-31T13:18:10Z","title":"Principal Components for Neural Network Initialization","summary":"  Principal Component Analysis (PCA) is a commonly used tool for dimension\nreduction and denoising. Therefore, it is also widely used on the data prior to\ntraining a neural network. However, this approach can complicate the\nexplanation of explainable AI (XAI) methods for the decision of the model. In\nthis work, we analyze the potential issues with this approach and propose\nPrincipal Components-based Initialization (PCsInit), a strategy to incorporate\nPCA into the first layer of a neural network via initialization of the first\nlayer in the network with the principal components, and its two variants\nPCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct\nand straightforward as for neural networks and are simpler than using PCA prior\nto training a neural network on the principal components. Moreover, as will be\nillustrated in the experiments, such training strategies can also allow further\nimprovement of training via backpropagation.\n","authors":["Nhan Phan","Thu Nguyen","Pål Halvorsen","Michael A. Riegler"],"pdf_url":"https://arxiv.org/pdf/2501.19114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07480v3","updated":"2025-01-31T13:16:12Z","published":"2024-09-02T10:03:03Z","title":"EEG-Language Modeling for Pathology Detection","summary":"  Multimodal language modeling has enabled breakthroughs for representation\nlearning, yet remains unexplored in the realm of functional brain data for\npathology detection. This paper pioneers EEG-language models (ELMs) trained on\nclinical reports and 15000 EEGs. We propose to combine multimodal alignment in\nthis novel domain with timeseries cropping and text segmentation, enabling an\nextension based on multiple instance learning to alleviate misalignment between\nirrelevant EEG or text segments. Our multimodal models significantly improve\npathology detection compared to EEG-only models across four evaluations and for\nthe first time enable zero-shot classification as well as retrieval of both\nneural signals and reports. In sum, these results highlight the potential of\nELMs, representing significant progress for clinical applications.\n","authors":["Sam Gijsen","Kerstin Ritter"],"pdf_url":"https://arxiv.org/pdf/2409.07480v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02387v3","updated":"2025-01-31T13:14:18Z","published":"2024-10-03T11:07:43Z","title":"BiSSL: A Bilevel Optimization Framework for Enhancing the Alignment\n  Between Self-Supervised Pre-Training and Downstream Fine-Tuning","summary":"  This study presents BiSSL, a novel training framework that utilizes bilevel\noptimization to enhance the alignment between the pretext pre-training and\ndownstream fine-tuning stages in self-supervised learning. BiSSL formulates the\npretext and downstream task objectives as the lower- and upper-level objectives\nin a bilevel optimization problem and serves as an intermediate training stage\nwithin the self-supervised learning pipeline. By explicitly modeling the\ninterdependence of these training stages, BiSSL facilitates enhanced\ninformation sharing between them, ultimately leading to a backbone parameter\ninitialization that is better aligned for the downstream task. We propose a\nversatile training algorithm that alternates between optimizing the two\nobjectives defined in BiSSL, which is applicable to a broad range of pretext\nand downstream tasks. Using SimCLR and Bootstrap Your Own Latent to pre-train\nResNet-50 backbones on the ImageNet dataset, we demonstrate that our proposed\nframework significantly outperforms the conventional self-supervised learning\npipeline on the vast majority of 12 downstream image classification datasets,\nas well as on object detection. Visualizations of the backbone features provide\nfurther evidence that BiSSL improves the downstream task alignment of the\nbackbone features prior to fine-tuning.\n","authors":["Gustav Wagner Zakarias","Lars Kai Hansen","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2410.02387v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15655v2","updated":"2025-01-31T13:12:36Z","published":"2025-01-26T19:31:56Z","title":"A Machine Learning Approach to Automatic Fall Detection of Soldiers","summary":"  Military personnel and security agents often face significant physical risks\nduring conflict and engagement situations, particularly in urban operations.\nEnsuring the rapid and accurate communication of incidents involving injuries\nis crucial for the timely execution of rescue operations. This article presents\nresearch conducted under the scope of the Brazilian Navy's ``Soldier of the\nFuture'' project, focusing on the development of a Casualty Detection System to\nidentify injuries that could incapacitate a soldier and lead to severe blood\nloss. The study specifically addresses the detection of soldier falls, which\nmay indicate critical injuries such as hypovolemic hemorrhagic shock. To\ngenerate the publicly available dataset, we used smartwatches and smartphones\nas wearable devices to collect inertial data from soldiers during various\nactivities, including simulated falls. The data were used to train 1D\nConvolutional Neural Networks (CNN1D) with the objective of accurately\nclassifying falls that could result from life-threatening injuries. We explored\ndifferent sensor placements (on the wrists and near the center of mass) and\nvarious approaches to using inertial variables, including linear and angular\naccelerations. The neural network models were optimized using Bayesian\ntechniques to enhance their performance. The best-performing model and its\nresults, discussed in this article, contribute to the advancement of automated\nsystems for monitoring soldier safety and improving response times in\nengagement scenarios.\n","authors":["Leandro Soares","Gustavo Venturini","José Gomes","Jonathan Efigenio","Pablo Rangel","Pedro Gonzalez","Joel dos Santos","Diego Brandão","Eduardo Bezerra"],"pdf_url":"https://arxiv.org/pdf/2501.15655v2.pdf","comment":"15 pages, 9 figures, submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2409.17625v2","updated":"2025-01-31T13:09:22Z","published":"2024-09-26T08:20:05Z","title":"Benign Overfitting in Token Selection of Attention Mechanism","summary":"  Attention mechanism is a fundamental component of the transformer model and\nplays a significant role in its success. However, the theoretical understanding\nof how attention learns to select tokens is still an emerging area of research.\nIn this work, we study the training dynamics and generalization ability of the\nattention mechanism under classification problems with label noise. We show\nthat, with the characterization of signal-to-noise ratio (SNR), the token\nselection of attention mechanism achieves benign overfitting, i.e., maintaining\nhigh generalization performance despite fitting label noise. Our work also\ndemonstrates an interesting delayed acquisition of generalization after an\ninitial phase of overfitting. Finally, we provide experiments to support our\ntheoretical analysis using both synthetic and real-world datasets.\n","authors":["Keitaro Sakamoto","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2409.17625v2.pdf","comment":"Largely updated from the previous version"},{"id":"http://arxiv.org/abs/2410.06895v2","updated":"2025-01-31T13:07:32Z","published":"2024-10-09T13:58:41Z","title":"Average Certified Radius is a Poor Metric for Randomized Smoothing","summary":"  Randomized smoothing is a popular approach for providing certified robustness\nguarantees against adversarial attacks, and has become an active area of\nresearch. Over the past years, the average certified radius (ACR) has emerged\nas the most important metric for comparing methods and tracking progress in the\nfield. However, in this work, for the first time we show that ACR is a poor\nmetric for evaluating robustness guarantees provided by randomized smoothing.\nWe theoretically prove not only that a trivial classifier can have arbitrarily\nlarge ACR, but also that ACR is much more sensitive to improvements on easy\nsamples than on hard ones. Empirically, we confirm that existing training\nstrategies, though improving ACR with different approaches, reduce the model's\nrobustness on hard samples consistently. To strengthen our conclusion, we\npropose strategies, including explicitly discarding hard samples, reweighting\nthe dataset with approximate certified radius, and extreme optimization for\neasy samples, to achieve state-of-the-art ACR, without training for robustness\non the full data distribution. Overall, our results suggest that ACR has\nintroduced a strong undesired bias to the field, and its application should be\ndiscontinued when evaluating randomized smoothing.\n","authors":["Chenhao Sun","Yuhao Mao","Mark Niklas Müller","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2410.06895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19107v1","updated":"2025-01-31T13:04:37Z","published":"2025-01-31T13:04:37Z","title":"Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected","summary":"  This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations.\n","authors":["Yingtao Zhang","Jialin Zhao","Wenjing Wu","Ziheng Liao","Umberto Michieli","Carlo Vittorio Cannistraci"],"pdf_url":"https://arxiv.org/pdf/2501.19107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17233v2","updated":"2025-01-31T13:04:34Z","published":"2024-10-22T17:53:34Z","title":"Large Language Models are In-context Preference Learners","summary":"  Preference-based reinforcement learning is an effective way to handle tasks\nwhere rewards are hard to specify but can be exceedingly inefficient as\npreference learning is often tabula rasa. We demonstrate that Large Language\nModels (LLMs) have native preference-learning capabilities that allow them to\nachieve sample-efficient preference learning, addressing this challenge. We\npropose In-Context Preference Learning (ICPL), which uses in-context learning\ncapabilities of LLMs to reduce human query inefficiency. ICPL uses the task\ndescription and basic environment code to create sets of reward functions which\nare iteratively refined by placing human feedback over videos of the resultant\npolicies into the context of an LLM and then requesting better rewards. We\nfirst demonstrate ICPL's effectiveness through a synthetic preference study,\nproviding quantitative evidence that it significantly outperforms baseline\npreference-based methods with much higher performance and orders of magnitude\ngreater efficiency. We observe that these improvements are not solely coming\nfrom LLM grounding in the task but that the quality of the rewards improves\nover time, indicating preference learning capabilities. Additionally, we\nperform a series of real human preference-learning trials and observe that ICPL\nextends beyond synthetic settings and can work effectively with\nhumans-in-the-loop.\n","authors":["Chao Yu","Qixin Tan","Hong Lu","Jiaxuan Gao","Xinting Yang","Yu Wang","Yi Wu","Eugene Vinitsky"],"pdf_url":"https://arxiv.org/pdf/2410.17233v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19105v1","updated":"2025-01-31T12:57:58Z","published":"2025-01-31T12:57:58Z","title":"Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the\n  Squared Loss","summary":"  The paradigm of weak-to-strong generalization constitutes the training of a\nstrong AI model on data labeled by a weak AI model, with the goal that the\nstrong model nevertheless outperforms its weak supervisor on the target task of\ninterest. For the setting of real-valued regression with the squared loss,\nrecent work quantitatively characterizes the gain in performance of the strong\nmodel over the weak model in terms of the misfit between the strong and weak\nmodel. We generalize such a characterization to learning tasks whose loss\nfunctions correspond to arbitrary Bregman divergences when the strong class is\nconvex. This extends the misfit-based characterization of performance gain in\nweak-to-strong generalization to classification tasks, as the cross-entropy\nloss can be expressed in terms of a Bregman divergence. In most practical\nscenarios, however, the strong model class may not be convex. We therefore\nweaken this assumption and study weak-to-strong generalization for convex\ncombinations of $k$ strong models in the strong class, in the concrete setting\nof classification. This allows us to obtain a similar misfit-based\ncharacterization of performance gain, upto an additional error term that\nvanishes as $k$ gets large. Our theoretical findings are supported by thorough\nexperiments on synthetic as well as real-world datasets.\n","authors":["Abhijeet Mulgund","Chirag Pabbaraju"],"pdf_url":"https://arxiv.org/pdf/2501.19105v1.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.19104v1","updated":"2025-01-31T12:57:29Z","published":"2025-01-31T12:57:29Z","title":"Neural Collapse Beyond the Unconstrainted Features Model: Landscape,\n  Dynamics, and Generalization in the Mean-Field Regime","summary":"  Neural Collapse is a phenomenon where the last-layer representations of a\nwell-trained neural network converge to a highly structured geometry. In this\npaper, we focus on its first (and most basic) property, known as NC1: the\nwithin-class variability vanishes. While prior theoretical studies establish\nthe occurrence of NC1 via the data-agnostic unconstrained features model, our\nwork adopts a data-specific perspective, analyzing NC1 in a three-layer neural\nnetwork, with the first two layers operating in the mean-field regime and\nfollowed by a linear layer. In particular, we establish a fundamental\nconnection between NC1 and the loss landscape: we prove that points with small\nempirical loss and gradient norm (thus, close to being stationary)\napproximately satisfy NC1, and the closeness to NC1 is controlled by the\nresidual loss and gradient norm. We then show that (i) gradient flow on the\nmean squared error converges to NC1 solutions with small empirical loss, and\n(ii) for well-separated data distributions, both NC1 and vanishing test loss\nare achieved simultaneously. This aligns with the empirical observation that\nNC1 emerges during training while models attain near-zero test error. Overall,\nour results demonstrate that NC1 arises from gradient training due to the\nproperties of the loss landscape, and they show the co-occurrence of NC1 and\nsmall test error for certain data distributions.\n","authors":["Diyuan Wu","Marco Mondelli"],"pdf_url":"https://arxiv.org/pdf/2501.19104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19102v1","updated":"2025-01-31T12:51:55Z","published":"2025-01-31T12:51:55Z","title":"Reinforcement Learning on Reconfigurable Hardware: Overcoming Material\n  Variability in Laser Material Processing","summary":"  Ensuring consistent processing quality is challenging in laser processes due\nto varying material properties and surface conditions. Although some approaches\nhave shown promise in solving this problem via automation, they often rely on\npredetermined targets or are limited to simulated environments. To address\nthese shortcomings, we propose a novel real-time reinforcement learning\napproach for laser process control, implemented on a Field Programmable Gate\nArray to achieve real-time execution. Our experimental results from laser\nwelding tests on stainless steel samples with a range of surface roughnesses\nvalidated the method's ability to adapt autonomously, without relying on reward\nengineering or prior setup information. Specifically, the algorithm learned the\ncorrect power profile for each unique surface characteristic, demonstrating\nsignificant improvements over hand-engineered optimal constant power strategies\n-- up to 23% better performance on rougher surfaces and 7% on mixed surfaces.\nThis approach represents a significant advancement in automating and optimizing\nlaser processes, with potential applications across multiple industries.\n","authors":["Giulio Masinelli","Chang Rajani","Patrik Hoffmann","Kilian Wasmer","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2501.19102v1.pdf","comment":"Accepted for the 2025 IEEE International Conference on Robotics and\n  Automation (ICRA), May 19-23, 2025, Atlanta, USA"},{"id":"http://arxiv.org/abs/2501.19099v1","updated":"2025-01-31T12:46:04Z","published":"2025-01-31T12:46:04Z","title":"Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations","summary":"  Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy.\n","authors":["Sihwan Park","Jihun Yun","SungYub Kim","Souvik Kundu","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2501.19099v1.pdf","comment":"35 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.19098v1","updated":"2025-01-31T12:45:46Z","published":"2025-01-31T12:45:46Z","title":"$\\infty$-Video: A Training-Free Approach to Long Video Understanding via\n  Continuous-Time Memory Consolidation","summary":"  Current video-language models struggle with long-video understanding due to\nlimited context lengths and reliance on sparse frame subsampling, often leading\nto information loss. This paper introduces $\\infty$-Video, which can process\narbitrarily long videos through a continuous-time long-term memory (LTM)\nconsolidation mechanism. Our framework augments video Q-formers by allowing\nthem to process unbounded video contexts efficiently and without requiring\nadditional training. Through continuous attention, our approach dynamically\nallocates higher granularity to the most relevant video segments, forming\n\"sticky\" memories that evolve over time. Experiments with Video-LLaMA and\nVideoChat2 demonstrate improved performance in video question-answering tasks,\nshowcasing the potential of continuous-time LTM mechanisms to enable scalable\nand training-free comprehension of long videos.\n","authors":["Saul Santos","António Farinhas","Daniel C. McNamee","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2501.19098v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.17745v2","updated":"2025-01-31T12:45:13Z","published":"2025-01-29T16:32:14Z","title":"Dynamics of Transient Structure in In-Context Linear Regression\n  Transformers","summary":"  Modern deep neural networks display striking examples of rich internal\ncomputational structure. Uncovering principles governing the development of\nsuch structure is a priority for the science of deep learning. In this paper,\nwe explore the transient ridge phenomenon: when transformers are trained on\nin-context linear regression tasks with intermediate task diversity, they\ninitially behave like ridge regression before specializing to the tasks in\ntheir training distribution. This transition from a general solution to a\nspecialized solution is revealed by joint trajectory principal component\nanalysis. Further, we draw on the theory of Bayesian internal model selection\nto suggest a general explanation for the phenomena of transient structure in\ntransformers, based on an evolving tradeoff between loss and complexity. We\nempirically validate this explanation by measuring the model complexity of our\ntransformers as defined by the local learning coefficient.\n","authors":["Liam Carroll","Jesse Hoogland","Matthew Farrugia-Roberts","Daniel Murfet"],"pdf_url":"https://arxiv.org/pdf/2501.17745v2.pdf","comment":"37 pages, 27 figures"},{"id":"http://arxiv.org/abs/2501.19095v1","updated":"2025-01-31T12:41:02Z","published":"2025-01-31T12:41:02Z","title":"PathE: Leveraging Entity-Agnostic Paths for Parameter-Efficient\n  Knowledge Graph Embeddings","summary":"  Knowledge Graphs (KGs) store human knowledge in the form of entities (nodes)\nand relations, and are used extensively in various applications. KG embeddings\nare an effective approach to addressing tasks like knowledge discovery, link\nprediction, and reasoning. This is often done by allocating and learning\nembedding tables for all or a subset of the entities. As this scales linearly\nwith the number of entities, learning embedding models in real-world KGs with\nmillions of nodes can be computationally intractable. To address this\nscalability problem, our model, PathE, only allocates embedding tables for\nrelations (which are typically orders of magnitude fewer than the entities) and\nrequires less than 25% of the parameters of previous parameter efficient\nmethods. Rather than storing entity embeddings, we learn to compute them by\nleveraging multiple entity-relation paths to contextualise individual entities\nwithin triples. Evaluated on four benchmarks, PathE achieves state-of-the-art\nperformance in relation prediction, and remains competitive in link prediction\non path-rich KGs while training on consumer-grade hardware. We perform ablation\nexperiments to test our design choices and analyse the sensitivity of the model\nto key hyper-parameters. PathE is efficient and cost-effective for relationally\ndiverse and well-connected KGs commonly found in real-world applications.\n","authors":["Ioannis Reklos","Jacopo de Berardinis","Elena Simperl","Albert Meroño-Peñuela"],"pdf_url":"https://arxiv.org/pdf/2501.19095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19091v1","updated":"2025-01-31T12:37:09Z","published":"2025-01-31T12:37:09Z","title":"FL-APU: A Software Architecture to Ease Practical Implementation of\n  Cross-Silo Federated Learning","summary":"  Federated Learning (FL) is an upcoming technology that is increasingly\napplied in real-world applications. Early applications focused on cross-device\nscenarios, where many participants with limited resources train machine\nlearning (ML) models together, e.g., in the case of Google's GBoard.\nContrarily, cross-silo scenarios have only few participants but with many\nresources, e.g., in the healthcare domain. Despite such early efforts, FL is\nstill rarely used in practice and best practices are, hence, missing. For new\napplications, in our case inter-organizational cross-silo applications,\novercoming this lack of role models is a significant challenge.\n  In order to ease the use of FL in real-world cross-silo applications, we here\npropose a scenario-based architecture for the practical use of FL in the\ncontext of multiple companies collaborating to improve the quality of their ML\nmodels. The architecture emphasizes the collaboration between the participants\nand the FL server and extends basic interactions with domain-specific features.\nFirst, it combines governance with authentication, creating an environment\nwhere only trusted participants can join. Second, it offers traceability of\ngovernance decisions and tracking of training processes, which are also crucial\nin a production environment. Beyond presenting the architectural design, we\nanalyze requirements for the real-world use of FL and evaluate the architecture\nwith a scenario-based analysis method.\n","authors":["F. Stricker","J. A. Peregrina","D. Bermbach","C. Zirpins"],"pdf_url":"https://arxiv.org/pdf/2501.19091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19090v1","updated":"2025-01-31T12:36:31Z","published":"2025-01-31T12:36:31Z","title":"Pivoting Factorization: A Compact Meta Low-Rank Representation of\n  Sparsity for Efficient Inference in Large Language Models","summary":"  The rapid growth of Large Language Models has driven demand for effective\nmodel compression techniques to reduce memory and computation costs. Low-rank\npruning has gained attention for its tensor coherence and GPU compatibility\nacross all densities. However, low-rank pruning has struggled to match the\nperformance of semi-structured pruning, often doubling perplexity (PPL) at\nsimilar densities. In this paper, we propose Pivoting Factorization (PIFA), a\nnovel lossless meta low-rank representation that unsupervisedly learns a\ncompact form of any low-rank representation, effectively eliminating redundant\ninformation. PIFA identifies pivot rows (linearly independent rows) and\nexpresses non-pivot rows as linear combinations, achieving an additional 24.2\\%\nmemory savings and 24.6\\% faster inference over low-rank layers at r/d = 0.5,\nthereby significantly enhancing performance at the same density. To mitigate\nthe performance degradation caused by low-rank pruning, we introduce a novel,\nretraining-free low-rank reconstruction method that minimizes error\naccumulation (M). MPIFA, combining M and PIFA into an end-to-end framework,\nsignificantly outperforms existing low-rank pruning methods and, for the first\ntime, achieves performance comparable to semi-structured pruning, while\nsurpassing it in GPU efficiency and compatibility.\n","authors":["Jialin Zhao","Yingtao Zhang","Carlo Vittorio Cannistraci"],"pdf_url":"https://arxiv.org/pdf/2501.19090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17978v2","updated":"2025-01-31T12:35:35Z","published":"2025-01-29T20:23:48Z","title":"VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting","summary":"  Reconstructing a 3D scene from images is challenging due to the different\nways light interacts with surfaces depending on the viewer's position and the\nsurface's material. In classical computer graphics, materials can be classified\nas diffuse or specular, interacting with light differently. The standard 3D\nGaussian Splatting model struggles to represent view-dependent content, since\nit cannot differentiate an object within the scene from the light interacting\nwith its specular surfaces, which produce highlights or reflections. In this\npaper, we propose to extend the 3D Gaussian Splatting model by introducing an\nadditional symmetric matrix to enhance the opacity representation of each 3D\nGaussian. This improvement allows certain Gaussians to be suppressed based on\nthe viewer's perspective, resulting in a more accurate representation of\nview-dependent reflections and specular highlights without compromising the\nscene's integrity. By allowing the opacity to be view dependent, our enhanced\nmodel achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep\nBlending, and Nerf-Synthetic datasets without a significant loss in rendering\nspeed, achieving >60FPS, and only incurring a minimal increase in memory used.\n","authors":["Mateusz Nowak","Wojciech Jarosz","Peter Chin"],"pdf_url":"https://arxiv.org/pdf/2501.17978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19089v1","updated":"2025-01-31T12:34:09Z","published":"2025-01-31T12:34:09Z","title":"Understanding Oversmoothing in GNNs as Consensus in Opinion Dynamics","summary":"  In contrast to classes of neural networks where the learned representations\nbecome increasingly expressive with network depth, the learned representations\nin graph neural networks (GNNs), tend to become increasingly similar. This\nphenomena, known as oversmoothing, is characterized by learned representations\nthat cannot be reliably differentiated leading to reduced predictive\nperformance. In this paper, we propose an analogy between oversmoothing in GNNs\nand consensus or agreement in opinion dynamics. Through this analogy, we show\nthat the message passing structure of recent continuous-depth GNNs is\nequivalent to a special case of opinion dynamics (i.e., linear consensus\nmodels) which has been theoretically proven to converge to consensus (i.e.,\noversmoothing) for all inputs. Using the understanding developed through this\nanalogy, we design a new continuous-depth GNN model based on nonlinear opinion\ndynamics and prove that our model, which we call behavior-inspired message\npassing neural network (BIMP) circumvents oversmoothing for general inputs.\nThrough extensive experiments, we show that BIMP is robust to oversmoothing and\nadversarial attack, and consistently outperforms competitive baselines on\nnumerous benchmarks.\n","authors":["Keqin Wang","Yulong Yang","Ishan Saha","Christine Allen-Blanchette"],"pdf_url":"https://arxiv.org/pdf/2501.19089v1.pdf","comment":"23 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.06706v2","updated":"2025-01-31T12:25:17Z","published":"2024-08-24T03:27:29Z","title":"SAN: Hypothesizing Long-Term Synaptic Development and Neural Engram\n  Mechanism in Scalable Model's Parameter-Efficient Fine-Tuning","summary":"  Advances in Parameter-Efficient Fine-Tuning (PEFT) bridged the performance\ngap with Full Fine-Tuning (FFT) through sophisticated analysis of pre-trained\nparameter spaces. Starting from drawing insights from Neural Engrams (NE) in\nBiological Neural Networks (BNNs), we establish a connection between the\nlow-rank property observed during PEFT's parameter space shifting and\nneurobiological mechanisms. This observation leads to our proposed method,\nSynapse and Neuron (SAN), which decomposes and propagates scaling components\nfrom anterior feature adjusting vectors towards posterior weight matrices. Our\napproach is theoretically grounded in Long-Term Potentiation/Depression (LTP/D)\nphenomena, which govern synapse development through neurotransmitter release\nmodulation. Extensive experiments demonstrate its effectiveness: on\n\\textbf{vision tasks} across VTAB, FGVC, and GIC (25 datasets) using ViT, SwinT\nand ConvNeXt, SAN outperforms FFT up to 8.7% and LoRA by 3.2%; on language\ntasks using Commonsense Reasoning (8 datasets) with LLaMA models (all\ngenerations), surpassing ChatGPT up to 8.5% and LoRA by 4.7%; on\nvisual-language tasks using Mixed Visual Instruction (7 datasets) with LLaVA\nmodels, it exceeds FFT up to 2.4% and LoRA by 1.9%. Our code and W&B log will\nbe released in https://github.com/daviddaiiiii/SAN-PEFT\n","authors":["Gaole Dai","Chun-Kai Fan","Yiming Tang","Zhi Zhang","Yuan Zhang","Yulu Gan","Qizhe Zhang","Cheng-Ching Tseng","Shanghang Zhang","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2409.06706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19082v1","updated":"2025-01-31T12:15:58Z","published":"2025-01-31T12:15:58Z","title":"A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration","summary":"  Distributed stochastic optimization algorithms can handle large-scale data\nsimultaneously and accelerate model training. However, the sparsity of\ndistributed networks and the heterogeneity of data limit these advantages. This\npaper proposes a momentum-accelerated distributed stochastic gradient\nalgorithm, referred to as Exact-Diffusion with Momentum (EDM), which can\ncorrect the bias caused by data heterogeneity and introduces the momentum\nmethod commonly used in deep learning to accelerate the convergence of the\nalgorithm. We theoretically demonstrate that this algorithm converges to the\nneighborhood of the optimum sub-linearly irrelevant to data heterogeneity when\napplied to non-convex objective functions and linearly under the\nPolyak-{\\L}ojasiewicz condition (a weaker assumption than $\\mu$-strongly\nconvexity). Finally, we evaluate the performance of the proposed algorithm by\nsimulation, comparing it with a range of existing decentralized optimization\nalgorithms to demonstrate its effectiveness in addressing data heterogeneity\nand network sparsity.\n","authors":["Yuchen Hu","Xi Chen","Weidong Liu","Xiaojun Mao"],"pdf_url":"https://arxiv.org/pdf/2501.19082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03340v2","updated":"2025-01-31T12:15:57Z","published":"2024-05-20T13:09:32Z","title":"A Multi-Modal Explainability Approach for Human-Aware Robots in\n  Multi-Party Conversation","summary":"  The addressee estimation (understanding to whom somebody is talking) is a\nfundamental task for human activity recognition in multi-party conversation\nscenarios. Specifically, in the field of human-robot interaction, it becomes\neven more crucial to enable social robots to participate in such interactive\ncontexts. However, it is usually implemented as a binary classification task,\nrestricting the robot's capability to estimate whether it was addressed\n\\review{or not, which} limits its interactive skills. For a social robot to\ngain the trust of humans, it is also important to manifest a certain level of\ntransparency and explainability. Explainable artificial intelligence thus plays\na significant role in the current machine learning applications and models, to\nprovide explanations for their decisions besides excellent performance. In our\nwork, we a) present an addressee estimation model with improved performance in\ncomparison with the previous state-of-the-art; b) further modify this model to\ninclude inherently explainable attention-based segments; c) implement the\nexplainable addressee estimation as part of a modular cognitive architecture\nfor multi-party conversation in an iCub robot; d) validate the real-time\nperformance of the explainable model in multi-party human-robot interaction; e)\npropose several ways to incorporate explainability and transparency in the\naforementioned architecture; and f) perform an online user study to analyze the\neffect of various explanations on how human participants perceive the robot.\n","authors":["Iveta Bečková","Štefan Pócoš","Giulia Belgiovine","Marco Matarese","Omar Eldardeer","Alessandra Sciutti","Carlo Mazzola"],"pdf_url":"https://arxiv.org/pdf/2407.03340v2.pdf","comment":"32pp (+6pp sup.mat.) Accepted in Computer Vision and Image\n  Understanding Journal on January 23, 2025. This research received funding\n  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and\n  Development Agency, project no. APVV-21-0105"},{"id":"http://arxiv.org/abs/2501.19080v1","updated":"2025-01-31T12:11:13Z","published":"2025-01-31T12:11:13Z","title":"Differentially Private Policy Gradient","summary":"  Motivated by the increasing deployment of reinforcement learning in the real\nworld, involving a large consumption of personal data, we introduce a\ndifferentially private (DP) policy gradient algorithm. We show that, in this\nsetting, the introduction of Differential Privacy can be reduced to the\ncomputation of appropriate trust regions, thus avoiding the sacrifice of\ntheoretical properties of the DP-less methods. Therefore, we show that it is\npossible to find the right trade-off between privacy noise and trust-region\nsize to obtain a performant differentially private policy gradient algorithm.\nWe then outline its performance empirically on various benchmarks. Our results\nand the complexity of the tasks addressed represent a significant improvement\nover existing DP algorithms in online RL.\n","authors":["Alexandre Rio","Merwan Barlier","Igor Colin"],"pdf_url":"https://arxiv.org/pdf/2501.19080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19077v1","updated":"2025-01-31T12:09:40Z","published":"2025-01-31T12:09:40Z","title":"Temperature-Annealed Boltzmann Generators","summary":"  Efficient sampling of unnormalized probability densities such as the\nBoltzmann distribution of molecular systems is a longstanding challenge. Next\nto conventional approaches like molecular dynamics or Markov chain Monte Carlo,\nvariational approaches, such as training normalizing flows with the reverse\nKullback-Leibler divergence, have been introduced. However, such methods are\nprone to mode collapse and often do not learn to sample the full\nconfigurational space. Here, we present temperature-annealed Boltzmann\ngenerators (TA-BG) to address this challenge. First, we demonstrate that\ntraining a normalizing flow with the reverse Kullback-Leibler divergence at\nhigh temperatures is possible without mode collapse. Furthermore, we introduce\na reweighting-based training objective to anneal the distribution to lower\ntarget temperatures. We apply this methodology to three molecular systems of\nincreasing complexity and, compared to the baseline, achieve better results in\nalmost all metrics while requiring up to three times fewer target energy\nevaluations. For the largest system, our approach is the only method that\naccurately resolves the metastable states of the system.\n","authors":["Henrik Schopmans","Pascal Friederich"],"pdf_url":"https://arxiv.org/pdf/2501.19077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19073v1","updated":"2025-01-31T12:03:17Z","published":"2025-01-31T12:03:17Z","title":"Pareto-frontier Entropy Search with Variational Lower Bound Maximization","summary":"  This study considers multi-objective Bayesian optimization (MOBO) through the\ninformation gain of the Pareto-frontier. To calculate the information gain, a\npredictive distribution conditioned on the Pareto-frontier plays a key role,\nwhich is defined as a distribution truncated by the Pareto-frontier. However,\nit is usually impossible to obtain the entire Pareto-frontier in a continuous\ndomain, and therefore, the complete truncation cannot be known. We consider an\napproximation of the truncate distribution by using a mixture distribution\nconsisting of two possible approximate truncation obtainable from a subset of\nthe Pareto-frontier, which we call over- and under-truncation. Since the\noptimal balance of the mixture is unknown beforehand, we propose optimizing the\nbalancing coefficient through the variational lower bound maximization\nframework, by which the approximation error of the information gain can be\nminimized. Our empirical evaluation demonstrates the effectiveness of the\nproposed method particularly when the number of objective functions is large.\n","authors":["Masanori Ishikura","Masayuki Karasuyama"],"pdf_url":"https://arxiv.org/pdf/2501.19073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02870v2","updated":"2025-01-31T12:01:15Z","published":"2024-02-05T10:36:48Z","title":"Position: Rethinking Explainable Machine Learning as Applied Statistics","summary":"  In the rapidly growing literature on explanation algorithms, it often remains\nunclear what precisely these algorithms are for and how they should be used. In\nthis position paper, we argue for a novel and pragmatic perspective:\nExplainable machine learning needs to recognize its parallels with applied\nstatistics. Concretely, explanations are statistics of high-dimensional\nfunctions, and we should think about them analogously to traditional\nstatistical quantities. Among others, this implies that we must think carefully\nabout the matter of interpretation, or how the explanations relate to intuitive\nquestions that humans have about the world. The fact that this is scarcely\nbeing discussed in research papers is one of the main drawbacks of the current\nliterature. Luckily, the analogy between explainable machine learning and\napplied statistics suggests fruitful ways for how research practices can be\nimproved.\n","authors":["Sebastian Bordt","Eric Raidl","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2402.02870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19072v1","updated":"2025-01-31T12:00:54Z","published":"2025-01-31T12:00:54Z","title":"SpikingSoft: A Spiking Neuron Controller for Bio-inspired Locomotion\n  with Soft Snake Robots","summary":"  Inspired by the dynamic coupling of moto-neurons and physical elasticity in\nanimals, this work explores the possibility of generating locomotion gaits by\nutilizing physical oscillations in a soft snake by means of a low-level spiking\nneural mechanism. To achieve this goal, we introduce the Double Threshold\nSpiking neuron model with adjustable thresholds to generate varied output\npatterns. This neuron model can excite the natural dynamics of soft robotic\nsnakes, and it enables distinct movements, such as turning or moving forward,\nby simply altering the neural thresholds. Finally, we demonstrate that our\napproach, termed SpikingSoft, naturally pairs and integrates with reinforcement\nlearning. The high-level agent only needs to adjust the two thresholds to\ngenerate complex movement patterns, thus strongly simplifying the learning of\nreactive locomotion. Simulation results demonstrate that the proposed\narchitecture significantly enhances the performance of the soft snake robot,\nenabling it to achieve target objectives with a 21.6% increase in success rate,\na 29% reduction in time to reach the target, and smoother movements compared to\nthe vanilla reinforcement learning controllers or Central Pattern Generator\ncontroller acting in torque space.\n","authors":["Chuhan Zhang","Cong Wang","Wei Pan","Cosimo Della Santina"],"pdf_url":"https://arxiv.org/pdf/2501.19072v1.pdf","comment":"8th IEEE-RAS International Conference on Soft Robotics"}],"Multi Media":[{"id":"http://arxiv.org/abs/2501.11403v2","updated":"2025-01-31T10:08:36Z","published":"2025-01-20T11:06:05Z","title":"Verifying Cross-modal Entity Consistency in News using Vision-language\n  Models","summary":"  The web has become a crucial source of information, but it is also used to\nspread disinformation, often conveyed through multiple modalities like images\nand text. The identification of inconsistent cross-modal information, in\nparticular entities such as persons, locations, and events, is critical to\ndetect disinformation. Previous works either identify out-of-context\ndisinformation by assessing the consistency of images to the whole document,\nneglecting relations of individual entities, or focus on generic entities that\nare not relevant to news. So far, only few approaches have addressed the task\nof validating entity consistency between images and text in news. However, the\npotential of large vision-language models (LVLMs) has not been explored yet. In\nthis paper, we propose an LVLM-based framework for verifying Cross-modal Entity\nConsistency~(LVLM4CEC), to assess whether persons, locations and events in news\narticles are consistent across both modalities. We suggest effective prompting\nstrategies for LVLMs for entity verification that leverage reference images\ncrawled from web. Moreover, we extend three existing datasets for the task of\nentity verification in news providing manual ground-truth data. Our results\nshow the potential of LVLMs for automating cross-modal entity verification,\nshowing improved accuracy in identifying persons and events when using evidence\nimages. Moreover, our method outperforms a baseline for location and event\nverification in documents. The datasets and source code are available on GitHub\nat https://github.com/TIBHannover/LVLM4CEC.\n","authors":["Sahar Tahmasebi","David Ernst","Eric Müller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2501.11403v2.pdf","comment":"Accepted for publication in: European Conference on Information\n  Retrieval (ECIR) 2025"}]},"2025-01-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.01030v4","updated":"2025-01-30T23:31:25Z","published":"2024-11-01T21:01:13Z","title":"Birdie: Advancing State Space Models with Reward-Driven Objectives and\n  Curricula","summary":"  Efficient state space models (SSMs), such as linear recurrent neural networks\nand linear attention variants, offer computational advantages over Transformers\nbut struggle with tasks requiring long-range in-context retrieval-like text\ncopying, associative recall, and question answering over long contexts.\nPrevious efforts to address these challenges have focused on architectural\nmodifications, often reintroducing computational inefficiencies. In this paper,\nwe propose a novel training procedure, Birdie, that significantly enhances the\nin-context retrieval capabilities of SSMs without altering their architecture.\nOur approach combines bidirectional input processing with dynamic mixtures of\nspecialized pre-training objectives, optimized via reinforcement learning. We\nintroduce a new bidirectional SSM architecture that seamlessly transitions from\nbidirectional context processing to causal generation. Experimental evaluations\ndemonstrate that Birdie markedly improves performance on retrieval-intensive\ntasks such as multi-number phone book lookup, long paragraph\nquestion-answering, and infilling. This narrows the performance gap with\nTransformers, while retaining computational efficiency. Our findings highlight\nthe importance of training procedures in leveraging the fixed-state capacity of\nSSMs, offering a new direction to advance their capabilities. All code and\npre-trained models are available at https://www.github.com/samblouir/birdie,\nwith support for JAX and PyTorch.\n","authors":["Sam Blouir","Jimmy T. H. Smith","Antonios Anastasopoulos","Amarda Shehu"],"pdf_url":"https://arxiv.org/pdf/2411.01030v4.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2410.14817v3","updated":"2025-01-30T23:21:29Z","published":"2024-10-18T18:37:27Z","title":"A Complexity-Based Theory of Compositionality","summary":"  Compositionality is believed to be fundamental to intelligence. In humans, it\nunderlies the structure of thought, language, and higher-level reasoning. In\nAI, compositional representations can enable a powerful form of\nout-of-distribution generalization, in which a model systematically adapts to\nnovel combinations of known concepts. However, while we have strong intuitions\nabout what compositionality is, there currently exists no formal definition for\nit that is measurable and mathematical. Here, we propose such a definition,\nwhich we call representational compositionality, that accounts for and extends\nour intuitions about compositionality. The definition is conceptually simple,\nquantitative, grounded in algorithmic information theory, and applicable to any\nrepresentation. Intuitively, representational compositionality states that a\ncompositional representation satisfies three properties. First, it must be\nexpressive. Second, it must be possible to re-describe the representation as a\nfunction of discrete symbolic sequences with re-combinable parts, analogous to\nsentences in natural language. Third, the function that relates these symbolic\nsequences to the representation, analogous to semantics in natural language,\nmust be simple. Through experiments on both synthetic and real world data, we\nvalidate our definition of compositionality and show how it unifies disparate\nintuitions from across the literature in both AI and cognitive science. We also\nshow that representational compositionality, while theoretically intractable,\ncan be readily estimated using standard deep learning tools. Our definition has\nthe potential to inspire the design of novel, theoretically-driven models that\nbetter capture the mechanisms of compositional thought.\n","authors":["Eric Elmoznino","Thomas Jiralerspong","Yoshua Bengio","Guillaume Lajoie"],"pdf_url":"https://arxiv.org/pdf/2410.14817v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18795v1","updated":"2025-01-30T23:05:57Z","published":"2025-01-30T23:05:57Z","title":"Rope to Nope and Back Again: A New Hybrid Attention Strategy","summary":"  Long-context large language models (LLMs) have achieved remarkable\nadvancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et\nal., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et\nal., 2023). By adjusting RoPE parameters and incorporating training data with\nextended contexts, we can train performant models with considerably longer\ninput sequences. However, existing RoPE-based methods exhibit performance\nlimitations when applied to extended context lengths. This paper presents a\ncomprehensive analysis of various attention mechanisms, including RoPE, No\nPositional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying\ntheir strengths and shortcomings in long-context modeling. Our investigation\nidentifies distinctive attention patterns in these methods and highlights their\nimpact on long-context performance, providing valuable insights for\narchitectural design. Building on these findings, we propose a novel\narchitectural based on a hybrid attention mechanism that not only surpasses\nconventional RoPE-based transformer models in long context tasks but also\nachieves competitive performance on benchmarks requiring shorter context\nlengths.\n","authors":["Bowen Yang","Bharat Venkitesh","Dwarak Talupuru","Hangyu Lin","David Cairuz","Phil Blunsom","Acyr Locatelli"],"pdf_url":"https://arxiv.org/pdf/2501.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04960v2","updated":"2025-01-30T22:55:18Z","published":"2024-03-08T00:02:30Z","title":"IsolateGPT: An Execution Isolation Architecture for LLM-Based Agentic\n  Systems","summary":"  Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nnatural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we evaluate whether these issues can be\naddressed through execution isolation and what that isolation might look like\nin the context of LLM-based systems, where there are arbitrary natural\nlanguage-based interactions between system components, between LLM and apps,\nand between apps. To that end, we propose IsolateGPT, a design architecture\nthat demonstrates the feasibility of execution isolation and provides a\nblueprint for implementing isolation, in LLM-based systems. We evaluate\nIsolateGPT against a number of attacks and demonstrate that it protects against\nmany security, privacy, and safety issues that exist in non-isolated LLM-based\nsystems, without any loss of functionality. The performance overhead incurred\nby IsolateGPT to improve security is under 30% for three-quarters of tested\nqueries.\n","authors":["Yuhao Wu","Franziska Roesner","Tadayoshi Kohno","Ning Zhang","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2403.04960v2.pdf","comment":"Accepted by the Network and Distributed System Security (NDSS)\n  Symposium 2025"},{"id":"http://arxiv.org/abs/2402.14547v6","updated":"2025-01-30T22:12:27Z","published":"2024-02-22T13:36:53Z","title":"OmniPred: Language Models as Universal Regressors","summary":"  Regression is a powerful tool to accurately predict the outcome metric of a\nsystem given a set of parameters, but has traditionally been restricted to\nmethods which are only applicable to a specific task. In this paper, we propose\nOmniPred, a framework for training language models as universal end-to-end\nregressors over $(x,y)$ data from arbitrary formats. Using data sourced from\nGoogle Vizier, one of the largest proprietary blackbox optimization databases\nin the world, our extensive experiments demonstrate that language models are\ncapable of very precise numerical regression using only textual representations\nof mathematical parameters and values, and if given the opportunity to train at\nscale over multiple tasks, can significantly outperform traditional regression\nmodels.\n","authors":["Xingyou Song","Oscar Li","Chansoo Lee","Bangding Yang","Daiyi Peng","Sagi Perel","Yutian Chen"],"pdf_url":"https://arxiv.org/pdf/2402.14547v6.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR) 2024.\n  Code can be found in\n  https://github.com/google-research/optformer/tree/main/optformer/omnipred"},{"id":"http://arxiv.org/abs/2501.18771v1","updated":"2025-01-30T21:51:18Z","published":"2025-01-30T21:51:18Z","title":"Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data\n  Contamination's Impact on Machine Translation","summary":"  Data contamination -- the accidental consumption of evaluation examples\nwithin the pre-training data -- can undermine the validity of evaluation\nbenchmarks. In this paper, we present a rigorous analysis of the effects of\ncontamination on language models at 1B and 8B scales on the machine translation\ntask. Starting from a carefully decontaminated train-test split, we\nsystematically introduce contamination at various stages, scales, and data\nformats to isolate its effect and measure its impact on performance metrics.\nOur experiments reveal that contamination with both source and target\nsubstantially inflates BLEU scores, and this inflation is 2.5 times larger (up\nto 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and\ntarget-only contamination generally produce smaller, less consistent\nover-estimations. Finally, we study how the temporal distribution and frequency\nof contaminated samples influence performance over-estimation across languages\nwith varying degrees of data resources.\n","authors":["Muhammed Yusuf Kocyigit","Eleftheria Briakou","Daniel Deutsch","Jiaming Luo","Colin Cherry","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2501.18771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18766v1","updated":"2025-01-30T21:41:26Z","published":"2025-01-30T21:41:26Z","title":"Breaking the Fake News Barrier: Deep Learning Approaches in Bangla\n  Language","summary":"  The rapid development of digital stages has greatly compounded the dispersal\nof untrue data, dissolving certainty and judgment in society, especially among\nthe Bengali-speaking community. Our ponder addresses this critical issue by\npresenting an interesting strategy that utilizes a profound learning\ninnovation, particularly the Gated Repetitive Unit (GRU), to recognize fake\nnews within the Bangla dialect. The strategy of our proposed work incorporates\nintensive information preprocessing, which includes lemmatization,\ntokenization, and tending to course awkward nature by oversampling. This comes\nabout in a dataset containing 58,478 passages. We appreciate the creation of a\ndemonstration based on GRU (Gated Repetitive Unit) that illustrates remarkable\nexecution with a noteworthy precision rate of 94%. This ponder gives an\nintensive clarification of the methods included in planning the information,\nselecting the show, preparing it, and assessing its execution. The performance\nof the model is investigated by reliable metrics like precision, recall, F1\nscore, and accuracy. The commitment of the work incorporates making a huge fake\nnews dataset in Bangla and a demonstration that has outperformed other Bangla\nfake news location models.\n","authors":["Pronoy Kumar Mondal","Sadman Sadik Khan","Md. Masud Rana","Shahriar Sultan Ramit","Abdus Sattar","Md. Sadekur Rahman"],"pdf_url":"https://arxiv.org/pdf/2501.18766v1.pdf","comment":"6 pages, THE 15th INTERNATIONAL IEEE CONFERENCE ON COMPUTING,\n  COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)"},{"id":"http://arxiv.org/abs/2501.18750v1","updated":"2025-01-30T21:00:47Z","published":"2025-01-30T21:00:47Z","title":"Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity\n  Recognition in Low-Resource Languages","summary":"  Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer\nbetween languages to identify and classify named entities, making it\nparticularly useful for low-resource languages. We show that the data-based\ncross-lingual transfer method is an effective technique for crosslingual NER\nand can outperform multilingual language models for low-resource languages.\nThis paper introduces two key enhancements to the annotation projection step in\ncross-lingual NER for low-resource languages. First, we explore refining word\nalignments using back-translation to improve accuracy. Second, we present a\nnovel formalized projection approach of matching source entities with extracted\ntarget candidates. Through extensive experiments on two datasets spanning 57\nlanguages, we demonstrated that our approach surpasses existing projectionbased\nmethods in low-resource settings. These findings highlight the robustness of\nprojection-based data transfer as an alternative to model-based methods for\ncrosslingual named entity recognition in lowresource languages.\n","authors":["Andrei Politov","Oleh Shkalikov","René Jäkel","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2501.18750v1.pdf","comment":"Accepted at NoDaLiDa/Baltic-HLT 2025"},{"id":"http://arxiv.org/abs/2501.18738v1","updated":"2025-01-30T20:33:59Z","published":"2025-01-30T20:33:59Z","title":"Examining the Robustness of Large Language Models across Language\n  Complexity","summary":"  With the advancement of large language models (LLMs), an increasing number of\nstudent models have leveraged LLMs to analyze textual artifacts generated by\nstudents to understand and evaluate their learning. These student models\ntypically employ pre-trained LLMs to vectorize text inputs into embeddings and\nthen use the embeddings to train models to detect the presence or absence of a\nconstruct of interest. However, how reliable and robust are these models at\nprocessing language with different levels of complexity? In the context of\nlearning where students may have different language backgrounds with various\nlevels of writing skills, it is critical to examine the robustness of such\nmodels to ensure that these models work equally well for text with varying\nlevels of language complexity. Coincidentally, a few (but limited) research\nstudies show that the use of language can indeed impact the performance of\nLLMs. As such, in the current study, we examined the robustness of several\nLLM-based student models that detect student self-regulated learning (SRL) in\nmath problem-solving. Specifically, we compared how the performance of these\nmodels vary using texts with high and low lexical, syntactic, and semantic\ncomplexity measured by three linguistic measures.\n","authors":["Jiayi Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18731v1","updated":"2025-01-30T20:17:17Z","published":"2025-01-30T20:17:17Z","title":"Evaluating Spoken Language as a Biomarker for Automated Screening of\n  Cognitive Impairment","summary":"  Timely and accurate assessment of cognitive impairment is a major unmet need\nin populations at risk. Alterations in speech and language can be early\npredictors of Alzheimer's disease and related dementias (ADRD) before clinical\nsigns of neurodegeneration. Voice biomarkers offer a scalable and non-invasive\nsolution for automated screening. However, the clinical applicability of\nmachine learning (ML) remains limited by challenges in generalisability,\ninterpretability, and access to patient data to train clinically applicable\npredictive models. Using DementiaBank recordings (N=291, 64% female), we\nevaluated ML techniques for ADRD screening and severity prediction from spoken\nlanguage. We validated model generalisability with pilot data collected\nin-residence from older adults (N=22, 59% female). Risk stratification and\nlinguistic feature importance analysis enhanced the interpretability and\nclinical utility of predictions. For ADRD classification, a Random Forest\napplied to lexical features achieved a mean sensitivity of 69.4% (95%\nconfidence interval (CI) = 66.4-72.5) and specificity of 83.3% (78.0-88.7). On\nreal-world pilot data, this model achieved a mean sensitivity of 70.0%\n(58.0-82.0) and specificity of 52.5% (39.3-65.7). For severity prediction using\nMini-Mental State Examination (MMSE) scores, a Random Forest Regressor achieved\na mean absolute MMSE error of 3.7 (3.7-3.8), with comparable performance of 3.3\n(3.1-3.5) on pilot data. Linguistic features associated with higher ADRD risk\nincluded increased use of pronouns and adverbs, greater disfluency, reduced\nanalytical thinking, lower lexical diversity and fewer words reflecting a\npsychological state of completion. Our interpretable predictive modelling\noffers a novel approach for in-home integration with conversational AI to\nmonitor cognitive health and triage higher-risk individuals, enabling earlier\ndetection and intervention.\n","authors":["Maria R. Lima","Alexander Capstick","Fatemeh Geranmayeh","Ramin Nilforooshan","Maja Matarić","Ravi Vaidyanathan","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2501.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00966v2","updated":"2025-01-30T20:13:25Z","published":"2024-12-01T21:06:08Z","title":"From Priest to Doctor: Domain Adaptaion for Low-Resource Neural Machine\n  Translation","summary":"  Many of the world's languages have insufficient data to train high-performing\ngeneral neural machine translation (NMT) models, let alone domain-specific\nmodels, and often the only available parallel data are small amounts of\nreligious texts. Hence, domain adaptation (DA) is a crucial issue faced by\ncontemporary NMT and has, so far, been underexplored for low-resource\nlanguages. In this paper, we evaluate a set of methods from both low-resource\nNMT and DA in a realistic setting, in which we aim to translate between a\nhigh-resource and a low-resource language with access to only: a) parallel\nBible data, b) a bilingual dictionary, and c) a monolingual target-domain\ncorpus in the high-resource language. Our results show that the effectiveness\nof the tested methods varies, with the simplest one, DALI, being most\neffective. We follow up with a small human evaluation of DALI, which shows that\nthere is still a need for more careful investigation of how to accomplish DA\nfor low-resource NMT.\n","authors":["Ali Marashian","Enora Rice","Luke Gessler","Alexis Palmer","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2412.00966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10150v2","updated":"2025-01-30T20:11:45Z","published":"2025-01-17T12:23:30Z","title":"Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair\n  Language Modeling and Translation","summary":"  Mitigation of biases, such as language models' reliance on gender\nstereotypes, is a crucial endeavor required for the creation of reliable and\nuseful language technology. The crucial aspect of debiasing is to ensure that\nthe models preserve their versatile capabilities, including their ability to\nsolve language tasks and equitably represent various genders. To address this\nissue, we introduce a streamlined Dual Dabiasing Algorithm through Model\nAdaptation (2DAMA). Novel Dual Debiasing enables robust reduction of\nstereotypical bias while preserving desired factual gender information encoded\nby language models. We show that 2DAMA effectively reduces gender bias in\nEnglish and is one of the first approaches facilitating the mitigation of\nstereotypical tendencies in translation. The proposed method's key advantage is\nthe preservation of factual gender cues, which are useful in a wide range of\nnatural language processing tasks.\n","authors":["Tomasz Limisiewicz","David Mareček","Tomáš Musil"],"pdf_url":"https://arxiv.org/pdf/2501.10150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18724v1","updated":"2025-01-30T19:58:45Z","published":"2025-01-30T19:58:45Z","title":"Zero-shot Large Language Models for Long Clinical Text Summarization\n  with Temporal Reasoning","summary":"  Recent advancements in large language models (LLMs) have shown potential for\ntransforming data processing in healthcare, particularly in understanding\ncomplex clinical narratives. This study evaluates the efficacy of zero-shot\nLLMs in summarizing long clinical texts that require temporal reasoning, a\ncritical aspect for comprehensively capturing patient histories and treatment\ntrajectories. We applied a series of advanced zero-shot LLMs to extensive\nclinical documents, assessing their ability to integrate and accurately reflect\ntemporal dynamics without prior task-specific training. While the models\nefficiently identified key temporal events, they struggled with chronological\ncoherence over prolonged narratives. The evaluation, combining quantitative and\nqualitative methods, highlights the strengths and limitations of zero-shot LLMs\nin clinical text summarization. The results suggest that while promising,\nzero-shot LLMs require further refinement to effectively support clinical\ndecision-making processes, underscoring the need for enhanced model training\napproaches that better capture the nuances of temporal information in long\ncontext medical documents.\n","authors":["Maya Kruse","Shiyue Hu","Nicholas Derby","Yifu Wu","Samantha Stonbraker","Bingsheng Yao","Dakuo Wang","Elizabeth Goldberg","Yanjun Gao"],"pdf_url":"https://arxiv.org/pdf/2501.18724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16464v2","updated":"2025-01-30T18:58:30Z","published":"2024-10-21T19:46:06Z","title":"Beyond Browsing: API-Based Web Agents","summary":"  Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by browsing agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-based agents outperform web browsing agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n20.0% absolute improvement over web browsing alone, achieving a success rate of\n35.8%, achiving the SOTA performance among task-agnostic agents. These results\nstrongly suggest that when APIs are available, they present an attractive\nalternative to relying on web browsing alone.\n","authors":["Yueqi Song","Frank Xu","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16464v2.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.18585v1","updated":"2025-01-30T18:58:18Z","published":"2025-01-30T18:58:18Z","title":"Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs","summary":"  Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable\nabilities in complex reasoning tasks by scaling test-time compute and\nexhibiting human-like deep thinking. However, we identify a phenomenon we term\nunderthinking, where o1-like LLMs frequently switch between different reasoning\nthoughts without sufficiently exploring promising paths to reach a correct\nsolution. This behavior leads to inadequate depth of reasoning and decreased\nperformance, particularly on challenging mathematical problems. To\nsystematically analyze this issue, we conduct experiments on three challenging\ntest sets and two representative open-source o1-like models, revealing that\nfrequent thought switching correlates with incorrect responses. We introduce a\nnovel metric to quantify underthinking by measuring token efficiency in\nincorrect answers. To address underthinking, we propose a decoding strategy\nwith thought switching penalty TIP that discourages premature transitions\nbetween thoughts, encouraging deeper exploration of each reasoning path.\nExperimental results demonstrate that our approach improves accuracy across\nchallenging datasets without requiring model fine-tuning. Our findings\ncontribute to understanding reasoning inefficiencies in o1-like LLMs and offer\na practical solution to enhance their problem-solving capabilities.\n","authors":["Yue Wang","Qiuzhi Liu","Jiahao Xu","Tian Liang","Xingyu Chen","Zhiwei He","Linfeng Song","Dian Yu","Juntao Li","Zhuosheng Zhang","Rui Wang","Zhaopeng Tu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2501.18585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18578v1","updated":"2025-01-30T18:50:25Z","published":"2025-01-30T18:50:25Z","title":"R.I.P.: Better Models by Survival of the Fittest Prompts","summary":"  Training data quality is one of the most important drivers of final model\nquality. In this work, we introduce a method for evaluating data integrity\nbased on the assumption that low-quality input prompts result in high variance\nand low quality responses. This is achieved by measuring the rejected response\nquality and the reward gap between the chosen and rejected preference pair. Our\nmethod, Rejecting Instruction Preferences (RIP) can be used to filter prompts\nfrom existing training sets, or to make high quality synthetic datasets,\nyielding large performance gains across various benchmarks compared to\nunfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win\nRate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama\n3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th\nplace to 6th overall in the leaderboard.\n","authors":["Ping Yu","Weizhe Yuan","Olga Golovneva","Tianhao Wu","Sainbayar Sukhbaatar","Jason Weston","Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2501.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07176v3","updated":"2025-01-30T18:17:13Z","published":"2024-11-11T17:56:28Z","title":"More Expressive Attention with Negative Weights","summary":"  We propose a novel attention mechanism, named Cog Attention, that enables\nattention weights to be negative for enhanced expressiveness, which stems from\ntwo key factors: (1) Cog Attention enhances parameter flexibility. For example,\nunlike traditional softmax attention heads that use a static output-value (OV)\nmatrix to delete or copy inputs that the heads attend to, Cog Attention\nnaturally learns to use the sign of dynamic query-key (QK) inner products to\nrepresent these operations. This enables Cog Attention to perform multiple\noperations simultaneously within a single head. Meanwhile, Cog Attention's OV\nmatrix can focus more on refinement or modification. (2) Cog Attention enhances\nthe model's robustness against representational collapse by preventing the\n``over-squashing'' of earlier tokens into later positions. We develop\nTransformer-like models which use Cog Attention as attention modules, including\ndecoder-only models at various scales for language modeling and U-ViT diffusion\nmodels for image generation. Experiments show that models using Cog Attention\nexhibit superior performance compared to those employing traditional softmax\nattention modules. Our approach suggests a promising research direction for\nrethinking and breaking the entrenched constraints of traditional softmax\nattention, such as the requirement for non-negative weights.\n","authors":["Ang Lv","Ruobing Xie","Shuaipeng Li","Jiayi Liao","Xingwu Sun","Zhanhui Kang","Di Wang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2411.07176v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14883v2","updated":"2025-01-30T18:13:05Z","published":"2025-01-24T19:17:06Z","title":"Verify with Caution: The Pitfalls of Relying on Imperfect Factuality\n  Metrics","summary":"  Improvements in large language models have led to increasing optimism that\nthey can serve as reliable evaluators of natural language generation outputs.\nIn this paper, we challenge this optimism by thoroughly re-evaluating five\nstate-of-the-art factuality metrics on a collection of 11 datasets for\nsummarization, retrieval-augmented generation, and question answering. We find\nthat these evaluators are inconsistent with each other and often misestimate\nsystem-level performance, both of which can lead to a variety of pitfalls. We\nfurther show that these metrics exhibit biases against highly paraphrased\noutputs and outputs that draw upon faraway parts of the source documents. We\nurge users of these factuality metrics to proceed with caution and manually\nvalidate the reliability of these metrics in their domain of interest before\nproceeding.\n","authors":["Ameya Godbole","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2501.14883v2.pdf","comment":"v2: Added Acknowledgements to funding sources and advisors"},{"id":"http://arxiv.org/abs/2501.18539v1","updated":"2025-01-30T18:07:19Z","published":"2025-01-30T18:07:19Z","title":"Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method","summary":"  Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches.\n","authors":["Peter Baile Chen","Yi Zhang","Michael Cafarella","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2501.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18533v1","updated":"2025-01-30T17:59:45Z","published":"2025-01-30T17:59:45Z","title":"Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models","summary":"  Large Vision-Language Models (VLMs) have achieved remarkable performance\nacross a wide range of tasks. However, their deployment in safety-critical\ndomains poses significant challenges. Existing safety fine-tuning methods,\nwhich focus on textual or multimodal content, fall short in addressing\nchallenging cases or disrupt the balance between helpfulness and harmlessness.\nOur evaluation highlights a safety reasoning gap: these methods lack safety\nvisual reasoning ability, leading to such bottlenecks. To address this\nlimitation and enhance both visual perception and reasoning in safety-critical\ncontexts, we propose a novel dataset that integrates multi-image inputs with\nsafety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve\nmodel performance. Specifically, we introduce the Multi-Image Safety (MIS)\ndataset, an instruction-following dataset tailored for multi-image safety\nscenarios, consisting of training and test splits. Our experiments demonstrate\nthat fine-tuning InternVL2.5-8B with MIS significantly outperforms both\npowerful open-source models and API-based models in challenging multi-image\ntasks requiring safety-related visual reasoning. This approach not only\ndelivers exceptional safety performance but also preserves general capabilities\nwithout any trade-offs. Specifically, fine-tuning with MIS increases average\naccuracy by 0.83% across five general benchmarks and reduces the Attack Success\nRate (ASR) on multiple safety benchmarks by a large margin. Data and Models are\nreleased under:\n\\href{https://dripnowhy.github.io/MIS/}{\\texttt{https://dripnowhy.github.io/MIS/}}\n","authors":["Yi Ding","Lijun Li","Bing Cao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2501.18533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17635v2","updated":"2025-01-30T17:59:08Z","published":"2025-01-29T13:12:01Z","title":"In-Context Meta LoRA Generation","summary":"  Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.\n","authors":["Yihua Shao","Minxi Yan","Yang Liu","Siyu Chen","Wenjie Chen","Xinwei Long","Ziyang Yan","Lei Li","Chenyu Zhang","Nicu Sebe","Hao Tang","Yan Wang","Hao Zhao","Mengzhu Wang","Jingcai Guo"],"pdf_url":"https://arxiv.org/pdf/2501.17635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17703v2","updated":"2025-01-30T17:58:54Z","published":"2025-01-29T15:20:30Z","title":"Critique Fine-Tuning: Learning to Critique is More Effective than\n  Learning to Imitate","summary":"  Supervised Fine-Tuning (SFT) is commonly used to train language models to\nimitate annotated responses for given instructions. In this paper, we challenge\nthis paradigm and propose Critique Fine-Tuning (CFT), a strategy where models\nlearn to critique noisy responses rather than simply imitate correct ones.\nInspired by human learning processes that emphasize critical thinking, CFT\nencourages deeper analysis and nuanced understanding-traits often overlooked by\nstandard SFT. To validate the effectiveness of CFT, we construct a 50K-sample\ndataset from WebInstruct, using GPT-4o as the teacher to generate critiques in\nthe form of ([query; noisy response], critique). CFT on this dataset yields a\nconsistent 4-10% improvement over SFT on six math benchmarks with different\nbase models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to\nMetaMath and NuminaMath datasets and observe similar gains over SFT. Notably,\nour model Qwen2.5-Math-CFT only requires 1 hour training on 8xH100 over the 50K\nexamples. It can match or outperform strong competitors like\nQwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover,\nit can match the performance of SimpleRL, which is a deepseek-r1 replication\ntrained with 140x more compute. Ablation studies show that CFT is robust to the\nsource of noisy response and teacher critique model. Through these findings, we\nargue that CFT offers a more effective alternative to advance the reasoning of\nlanguage models.\n","authors":["Yubo Wang","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2501.17703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18532v1","updated":"2025-01-30T17:58:36Z","published":"2025-01-30T17:58:36Z","title":"Differentially Private Steering for Large Language Model Alignment","summary":"  Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe \\textit{\\underline{P}rivate \\underline{S}teering for LLM\n\\underline{A}lignment (PSA)} algorithm to edit LLM activations with\ndifferential privacy (DP) guarantees. We conduct extensive experiments on seven\ndifferent benchmarks with open-source LLMs of different sizes (0.5B to 7B) and\nmodel families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA\nachieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our attack is tailored for activation\nediting and relies solely on the generated texts without their associated\nprobabilities. Our experiments support the theoretical guarantees by showing\nimproved guarantees for our \\textit{PSA} algorithm compared to several existing\nnon-private techniques.\n","authors":["Anmol Goel","Yaxi Hu","Iryna Gurevych","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2501.18532v1.pdf","comment":"ICLR 2025; Code: https://github.com/UKPLab/iclr2025-psa"},{"id":"http://arxiv.org/abs/2501.13919v2","updated":"2025-01-30T17:35:08Z","published":"2025-01-23T18:58:03Z","title":"Temporal Preference Optimization for Long-Form Video Understanding","summary":"  Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.\n","authors":["Rui Li","Xiaohan Wang","Yuhui Zhang","Zeyu Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.13919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06595v3","updated":"2025-01-30T17:34:51Z","published":"2024-09-10T15:39:32Z","title":"GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.\n","authors":["Sacha Muller","António Loison","Bilel Omrani","Gautier Viaud"],"pdf_url":"https://arxiv.org/pdf/2409.06595v3.pdf","comment":"Proceedings of the 31st International Conference on Computational\n  Linguistics"},{"id":"http://arxiv.org/abs/2406.20095v3","updated":"2025-01-30T17:34:37Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Vision Language Models (VLMs) have recently been leveraged to generate\nrobotic actions, forming Vision-Language-Action (VLA) models. However, directly\nadapting a pretrained VLM for robotic control remains challenging, particularly\nwhen constrained by a limited number of robot demonstrations. In this work, we\nintroduce LLaRA: Large Language and Robotics Assistant, a framework that\nformulates robot action policy as visuo-textual conversations and enables an\nefficient transfer of a pretrained VLM into a powerful VLA, motivated by the\nsuccess of visual instruction tuning in Computer Vision. First, we present an\nautomated pipeline to generate conversation-style instruction tuning data for\nrobots from existing behavior cloning datasets, aligning robotic actions with\nimage pixel coordinates. Further, we enhance this dataset in a self-supervised\nmanner by defining six auxiliary tasks, without requiring any additional action\nannotations. We show that a VLM finetuned with a limited amount of such\ndatasets can produce meaningful action decisions for robotic control. Through\nexperiments across multiple simulated and real-world tasks, we demonstrate that\nLLaRA achieves state-of-the-art performance while preserving the generalization\ncapabilities of large language models. The code, datasets, and pretrained\nmodels are available at https://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.18512v1","updated":"2025-01-30T17:23:50Z","published":"2025-01-30T17:23:50Z","title":"Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch","summary":"  Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.\n","authors":["Arthur Douillard","Yanislav Donchev","Keith Rush","Satyen Kale","Zachary Charles","Zachary Garrett","Gabriel Teston","Dave Lacey","Ross McIlroy","Jiajun Shen","Alexandre Ramé","Arthur Szlam","Marc'Aurelio Ranzato","Paul Barham"],"pdf_url":"https://arxiv.org/pdf/2501.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18511v1","updated":"2025-01-30T17:21:44Z","published":"2025-01-30T17:21:44Z","title":"WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in\n  Post-Training","summary":"  Language model (LLM) post-training, from DPO to distillation, can refine\nbehaviors and unlock new skills, but the open science supporting these\npost-training techniques is still in its infancy. One limiting factor has been\nthe difficulty of conducting large-scale comparative analyses of synthetic data\ngenerating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,\nthe largest public chat dataset to date. We extend the existing WildChat\ndataset to include responses not only from GPT, but from over 50 different\nopen-weight models, ranging in size from 0.5B to 104B parameters. We conduct an\nextensive comparative analysis and demonstrate the potential of this dataset by\ncreating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3\nSFT mixture from Allen AI with only 40% as many samples. Our dataset, samples\nand code are available at https://github.com/penfever/wildchat-50m.\n","authors":["Benjamin Feuer","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2501.18511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16273v2","updated":"2025-01-30T16:44:45Z","published":"2025-01-27T18:06:36Z","title":"Return of the Encoder: Maximizing Parameter Efficiency for SLMs","summary":"  The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.\n","authors":["Mohamed Elfeki","Rui Liu","Chad Voegele"],"pdf_url":"https://arxiv.org/pdf/2501.16273v2.pdf","comment":"13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only"},{"id":"http://arxiv.org/abs/2501.16673v2","updated":"2025-01-30T16:40:12Z","published":"2025-01-28T03:18:48Z","title":"LLM-AutoDiff: Auto-Differentiate Any LLM Workflow","summary":"  Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research.\n","authors":["Li Yin","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2501.16673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03249v3","updated":"2025-01-30T16:31:31Z","published":"2024-10-04T09:14:11Z","title":"How Much Can We Forget about Data Contamination?","summary":"  The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we challenge the common assumption that small-scale\ncontamination renders benchmark evaluations invalid. First, we experimentally\nquantify the magnitude of benchmark overfitting based on scaling along three\ndimensions: The number of model parameters (up to 1.6B), the number of times an\nexample is seen (up to 144), and the number of training tokens (up to 40B). If\nmodel and data follow the Chinchilla scaling laws, minor contamination indeed\nleads to overfitting. At the same time, even 144 times of contamination can be\nforgotten if the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. Continual pre-training of OLMo-7B\ncorroborates these results. Next, we study the impact of the weight decay\nparameter on example forgetting, showing that empirical forgetting occurs\nfaster than the cumulative weight decay. This allows us to gauge the degree of\nexample forgetting in large-scale training runs, indicating that many LLMs,\nincluding Lllama 3 405B, have forgotten the data seen at the beginning of\ntraining.\n","authors":["Sebastian Bordt","Suraj Srinivas","Valentyn Boreiko","Ulrike von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2410.03249v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16727v2","updated":"2025-01-30T16:17:56Z","published":"2025-01-28T06:07:58Z","title":"xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking","summary":"  Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak.\n","authors":["Sunbowen Lee","Shiwen Ni","Chi Wei","Shuaimin Li","Liyang Fan","Ahmadreza Argha","Hamid Alinejad-Rokny","Ruifeng Xu","Yicheng Gong","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2501.16727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18457v1","updated":"2025-01-30T16:15:38Z","published":"2025-01-30T16:15:38Z","title":"CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language\n  Model Question Answering","summary":"  Large Language Models (LLMs) are pretrained on extensive multilingual corpora\nto acquire both language-specific cultural knowledge and general knowledge.\nIdeally, while LLMs should provide consistent responses to culture-independent\nquestions across languages, we observe significant performance disparities. To\naddress this, we explore the Cross-Lingual Self-Aligning ability of Language\nModels (CALM) to align knowledge across languages. Specifically, for a given\nquestion, we sample multiple responses across different languages, and select\nthe most self-consistent response as the target, leaving the remaining\nresponses as negative examples. We then employ direct preference optimization\n(DPO) to align the model's knowledge across different languages. Evaluations on\nthe MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing\ncross-lingual knowledge question answering, both in zero-shot and retrieval\naugmented settings. We also found that increasing the number of languages\ninvolved in CALM training leads to even higher accuracy and consistency. We\noffer a qualitative analysis of how cross-lingual consistency can enhance\nknowledge alignment and explore the method's generalizability. The source code\nand data of this paper are available on GitHub.\n","authors":["Yumeng Wang","Zhiyuan Fan","Qingyun Wang","May Fung","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2501.18457v1.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2410.18417v2","updated":"2025-01-30T15:45:45Z","published":"2024-10-24T04:02:30Z","title":"Large Language Models Reflect the Ideology of their Creators","summary":"  Large language models (LLMs) are trained on vast amounts of data to generate\nnatural language, enabling them to perform tasks like text summarization and\nquestion answering. These models have become popular in artificial intelligence\n(AI) assistants like ChatGPT and already play an influential role in how humans\naccess information. However, the behavior of LLMs varies depending on their\ndesign, training, and use.\n  In this paper, we prompt a diverse panel of popular LLMs to describe a large\nnumber of prominent personalities with political relevance, in all six official\nlanguages of the United Nations. By identifying and analyzing moral assessments\nreflected in their responses, we find normative differences between LLMs from\ndifferent geopolitical regions, as well as between the responses of the same\nLLM when prompted in different languages. Among only models in the United\nStates, we find that popularly hypothesized disparities in political views are\nreflected in significant normative differences related to progressive values.\nAmong Chinese models, we characterize a division between internationally- and\ndomestically-focused models.\n  Our results show that the ideological stance of an LLM appears to reflect the\nworldview of its creators. This poses the risk of political instrumentalization\nand raises concerns around technological and regulatory efforts with the stated\naim of making LLMs ideologically 'unbiased'.\n","authors":["Maarten Buyl","Alexander Rogiers","Sander Noels","Guillaume Bied","Iris Dominguez-Catena","Edith Heiter","Iman Johary","Alexandru-Cristian Mara","Raphaël Romero","Jefrey Lijffijt","Tijl De Bie"],"pdf_url":"https://arxiv.org/pdf/2410.18417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18435v1","updated":"2025-01-30T15:42:24Z","published":"2025-01-30T15:42:24Z","title":"GENIE: Generative Note Information Extraction model for structuring EHR\n  data","summary":"  Electronic Health Records (EHRs) hold immense potential for advancing\nhealthcare, offering rich, longitudinal data that combines structured\ninformation with valuable insights from unstructured clinical notes. However,\nthe unstructured nature of clinical text poses significant challenges for\nsecondary applications. Traditional methods for structuring EHR free-text data,\nsuch as rule-based systems and multi-stage pipelines, are often limited by\ntheir time-consuming configurations and inability to adapt across clinical\nnotes from diverse healthcare settings. Few systems provide a comprehensive\nattribute extraction for terminologies. While giant large language models\n(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,\ncostly, and impractical for large-scale use. To overcome these limitations, we\nintroduce GENIE, a Generative Note Information Extraction system that leverages\nLLMs to streamline the structuring of unstructured clinical text into usable\ndata with standardized format. GENIE processes entire paragraphs in a single\npass, extracting entities, assertion statuses, locations, modifiers, values,\nand purposes with high accuracy. Its unified, end-to-end approach simplifies\nworkflows, reduces errors, and eliminates the need for extensive manual\nintervention. Using a robust data preparation pipeline and fine-tuned small\nscale LLMs, GENIE achieves competitive performance across multiple information\nextraction tasks, outperforming traditional tools like cTAKES and MetaMap and\ncan handle extra attributes to be extracted. GENIE strongly enhances real-world\napplicability and scalability in healthcare systems. By open-sourcing the model\nand test data, we aim to encourage collaboration and drive further advancements\nin EHR structurization.\n","authors":["Huaiyuan Ying","Hongyi Yuan","Jinsen Lu","Zitian Qu","Yang Zhao","Zhengyun Zhao","Isaac Kohane","Tianxi Cai","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2501.18435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07066v3","updated":"2025-01-30T15:24:28Z","published":"2024-11-11T15:30:16Z","title":"Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training","summary":"  Network pruning focuses on computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has been pruning and re-training, which nowadays\nis inconvenient due to the vast amount of pre-trained models, which are in any\ncase too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAL}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs,\nwhich modifies the block-wise and row-wise sparsity exploiting information from\nboth the dense model and its sparse version to maximize the \\emph{neuron\nalignment} among activations. Differently from existing methods, our approach\nadaptively selects the best hyperparameters for the block-wise and row-wise\nsparsity ratios w.r.t. the model and the desired sparsity, and requires\n\\emph{no re-training}. We test our method over 276 cases combining four LLM\nfamilies, three sparsity ratios, and ten language tasks (three language\nmodeling and seven zero-shot datasets), showing how it consistently outperforms\nthe latest state-of-the-art methods in terms of performance-runtime trade-off.\nThe code is available at\n\\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.\n","authors":["Elia Cunegatti","Leonardo Lucio Custode","Giovanni Iacca"],"pdf_url":"https://arxiv.org/pdf/2411.07066v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2211.11337v4","updated":"2025-01-30T15:13:01Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist++: Controllable One-Shot Text-to-Image Generation via\n  Positive-Negative Adapter","summary":"  State-of-the-arts text-to-image generation models such as Imagen and Stable\nDiffusion Model have succeed remarkable progresses in synthesizing\nhigh-quality, feature-rich images with high resolution guided by human text\nprompts. Since certain characteristics of image content \\emph{e.g.}, very\nspecific object entities or styles, are very hard to be accurately described by\ntext, some example-based image generation approaches have been proposed,\n\\emph{i.e.} generating new concepts based on absorbing the salient features of\na few input references. Despite of acknowledged successes, these methods have\nstruggled on accurately capturing the reference examples' characteristics while\nkeeping diverse and high-quality image generation, particularly in the one-shot\nscenario (\\emph{i.e.} given only one reference). To tackle this problem, we\npropose a simple yet effective framework, namely DreamArtist, which adopts a\nnovel positive-negative prompt-tuning learning strategy on the pre-trained\ndiffusion model, and it has shown to well handle the trade-off between the\naccurate controllability and fidelity of image generation with only one\nreference example. Specifically, our proposed framework incorporates both\npositive and negative embeddings or adapters and optimizes them in a joint\nmanner. The positive part aggressively captures the salient characteristics of\nthe reference image to drive diversified generation and the negative part\nrectifies inadequacies from the positive part. We have conducted extensive\nexperiments and evaluated the proposed method from image similarity (fidelity)\nand diversity, generation controllability, and style cloning. And our\nDreamArtist has achieved a superior generation performance over existing\nmethods. Besides, our additional evaluation on extended tasks, including\nconcept compositions and prompt-guided image editing, demonstrates its\neffectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12851v2","updated":"2025-01-30T14:36:52Z","published":"2025-01-22T12:59:08Z","title":"ACEBench: Who Wins the Match Point in Tool Learning?","summary":"  Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types.\n","authors":["Chen Chen","Xinlong Hao","Weiwen Liu","Xu Huang","Xingshan Zeng","Shuai Yu","Dexun Li","Shuai Wang","Weinan Gan","Yuefeng Huang","Wulong Liu","Xinzhi Wang","Defu Lian","Baoqun Yin","Yasheng Wang","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2501.12851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18365v1","updated":"2025-01-30T14:15:09Z","published":"2025-01-30T14:15:09Z","title":"RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects","summary":"  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques.\n","authors":["Yiteng Tu","Weihang Su","Yujia Zhou","Yiqun Liu","Qingyao Ai"],"pdf_url":"https://arxiv.org/pdf/2501.18365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18362v1","updated":"2025-01-30T14:07:56Z","published":"2025-01-30T14:07:56Z","title":"MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding","summary":"  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to\nevaluate expert-level medical knowledge and advanced reasoning. MedXpertQA\nincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text for text evaluation and MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and rich clinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodal benchmarks with\nsimple QA pairs generated from image captions. MedXpertQA applies rigorous\nfiltering and augmentation to address the insufficient difficulty of existing\nbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We perform data synthesis to mitigate\ndata leakage risk and conduct multiple rounds of expert reviews to ensure\naccuracy and reliability. We evaluate 16 leading models on MedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop a reasoning-oriented subset to\nfacilitate the assessment of o1-like models.\n","authors":["Yuxin Zuo","Shang Qu","Yifei Li","Zhangren Chen","Xuekai Zhu","Ermo Hua","Kaiyan Zhang","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.18362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18356v1","updated":"2025-01-30T14:03:36Z","published":"2025-01-30T14:03:36Z","title":"State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence","summary":"  We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.\n","authors":["Thea Aviss"],"pdf_url":"https://arxiv.org/pdf/2501.18356v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.18324v1","updated":"2025-01-30T13:11:19Z","published":"2025-01-30T13:11:19Z","title":"A Video-grounded Dialogue Dataset and Metric for Event-driven Activities","summary":"  This paper presents VDAct, a dataset for a Video-grounded Dialogue on\nEvent-driven Activities, alongside VDEval, a session-based context evaluation\nmetric specially designed for the task. Unlike existing datasets, VDAct\nincludes longer and more complex video sequences that depict a variety of\nevent-driven activities that require advanced contextual understanding for\naccurate response generation. The dataset comprises 3,000 dialogues with over\n30,000 question-and-answer pairs, derived from 1,000 videos with diverse\nactivity scenarios. VDAct displays a notably challenging characteristic due to\nits broad spectrum of activity scenarios and wide range of question types.\nEmpirical studies on state-of-the-art vision foundation models highlight their\nlimitations in addressing certain question types on our dataset. Furthermore,\nVDEval, which integrates dialogue session history and video content summaries\nextracted from our supplementary Knowledge Graphs to evaluate individual\nresponses, demonstrates a significantly higher correlation with human\nassessments on the VDAct dataset than existing evaluation metrics that rely\nsolely on the context of single dialogue turns.\n","authors":["Wiradee Imrattanatrai","Masaki Asada","Kimihiro Hasegawa","Zhi-Qi Cheng","Ken Fukuda","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2501.18324v1.pdf","comment":"Accepted at AAAI2025"},{"id":"http://arxiv.org/abs/2410.01805v2","updated":"2025-01-30T13:07:37Z","published":"2024-10-02T17:59:52Z","title":"Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices","summary":"  Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.\n","authors":["Yuxiang Huang","Binhang Yuan","Xu Han","Chaojun Xiao","Zhiyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01805v2.pdf","comment":"Preprints"},{"id":"http://arxiv.org/abs/2501.18292v1","updated":"2025-01-30T12:08:00Z","published":"2025-01-30T12:08:00Z","title":"Citation Recommendation based on Argumentative Zoning of User Queries","summary":"  Citation recommendation aims to locate the important papers for scholars to\ncite. When writing the citing sentences, the authors usually hold different\nciting intents, which are referred to citation function in citation analysis.\nSince argumentative zoning is to identify the argumentative and rhetorical\nstructure in scientific literature, we want to use this information to improve\nthe citation recommendation task. In this paper, a multi-task learning model is\nbuilt for citation recommendation and argumentative zoning classification. We\nalso generated an annotated corpus of the data from PubMed Central based on a\nnew argumentative zoning schema. The experimental results show that, by\nconsidering the argumentative information in the citing sentence, citation\nrecommendation model will get better performance.\n","authors":["Shutian Ma","Chengzhi Zhang","Heng Zhang","Zheng Gao"],"pdf_url":"https://arxiv.org/pdf/2501.18292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13798v3","updated":"2025-01-30T12:03:48Z","published":"2024-05-22T16:23:40Z","title":"Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property\n  for Perplexity in Generative Language Models","summary":"  We prove a new asymptotic equipartition property for the perplexity of long\ntexts generated by a language model and present supporting experimental\nevidence from open-source models. Specifically we show that the logarithmic\nperplexity of any large text generated by a language model must asymptotically\nconverge to the average entropy of its token distributions. This defines a\n\"typical set\" that all long synthetic texts generated by a language model must\nbelong to. We show that this typical set is a vanishingly small subset of all\npossible grammatically correct outputs. These results suggest possible\napplications to important practical problems such as (a) detecting synthetic\nAI-generated text, and (b) testing whether a text was used to train a language\nmodel. We make no simplifying assumptions (such as stationarity) about the\nstatistics of language model outputs, and therefore our results are directly\napplicable to practical real-world models without any approximations.\n","authors":["Avinash Mudireddy","Tyler Bell","Raghu Mudumbai"],"pdf_url":"https://arxiv.org/pdf/2405.13798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18287v1","updated":"2025-01-30T11:55:44Z","published":"2025-01-30T11:55:44Z","title":"Mining for Species, Locations, Habitats, and Ecosystems from Scientific\n  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large\n  Language Models","summary":"  This paper presents an exploratory study that harnesses the capabilities of\nlarge language models (LLMs) to mine key ecological entities from invasion\nbiology literature. Specifically, we focus on extracting species names, their\nlocations, associated habitats, and ecosystems, information that is critical\nfor understanding species spread, predicting future invasions, and informing\nconservation efforts. Traditional text mining approaches often struggle with\nthe complexity of ecological terminology and the subtle linguistic patterns\nfound in these texts. By applying general-purpose LLMs without domain-specific\nfine-tuning, we uncover both the promise and limitations of using these models\nfor ecological entity extraction. In doing so, this study lays the groundwork\nfor more advanced, automated knowledge extraction tools that can aid\nresearchers and practitioners in understanding and managing biological\ninvasions.\n","authors":["Jennifer D'Souza","Zachary Laubach","Tarek Al Mustafa","Sina Zarrieß","Robert Frühstückl","Phyllis Illari"],"pdf_url":"https://arxiv.org/pdf/2501.18287v1.pdf","comment":"8 pages, 2 figures, accepted to the NLP4Ecology Workshop 2025\n  (https://nlp4ecology2025.di.unito.it/) co-located with the Joint 25th Nordic\n  Conference on Computational Linguistics and 11th Baltic Conference on Human\n  Language Technologies"},{"id":"http://arxiv.org/abs/2501.18280v1","updated":"2025-01-30T11:37:40Z","published":"2025-01-30T11:37:40Z","title":"Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models","summary":"  The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.\n","authors":["Haoyu Liang","Youran Sun","Yunfeng Cai","Jun Zhu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18265v1","updated":"2025-01-30T11:04:14Z","published":"2025-01-30T11:04:14Z","title":"Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence","summary":"  With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments.\n","authors":["Kevin Roitero","Dustin Wright","Michael Soprano","Isabelle Augenstein","Stefano Mizzaro"],"pdf_url":"https://arxiv.org/pdf/2501.18265v1.pdf","comment":"18 pages; 7 figures; 5 tables"},{"id":"http://arxiv.org/abs/2501.18251v1","updated":"2025-01-30T10:33:26Z","published":"2025-01-30T10:33:26Z","title":"How to Select Datapoints for Efficient Human Evaluation of NLG Models?","summary":"  Human evaluation is the gold-standard for evaluating text generation models.\nIt is also expensive, and to fit budgetary constraints, a random subset of the\ntest data is often chosen in practice. The randomly selected data may not\naccurately represent test performance, making this approach economically\ninefficient for model comparison. Thus, in this work, we develop a suite of\nselectors to get the most informative datapoints for human evaluation while\ntaking the evaluation costs into account. We show that selectors based on\nvariance in automated metric scores, diversity in model outputs, or Item\nResponse Theory outperform random selection. We further develop an approach to\ndistill these selectors to the scenario where the model outputs are not yet\navailable. In particular, we introduce source-based estimators, which predict\nitem usefulness for human evaluation just based on the source texts. We\ndemonstrate the efficacy of our selectors in two common NLG tasks, machine\ntranslation and summarization, and show that up to only ~50% of the test data\nis needed to produce the same evaluation result as the entire data. Our\nimplementations are published in the subset2evaluate package.\n","authors":["Vilém Zouhar","Peng Cui","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2501.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18243v1","updated":"2025-01-30T10:21:10Z","published":"2025-01-30T10:21:10Z","title":"Statistical multi-metric evaluation and visualization of LLM system\n  predictive performance","summary":"  The evaluation of generative or discriminative large language model\n(LLM)-based systems is often a complex multi-dimensional problem. Typically, a\nset of system configuration alternatives are evaluated on one or more benchmark\ndatasets, each with one or more evaluation metrics, which may differ between\ndatasets. We often want to evaluate -- with a statistical measure of\nsignificance -- whether systems perform differently either on a given dataset\naccording to a single metric, on aggregate across metrics on a dataset, or\nacross datasets. Such evaluations can be done to support decision-making, such\nas deciding whether a particular system component change (e.g., choice of LLM\nor hyperparameter values) significantly improves performance over the current\nsystem configuration, or, more generally, whether a fixed set of system\nconfigurations (e.g., a leaderboard list) have significantly different\nperformances according to metrics of interest. We present a framework\nimplementation that automatically performs the correct statistical tests,\nproperly aggregates the statistical results across metrics and datasets (a\nnontrivial task), and can visualize the results. The framework is demonstrated\non the multi-lingual code generation benchmark CrossCodeEval, for several\nstate-of-the-art LLMs.\n","authors":["Samuel Ackerman","Eitan Farchi","Orna Raz","Assaf Toledo"],"pdf_url":"https://arxiv.org/pdf/2501.18243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14528v2","updated":"2025-01-30T10:15:35Z","published":"2025-01-24T14:31:30Z","title":"Idiom Detection in Sorani Kurdish Texts","summary":"  Idiom detection using Natural Language Processing (NLP) is the computerized\nprocess of recognizing figurative expressions within a text that convey\nmeanings beyond the literal interpretation of the words. While idiom detection\nhas seen significant progress across various languages, the Kurdish language\nfaces a considerable research gap in this area despite the importance of idioms\nin tasks like machine translation and sentiment analysis. This study addresses\nidiom detection in Sorani Kurdish by approaching it as a text classification\ntask using deep learning techniques. To tackle this, we developed a dataset\ncontaining 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse\ncontexts. Using this dataset, we developed and evaluated three deep learning\nmodels: KuBERT-based transformer sequence classification, a Recurrent\nConvolutional Neural Network (RCNN), and a BiLSTM model with an attention\nmechanism. The evaluations revealed that the transformer model, the fine-tuned\nBERT, consistently outperformed the others, achieving nearly 99% accuracy while\nthe RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the\neffectiveness of Transformer-based architectures in low-resource languages like\nKurdish. This research provides a dataset, three optimized models, and insights\ninto idiom detection, laying a foundation for advancing Kurdish NLP.\n","authors":["Skala Kamaran Omer","Hossein Hassani"],"pdf_url":"https://arxiv.org/pdf/2501.14528v2.pdf","comment":"22 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2501.18205v1","updated":"2025-01-30T08:51:48Z","published":"2025-01-30T08:51:48Z","title":"Contextually Structured Token Dependency Encoding for Large Language\n  Models","summary":"  Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation.\n","authors":["James Blades","Frederick Somerfield","William Langley","Susan Everingham","Maurice Witherington"],"pdf_url":"https://arxiv.org/pdf/2501.18205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11149v7","updated":"2025-01-30T08:45:30Z","published":"2024-09-17T13:03:12Z","title":"SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with\n  Customisable Fairness Calibration","summary":"  The development of unbiased large language models is widely recognized as\ncrucial, yet existing benchmarks fall short in detecting biases due to limited\nscope, contamination, and lack of a fairness baseline. SAGED(bias) is the first\nholistic benchmarking pipeline to address these problems. The pipeline\nencompasses five core stages: scraping materials, assembling benchmarks,\ngenerating responses, extracting numeric features, and diagnosing with\ndisparity metrics. SAGED includes metrics for max disparity, such as impact\nratio, and bias concentration, such as Max Z-scores. Noticing that metric tool\nbias and contextual bias in prompts can distort evaluation, SAGED implements\ncounterfactual branching and baseline calibration for mitigation. For\ndemonstration, we use SAGED on G20 Countries with popular 8b-level models\nincluding Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we\nfind that while Mistral and Qwen2 show lower max disparity and higher bias\nconcentration than Gemma2 and Llama3.1, all models are notably biased against\ncountries like Russia and (except for Qwen2) China. With further experiments to\nhave models role-playing U.S. presidents, we see bias amplifies and shifts in\nheterogeneous directions. Moreover, we see Qwen2 and Mistral not engage in\nrole-playing, while Llama3.1 and Gemma2 role-play Trump notably more\nintensively than Biden and Harris, indicating role-playing performance bias in\nthese models.\n","authors":["Xin Guan","Ze Wang","Nathaniel Demchak","Saloni Gupta","Ediz Ertekin Jr.","Adriano Koshiyama","Emre Kazim","Zekun Wu"],"pdf_url":"https://arxiv.org/pdf/2409.11149v7.pdf","comment":"COLING 2025 Main Conference Oral Presentation"},{"id":"http://arxiv.org/abs/2410.08436v2","updated":"2025-01-30T08:06:33Z","published":"2024-10-11T00:45:50Z","title":"Exploring the Role of Reasoning Structures for Constructing Proofs in\n  Multi-Step Natural Language Reasoning with Large Language Models","summary":"  When performing complex multi-step reasoning tasks, the ability of Large\nLanguage Models (LLMs) to derive structured intermediate proof steps is\nimportant for ensuring that the models truly perform the desired reasoning and\nfor improving models' explainability. This paper is centred around a focused\nstudy: whether the current state-of-the-art generalist LLMs can leverage the\nstructures in a few examples to better construct the proof structures with\n\\textit{in-context learning}. Our study specifically focuses on structure-aware\ndemonstration and structure-aware pruning. We demonstrate that they both help\nimprove performance. A detailed analysis is provided to help understand the\nresults.\n","authors":["Zi'ou Zheng","Christopher Malon","Martin Renqiang Min","Xiaodan Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.08436v2.pdf","comment":"Accepted by EMNLP2024 main conference"},{"id":"http://arxiv.org/abs/2501.16513v2","updated":"2025-01-30T08:00:14Z","published":"2025-01-27T21:26:37Z","title":"Deception in LLMs: Self-Preservation and Autonomous Goals in Large\n  Language Models","summary":"  Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation.\n","authors":["Sudarshan Kamath Barkur","Sigurd Schacht","Johannes Scholl"],"pdf_url":"https://arxiv.org/pdf/2501.16513v2.pdf","comment":"Corrected Version - Solved Some Issues with reference compilation by\n  latex"},{"id":"http://arxiv.org/abs/2501.12746v3","updated":"2025-01-30T07:11:06Z","published":"2025-01-22T09:27:11Z","title":"EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small\n  Language Models for Biomedical Question Answering","summary":"  When addressing professional questions in the biomedical domain, humans\ntypically acquire multiple pieces of information as evidence and engage in\nmultifaceted evidence analysis to provide high-quality answers. Current\nLLM-based answer generation methods lack a detailed definition and learning\nprocess for evidence analysis, leading to the risk of error propagation and\nhallucinations while using evidence. Although increasing the parameter size of\nLLMs can alleviate these issues, it also presents challenges in model training\nand deployment with limited resources. In this study, we propose EvidenceMap,\nwhich aims to enable a tiny pre-trained language model to explicitly learn\nmultiple aspects of biomedical evidence, including supportive evaluation,\nlogical correlation and content summarization, thereby latently guiding a small\ngenerative model (around 3B parameters) to provide textual responses.\nExperimental results demonstrate that our method, fine-tuning a language model\nwith 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in\nreference-based quality and accuracy, respectively.\n","authors":["Chang Zong","Jian Wan","Siliang Tang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.12746v3.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.15188v3","updated":"2025-01-30T07:08:45Z","published":"2024-12-19T18:56:24Z","title":"LMFusion: Adapting Pretrained Language Models for Multimodal Generation","summary":"  We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v3.pdf","comment":"Name change: LlamaFusion to LMFusion"},{"id":"http://arxiv.org/abs/2501.15797v2","updated":"2025-01-30T06:10:23Z","published":"2025-01-27T05:46:06Z","title":"LemmaHead: RAG Assisted Proof Generation Using Large Language Models","summary":"  Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.\n","authors":["Tianbo Yang","Mingqi Yang","Hongyi Zhao","Tianshuo Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18154v1","updated":"2025-01-30T05:39:01Z","published":"2025-01-30T05:39:01Z","title":"Mixed-Precision Graph Neural Quantization for Low Bit Large Language\n  Models","summary":"  Post-Training Quantization (PTQ) is pivotal for deploying large language\nmodels (LLMs) within resource-limited settings by significantly reducing\nresource demands. However, existing PTQ strategies underperform at low bit\nlevels < 3 bits due to the significant difference between the quantized and\noriginal weights. To enhance the quantization performance at low bit widths, we\nintroduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a\ngraph neural network (GNN) module to capture dependencies among weights and\nadaptively assign quantization bit-widths. Through the information propagation\nof the GNN module, our method more effectively captures dependencies among\ntarget weights, leading to a more accurate assessment of weight importance and\noptimized allocation of quantization strategies. Extensive experiments on the\nWikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms\nprevious state-of-the-art PTQ method GPTQ, setting new benchmarks for\nquantization performance under low-bit conditions.\n","authors":["Wanlong Liu","Yichen Xiao","Dingyi Zeng","Hongyang Zhao","Wenyu Chen","Malu Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18154v1.pdf","comment":"ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.17762v2","updated":"2025-01-30T05:26:05Z","published":"2025-01-29T16:53:16Z","title":"Improving Privacy Benefits of Redaction","summary":"  We propose a novel redaction methodology that can be used to sanitize natural\ntext data. Our new technique provides better privacy benefits than other state\nof the art techniques while maintaining lower redaction levels.\n","authors":["Vaibhav Gusain","Douglas Leith"],"pdf_url":"https://arxiv.org/pdf/2501.17762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13101v2","updated":"2025-01-30T04:51:41Z","published":"2024-07-18T02:19:00Z","title":"Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with\n  an Iterative Approach","summary":"  Multi-hop question answering is a challenging task with distinct industrial\nrelevance, and Retrieval-Augmented Generation (RAG) methods based on large\nlanguage models (LLMs) have become a popular approach to tackle this task.\nOwing to the potential inability to retrieve all necessary information in a\nsingle iteration, a series of iterative RAG methods has been recently\ndeveloped, showing significant performance improvements. However, existing\nmethods still face two critical challenges: context overload resulting from\nmultiple rounds of retrieval, and over-planning and repetitive planning due to\nthe lack of a recorded retrieval trajectory. In this paper, we propose a novel\niterative RAG method called ReSP, equipped with a dual-function summarizer.\nThis summarizer compresses information from retrieved documents, targeting both\nthe overarching question and the current sub-question concurrently.\nExperimental results on the multi-hop question-answering datasets HotpotQA and\n2WikiMultihopQA demonstrate that our method significantly outperforms the\nstate-of-the-art, and exhibits excellent robustness concerning context length.\n","authors":["Zhouyu Jiang","Mengshu Sun","Lei Liang","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.13101v2.pdf","comment":"Accepted by WWW2025 Agent4IR Workshop"},{"id":"http://arxiv.org/abs/2501.12619v2","updated":"2025-01-30T04:25:00Z","published":"2025-01-22T03:57:52Z","title":"Distillation Quantification for Large Language Models","summary":"  Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification.\n","authors":["Sunbowen Lee","Junting Zhou","Chang Ao","Kaige Li","Xinrun Du","Sirui He","Jiaheng Liu","Min Yang","Zhoufutu Wen","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2501.12619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18128v1","updated":"2025-01-30T04:20:16Z","published":"2025-01-30T04:20:16Z","title":"Unraveling the Capabilities of Language Models in News Summarization","summary":"  Given the recent introduction of multiple language models and the ongoing\ndemand for improved Natural Language Processing tasks, particularly\nsummarization, this work provides a comprehensive benchmarking of 20 recent\nlanguage models, focusing on smaller ones for the news summarization task. In\nthis work, we systematically test the capabilities and effectiveness of these\nmodels in summarizing news article texts which are written in different styles\nand presented in three distinct datasets. Specifically, we focus in this study\non zero-shot and few-shot learning settings and we apply a robust evaluation\nmethodology that combines different evaluation concepts including automatic\nmetrics, human evaluation, and LLM-as-a-judge. Interestingly, including\ndemonstration examples in the few-shot learning setting did not enhance models'\nperformance and, in some cases, even led to worse quality of the generated\nsummaries. This issue arises mainly due to the poor quality of the gold\nsummaries that have been used as reference summaries, which negatively impacts\nthe models' performance. Furthermore, our study's results highlight the\nexceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate\ndue to their advanced capabilities. However, among the public models evaluated,\ncertain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B\nand Zephyr-7B-Beta demonstrated promising results. These models showed\nsignificant potential, positioning them as competitive alternatives to large\nmodels for the task of news summarization.\n","authors":["Abdurrahman Odabaşı","Göksel Biricik"],"pdf_url":"https://arxiv.org/pdf/2501.18128v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.18805v1","updated":"2025-01-30T23:48:02Z","published":"2025-01-30T23:48:02Z","title":"Are Representation Disentanglement and Interpretability Linked in\n  Recommendation Models? A Critical Review and Reproducibility Study","summary":"  Unsupervised learning of disentangled representations has been closely tied\nto enhancing the representation intepretability of Recommender Systems (RSs).\nThis has been achieved by making the representation of individual features more\ndistinctly separated, so that it is easier to attribute the contribution of\nfeatures to the model's predictions. However, such advantages in\ninterpretability and feature attribution have mainly been explored\nqualitatively. Moreover, the effect of disentanglement on the model's\nrecommendation performance has been largely overlooked. In this work, we\nreproduce the recommendation performance, representation disentanglement and\nrepresentation interpretability of five well-known recommendation models on\nfour RS datasets. We quantify disentanglement and investigate the link of\ndisentanglement with recommendation effectiveness and representation\ninterpretability. While several existing work in RSs have proposed disentangled\nrepresentations as a gateway to improved effectiveness and interpretability,\nour findings show that disentanglement is not necessarily related to\neffectiveness but is closely related to representation interpretability. Our\ncode and results are publicly available at\nhttps://github.com/edervishaj/disentanglement-interpretability-recsys.\n","authors":["Ervin Dervishaj","Tuukka Ruotsalo","Maria Maistro","Christina Lioma"],"pdf_url":"https://arxiv.org/pdf/2501.18805v1.pdf","comment":"Accepted at the 47th European Conference on Information Retrieval\n  (ECIR 2025)"},{"id":"http://arxiv.org/abs/2501.18750v1","updated":"2025-01-30T21:00:47Z","published":"2025-01-30T21:00:47Z","title":"Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity\n  Recognition in Low-Resource Languages","summary":"  Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer\nbetween languages to identify and classify named entities, making it\nparticularly useful for low-resource languages. We show that the data-based\ncross-lingual transfer method is an effective technique for crosslingual NER\nand can outperform multilingual language models for low-resource languages.\nThis paper introduces two key enhancements to the annotation projection step in\ncross-lingual NER for low-resource languages. First, we explore refining word\nalignments using back-translation to improve accuracy. Second, we present a\nnovel formalized projection approach of matching source entities with extracted\ntarget candidates. Through extensive experiments on two datasets spanning 57\nlanguages, we demonstrated that our approach surpasses existing projectionbased\nmethods in low-resource settings. These findings highlight the robustness of\nprojection-based data transfer as an alternative to model-based methods for\ncrosslingual named entity recognition in lowresource languages.\n","authors":["Andrei Politov","Oleh Shkalikov","René Jäkel","Michael Färber"],"pdf_url":"https://arxiv.org/pdf/2501.18750v1.pdf","comment":"Accepted at NoDaLiDa/Baltic-HLT 2025"},{"id":"http://arxiv.org/abs/2501.18707v1","updated":"2025-01-30T19:07:35Z","published":"2025-01-30T19:07:35Z","title":"Hierarchical Multi-field Representations for Two-Stage E-commerce\n  Retrieval","summary":"  Dense retrieval methods typically target unstructured text data represented\nas flat strings. However, e-commerce catalogs often include structured\ninformation across multiple fields, such as brand, title, and description,\nwhich contain important information potential for retrieval systems. We present\nCascading Hierarchical Attention Retrieval Model (CHARM), a novel framework\ndesigned to encode structured product data into hierarchical field-level\nrepresentations with progressively finer detail. Utilizing a novel\nblock-triangular attention mechanism, our method captures the interdependencies\nbetween product fields in a specified hierarchy, yielding field-level\nrepresentations and aggregated vectors suitable for fast and efficient\nretrieval. Combining both representations enables a two-stage retrieval\npipeline, in which the aggregated vectors support initial candidate selection,\nwhile more expressive field-level representations facilitate precise\nfine-tuning for downstream ranking. Experiments on publicly available\nlarge-scale e-commerce datasets demonstrate that CHARM matches or outperforms\nstate-of-the-art baselines. Our analysis highlights the framework's ability to\nalign different queries with appropriate product fields, enhancing retrieval\naccuracy and explainability.\n","authors":["Niklas Freymuth","Dong Liu","Thomas Ricatte","Saab Mansour"],"pdf_url":"https://arxiv.org/pdf/2501.18707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18539v1","updated":"2025-01-30T18:07:19Z","published":"2025-01-30T18:07:19Z","title":"Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented\n  LLM-based Retrieval Method","summary":"  Real-world open-domain questions can be complicated, particularly when\nanswering them involves information from multiple information sources. LLMs\nhave demonstrated impressive performance in decomposing complex tasks into\nsimpler steps, and previous work has used it for better retrieval in support of\ncomplex questions. However, LLM's decomposition of questions is unaware of what\ndata is available and how data is organized, often leading to a sub-optimal\nretrieval performance. Recent effort in agentic RAG proposes to perform\nretrieval in an iterative fashion, where a followup query is derived as an\naction based on previous rounds of retrieval. While this provides one way of\ninteracting with the data collection, agentic RAG's exploration of data is\ninefficient because successive queries depend on previous results rather than\nbeing guided by the organization of available data in the collection. To\naddress this problem, we propose an LLM-based retrieval method -- ARM, that\naims to better align the question with the organization of the data collection\nby exploring relationships among data objects beyond matching the utterance of\nthe query, thus leading to a retrieve-all-at-once solution for complex queries.\nWe evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms\nstandard RAG with query decomposition by up to 5.2 pt in execution accuracy and\nagentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and\n19.3 pt higher F1 match scores compared to these approaches.\n","authors":["Peter Baile Chen","Yi Zhang","Michael Cafarella","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2501.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18536v1","updated":"2025-01-30T18:02:15Z","published":"2025-01-30T18:02:15Z","title":"Illusions of Relevance: Using Content Injection Attacks to Deceive\n  Retrievers, Rerankers, and LLM Judges","summary":"  Consider a scenario in which a user searches for information, only to\nencounter texts flooded with misleading or non-relevant content. This scenario\nexemplifies a simple yet potent vulnerability in neural Information Retrieval\n(IR) pipelines: content injection attacks. We find that embedding models for\nretrieval, rerankers, and large language model (LLM) relevance judges are\nvulnerable to these attacks, in which adversaries insert misleading text into\npassages to manipulate model judgements. We identify two primary threats: (1)\ninserting unrelated or harmful content within passages that still appear\ndeceptively \"relevant\", and (2) inserting entire queries or key query terms\ninto passages to boost their perceived relevance. While the second tactic has\nbeen explored in prior research, we present, to our knowledge, the first\nempirical analysis of the first threat, demonstrating how state-of-the-art\nmodels can be easily misled. Our study systematically examines the factors that\ninfluence an attack's success, such as the placement of injected content and\nthe balance between relevant and non-relevant material. Additionally, we\nexplore various defense strategies, including adversarial passage classifiers,\nretriever fine-tuning to discount manipulated content, and prompting LLM judges\nto adopt a more cautious approach. However, we find that these countermeasures\noften involve trade-offs, sacrificing effectiveness for attack robustness and\nsometimes penalizing legitimate documents in the process. Our findings\nhighlight the need for stronger defenses against these evolving adversarial\nstrategies to maintain the trustworthiness of IR systems. We release our code\nand scripts to facilitate further research.\n","authors":["Manveer Singh Tamber","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2501.18536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18365v1","updated":"2025-01-30T14:15:09Z","published":"2025-01-30T14:15:09Z","title":"RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against\n  Retrieval Defects","summary":"  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge retrieved from a knowledge base. However, its\neffectiveness is fundamentally constrained by the reliability of both the\nretriever and the knowledge base. In real-world scenarios, imperfections in\nthese components often lead to the retrieval of noisy, irrelevant, or\nmisleading counterfactual information, ultimately undermining the\ntrustworthiness of RAG systems. To address this challenge, we propose Robust\nFine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against\nretrieval defects through two targeted fine-tuning tasks. Experimental results\ndemonstrate that RbFT significantly improves the robustness of RAG systems\nacross diverse retrieval conditions, surpassing existing methods while\nmaintaining high inference efficiency and compatibility with other robustness\ntechniques.\n","authors":["Yiteng Tu","Weihang Su","Yujia Zhou","Yiqun Liu","Qingyao Ai"],"pdf_url":"https://arxiv.org/pdf/2501.18365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18292v1","updated":"2025-01-30T12:08:00Z","published":"2025-01-30T12:08:00Z","title":"Citation Recommendation based on Argumentative Zoning of User Queries","summary":"  Citation recommendation aims to locate the important papers for scholars to\ncite. When writing the citing sentences, the authors usually hold different\nciting intents, which are referred to citation function in citation analysis.\nSince argumentative zoning is to identify the argumentative and rhetorical\nstructure in scientific literature, we want to use this information to improve\nthe citation recommendation task. In this paper, a multi-task learning model is\nbuilt for citation recommendation and argumentative zoning classification. We\nalso generated an annotated corpus of the data from PubMed Central based on a\nnew argumentative zoning schema. The experimental results show that, by\nconsidering the argumentative information in the citing sentence, citation\nrecommendation model will get better performance.\n","authors":["Shutian Ma","Chengzhi Zhang","Heng Zhang","Zheng Gao"],"pdf_url":"https://arxiv.org/pdf/2501.18292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18265v1","updated":"2025-01-30T11:04:14Z","published":"2025-01-30T11:04:14Z","title":"Collecting Cost-Effective, High-Quality Truthfulness Assessments with\n  LLM Summarized Evidence","summary":"  With the degradation of guardrails against mis- and disinformation online, it\nis more critical than ever to be able to effectively combat it. In this paper,\nwe explore the efficiency and effectiveness of using crowd-sourced truthfulness\nassessments based on condensed, large language model (LLM) generated summaries\nof online sources. We compare the use of generated summaries to the use of\noriginal web pages in an A/B testing setting, where we employ a large and\ndiverse pool of crowd-workers to perform the truthfulness assessment. We\nevaluate the quality of assessments, the efficiency with which assessments are\nperformed, and the behavior and engagement of participants. Our results\ndemonstrate that the Summary modality, which relies on summarized evidence,\noffers no significant change in assessment accuracy over the Standard modality,\nwhile significantly increasing the speed with which assessments are performed.\nWorkers using summarized evidence produce a significantly higher number of\nassessments in the same time frame, reducing the cost needed to acquire\ntruthfulness assessments. Additionally, the Summary modality maximizes both the\ninter-annotator agreements as well as the reliance on and perceived usefulness\nof evidence, demonstrating the utility of summarized evidence without\nsacrificing the quality of assessments.\n","authors":["Kevin Roitero","Dustin Wright","Michael Soprano","Isabelle Augenstein","Stefano Mizzaro"],"pdf_url":"https://arxiv.org/pdf/2501.18265v1.pdf","comment":"18 pages; 7 figures; 5 tables"},{"id":"http://arxiv.org/abs/2501.18216v1","updated":"2025-01-30T09:17:04Z","published":"2025-01-30T09:17:04Z","title":"Behavior Modeling Space Reconstruction for E-Commerce Search","summary":"  Delivering superior search services is crucial for enhancing customer\nexperience and driving revenue growth. Conventionally, search systems model\nuser behaviors by combining user preference and query item relevance\nstatically, often through a fixed logical 'and' relationship. This paper\nreexamines existing approaches through a unified lens using both causal graphs\nand Venn diagrams, uncovering two prevalent yet significant issues: entangled\npreference and relevance effects, and a collapsed modeling space. To surmount\nthese challenges, our research introduces a novel framework, DRP, which\nenhances search accuracy through two components to reconstruct the behavior\nmodeling space. Specifically, we implement preference editing to proactively\nremove the relevance effect from preference predictions, yielding untainted\nuser preferences. Additionally, we employ adaptive fusion, which dynamically\nadjusts fusion criteria to align with the varying patterns of relevance and\npreference, facilitating more nuanced and tailored behavior predictions within\nthe reconstructed modeling space. Empirical validation on two public datasets\nand a proprietary search dataset underscores the superiority of our proposed\nmethodology, demonstrating marked improvements in performance over existing\napproaches.\n","authors":["Yejing Wang","Chi Zhang","Xiangyu Zhao","Qidong Liu","Maolin Wang","Xuewei Tao","Zitao Liu","Xing Shi","Xudong Yang","Ling Zhong","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2501.18216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18210v1","updated":"2025-01-30T08:55:32Z","published":"2025-01-30T08:55:32Z","title":"Hashtag Re-Appropriation for Audience Control on Recommendation-Driven\n  Social Media Xiaohongshu (rednote)","summary":"  Algorithms have played a central role in personalized recommendations on\nsocial media. However, they also present significant obstacles for content\ncreators trying to predict and manage their audience reach. This issue is\nparticularly challenging for marginalized groups seeking to maintain safe\nspaces. Our study explores how women on Xiaohongshu (rednote), a\nrecommendation-driven social platform, proactively re-appropriate hashtags\n(e.g., #Baby Supplemental Food) by using them in posts unrelated to their\nliteral meaning. The hashtags were strategically chosen from topics that would\nbe uninteresting to the male audience they wanted to block. Through a\nmixed-methods approach, we analyzed the practice of hashtag re-appropriation\nbased on 5,800 collected posts and interviewed 24 active users from diverse\nbackgrounds to uncover users' motivations and reactions towards the\nre-appropriation. This practice highlights how users can reclaim agency over\ncontent distribution on recommendation-driven platforms, offering insights into\nself-governance within algorithmic-centered power structures.\n","authors":["Ruyuan Wan","Lingbo Tong","Tiffany Knearem","Toby Jia-Jun Li","Ting-Hao 'Kenneth' Huang","Qunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2501.18210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18177v1","updated":"2025-01-30T07:14:50Z","published":"2025-01-30T07:14:50Z","title":"Investigating Tax Evasion Emergence Using Dual Large Language Model and\n  Deep Reinforcement Learning Powered Agent-based Simulation","summary":"  Tax evasion, usually the largest component of an informal economy, is a\npersistent challenge over history with significant socio-economic implications.\nMany socio-economic studies investigate its dynamics, including influencing\nfactors, the role and influence of taxation policies, and the prediction of the\ntax evasion volume over time. These studies assumed such behavior is given, as\nobserved in the real world, neglecting the \"big bang\" of such activity in a\npopulation. To this end, computational economy studies adopted developments in\ncomputer simulations, in general, and recent innovations in artificial\nintelligence (AI), in particular, to simulate and study informal economy\nappearance in various socio-economic settings. This study presents a novel\ncomputational framework to examine the dynamics of tax evasion and the\nemergence of informal economic activity. Employing an agent-based simulation\npowered by Large Language Models and Deep Reinforcement Learning, the framework\nis uniquely designed to allow informal economic behaviors to emerge\norganically, without presupposing their existence or explicitly signaling\nagents about the possibility of evasion. This provides a rigorous approach for\nexploring the socio-economic determinants of compliance behavior. The\nexperimental design, comprising model validation and exploratory phases,\ndemonstrates the framework's robustness in replicating theoretical economic\nbehaviors. Findings indicate that individual personality traits, external\nnarratives, enforcement probabilities, and the perceived efficiency of public\ngoods provision significantly influence both the timing and extent of informal\neconomic activity. The results underscore that efficient public goods provision\nand robust enforcement mechanisms are complementary; neither alone is\nsufficient to curtail informal activity effectively.\n","authors":["Teddy Lazebnik","Labib Shami"],"pdf_url":"https://arxiv.org/pdf/2501.18177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15797v2","updated":"2025-01-30T06:10:23Z","published":"2025-01-27T05:46:06Z","title":"LemmaHead: RAG Assisted Proof Generation Using Large Language Models","summary":"  Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.\n","authors":["Tianbo Yang","Mingqi Yang","Hongyi Zhao","Tianshuo Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18126v1","updated":"2025-01-30T04:04:39Z","published":"2025-01-30T04:04:39Z","title":"HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation\n  with Hourly Feedback","summary":"  Modern recommendation systems can be broadly divided into two key stages: the\nranking stage, where the system predicts various user engagements (e.g.,\nclick-through rate, like rate, follow rate, watch time), and the value model\nstage, which aggregates these predictive scores through a function (e.g., a\nlinear combination defined by a weight vector) to measure the value of each\ncontent by a single numerical score. Both stages play roughly equally important\nroles in real industrial systems; however, how to optimize the model weights\nfor the second stage still lacks systematic study. This paper focuses on\noptimizing the second stage through auto-tuning technology. Although general\nauto-tuning systems and solutions - both from established production practices\nand open-source solutions - can address this problem, they typically require\nweeks or even months to identify a feasible solution. Such prolonged tuning\nprocesses are unacceptable in production environments for recommendation\nsystems, as suboptimal value models can severely degrade user experience. An\neffective auto-tuning solution is required to identify a viable model within\n2-3 days, rather than the extended timelines typically associated with existing\napproaches. In this paper, we introduce a practical auto-tuning system named\nHyperZero that addresses these time constraints while effectively solving the\nunique challenges inherent in modern recommendation systems. Moreover, this\nframework has the potential to be expanded to broader tuning tasks within\nrecommendation systems.\n","authors":["Xufeng Cai","Ziwei Guan","Lei Yuan","Ali Selman Aydin","Tengyu Xu","Boying Liu","Wenbo Ren","Renkai Xiang","Songyi He","Haichuan Yang","Serena Li","Mingze Gao","Yue Weng","Ji Liu"],"pdf_url":"https://arxiv.org/pdf/2501.18126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18117v1","updated":"2025-01-30T03:37:28Z","published":"2025-01-30T03:37:28Z","title":"Improving Minimax Group Fairness in Sequential Recommendation","summary":"  Training sequential recommenders such as SASRec with uniform sample weights\nachieves good overall performance but can fall short on specific user groups.\nOne such example is popularity bias, where mainstream users receive better\nrecommendations than niche content viewers. To improve recommendation quality\nacross diverse user groups, we explore three Distributionally Robust\nOptimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at\nRisk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and\nstruggle with users belonging to multiple groups, CVaR does not require such\nannotations and can naturally handle overlapping groups. In experiments on two\nreal-world datasets, we show that the DRO methods outperform standard training,\nwith CVaR delivering the best results. Additionally, we find that Group and\nStreaming DRO are sensitive to the choice of group used for loss computation.\nOur contributions include (i) a novel application of CVaR to recommenders, (ii)\nshowing that the DRO methods improve group metrics as well as overall\nperformance, and (iii) demonstrating CVaR's effectiveness in the practical\nscenario of intersecting user groups.\n","authors":["Krishna Acharya","David Wardrope","Timos Korres","Aleksandr Petrov","Anders Uhrenholt"],"pdf_url":"https://arxiv.org/pdf/2501.18117v1.pdf","comment":"This paper has been accepted to the IR for Good track at ECIR 2025"},{"id":"http://arxiv.org/abs/2501.17799v2","updated":"2025-01-30T02:23:25Z","published":"2025-01-29T17:38:39Z","title":"Leveraging Multimodal LLM for Inspirational User Interface Search","summary":"  Inspirational search, the process of exploring designs to inform and inspire\nnew creative work, is pivotal in mobile user interface (UI) design. However,\nexploring the vast space of UI references remains a challenge. Existing\nAI-based UI search methods often miss crucial semantics like target users or\nthe mood of apps. Additionally, these models typically require metadata like\nview hierarchies, limiting their practical use. We used a multimodal large\nlanguage model (MLLM) to extract and interpret semantics from mobile UI images.\nWe identified key UI semantics through a formative study and developed a\nsemantic-based UI search system. Through computational and human evaluations,\nwe demonstrate that our approach significantly outperforms existing UI\nretrieval methods, offering UI designers a more enriched and contextually\nrelevant search experience. We enhance the understanding of mobile UI design\nsemantics and highlight MLLMs' potential in inspirational search, providing a\nrich dataset of UI semantics for future studies.\n","authors":["Seokhyeon Park","Yumin Song","Soohyun Lee","Jaeyoung Kim","Jinwook Seo"],"pdf_url":"https://arxiv.org/pdf/2501.17799v2.pdf","comment":"In Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems (CHI '25)"},{"id":"http://arxiv.org/abs/2501.14269v2","updated":"2025-01-30T02:05:07Z","published":"2025-01-24T06:26:50Z","title":"Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential\n  Recommendation","summary":"  Multi-modal sequential recommendation (SR) leverages multi-modal data to\nlearn more comprehensive item features and user preferences than traditional SR\nmethods, which has become a critical topic in both academia and industry.\nExisting methods typically focus on enhancing multi-modal information utility\nthrough adaptive modality fusion to capture the evolving of user preference\nfrom user-item interaction sequences. However, most of them overlook the\ninterference caused by redundant interest-irrelevant information contained in\nrich multi-modal data. Additionally, they primarily rely on implicit temporal\ninformation based solely on chronological ordering, neglecting explicit\ntemporal signals that could more effectively represent dynamic user interest\nover time. To address these limitations, we propose a Hierarchical time-aware\nMixture of experts for multi-modal Sequential Recommendation (HM4SR) with a\ntwo-level Mixture of Experts (MoE) and a multi-task learning strategy.\nSpecifically, the first MoE, named Interactive MoE, extracts essential user\ninterest-related information from the multi-modal data of each item. Then, the\nsecond MoE, termed Temporal MoE, captures user dynamic interests by introducing\nexplicit temporal embeddings from timestamps in modality encoding. To further\naddress data sparsity, we propose three auxiliary supervision tasks:\nsequence-level category prediction (CP) for item feature understanding,\ncontrastive learning on ID (IDCL) to align sequence context with user\ninterests, and placeholder contrastive learning (PCL) to integrate temporal\ninformation with modalities for dynamic interest modeling. Extensive\nexperiments on four public datasets verify the effectiveness of HM4SR compared\nto several state-of-the-art approaches.\n","authors":["Shengzhe Zhang","Liyi Chen","Dazhong Shen","Chao Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2501.14269v2.pdf","comment":"Accepted to WWW 2025"},{"id":"http://arxiv.org/abs/2404.08137v3","updated":"2025-01-30T00:52:34Z","published":"2024-04-11T21:48:54Z","title":"Generative Information Retrieval Evaluation","summary":"  In this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of \"slow search\", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change.\n","authors":["Marwah Alaofi","Negar Arabzadeh","Charles L. A. Clarke","Mark Sanderson"],"pdf_url":"https://arxiv.org/pdf/2404.08137v3.pdf","comment":"This chapter is part of the book Information Access in the Era of\n  Generative AI, co-edited by Chirag Shah and Ryen White"},{"id":"http://arxiv.org/abs/2501.13579v2","updated":"2025-01-30T00:29:20Z","published":"2025-01-23T11:38:00Z","title":"MixRec: Individual and Collective Mixing Empowers Data Augmentation for\n  Recommender Systems","summary":"  The core of the general recommender systems lies in learning high-quality\nembedding representations of users and items to investigate their positional\nrelations in the feature space. Unfortunately, data sparsity caused by\ndifficult-to-access interaction data severely limits the effectiveness of\nrecommender systems. Faced with such a dilemma, various types of\nself-supervised learning methods have been introduced into recommender systems\nin an attempt to alleviate the data sparsity through distribution modeling or\ndata augmentation. However, most data augmentation relies on elaborate manual\ndesign, which is not only not universal, but the bloated and redundant\naugmentation process may significantly slow down model training progress. To\ntackle these limitations, we propose a novel Dual Mixing-based Recommendation\nFramework (MixRec) to empower data augmentation as we wish. Specifically, we\npropose individual mixing and collective mixing, respectively. The former aims\nto provide a new positive sample that is unique to the target (user or item)\nand to make the pair-wise recommendation loss benefit from it, while the latter\naims to portray a new sample that contains group properties in a batch. The two\nmentioned mixing mechanisms allow for data augmentation with only one parameter\nthat does not need to be set multiple times and can be done in linear time\ncomplexity. Besides, we propose the dual-mixing contrastive learning to\nmaximize the utilization of these new-constructed samples to enhance the\nconsistency between pairs of positive samples. Experimental results on four\nreal-world datasets demonstrate the advantages of MixRec in terms of\neffectiveness, simplicity, efficiency, and scalability.\n","authors":["Yi Zhang","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.13579v2.pdf","comment":"Accepted by WWW'25"}],"Multi Media":[{"id":"http://arxiv.org/abs/2501.18588v1","updated":"2025-01-30T18:59:04Z","published":"2025-01-30T18:59:04Z","title":"Inkspire: Supporting Design Exploration with Generative AI through\n  Analogical Sketching","summary":"  With recent advancements in the capabilities of Text-to-Image (T2I) AI\nmodels, product designers have begun experimenting with them in their work.\nHowever, T2I models struggle to interpret abstract language and the current\nuser experience of T2I tools can induce design fixation rather than a more\niterative, exploratory process. To address these challenges, we developed\nInkspire, a sketch-driven tool that supports designers in prototyping product\ndesign concepts with analogical inspirations and a complete\nsketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we\nconducted an exchange session with designers and distilled design goals for\nimproving T2I interactions. In a within-subjects study comparing Inkspire to\nControlNet, we found that Inkspire supported designers with more inspiration\nand exploration of design ideas, and improved aspects of the co-creative\nprocess by allowing designers to effectively grasp the current state of the AI\nto guide it towards novel design intentions.\n","authors":["David Chuan-En Lin","Hyeonsu B. Kang","Nikolas Martelaro","Aniket Kittur","Yan-Ying Chen","Matthew K. Hong"],"pdf_url":"https://arxiv.org/pdf/2501.18588v1.pdf","comment":"Accepted to CHI 2025"},{"id":"http://arxiv.org/abs/2211.11337v4","updated":"2025-01-30T15:13:01Z","published":"2022-11-21T10:37:56Z","title":"DreamArtist++: Controllable One-Shot Text-to-Image Generation via\n  Positive-Negative Adapter","summary":"  State-of-the-arts text-to-image generation models such as Imagen and Stable\nDiffusion Model have succeed remarkable progresses in synthesizing\nhigh-quality, feature-rich images with high resolution guided by human text\nprompts. Since certain characteristics of image content \\emph{e.g.}, very\nspecific object entities or styles, are very hard to be accurately described by\ntext, some example-based image generation approaches have been proposed,\n\\emph{i.e.} generating new concepts based on absorbing the salient features of\na few input references. Despite of acknowledged successes, these methods have\nstruggled on accurately capturing the reference examples' characteristics while\nkeeping diverse and high-quality image generation, particularly in the one-shot\nscenario (\\emph{i.e.} given only one reference). To tackle this problem, we\npropose a simple yet effective framework, namely DreamArtist, which adopts a\nnovel positive-negative prompt-tuning learning strategy on the pre-trained\ndiffusion model, and it has shown to well handle the trade-off between the\naccurate controllability and fidelity of image generation with only one\nreference example. Specifically, our proposed framework incorporates both\npositive and negative embeddings or adapters and optimizes them in a joint\nmanner. The positive part aggressively captures the salient characteristics of\nthe reference image to drive diversified generation and the negative part\nrectifies inadequacies from the positive part. We have conducted extensive\nexperiments and evaluated the proposed method from image similarity (fidelity)\nand diversity, generation controllability, and style cloning. And our\nDreamArtist has achieved a superior generation performance over existing\nmethods. Besides, our additional evaluation on extended tasks, including\nconcept compositions and prompt-guided image editing, demonstrates its\neffectiveness for more applications.\n","authors":["Ziyi Dong","Pengxu Wei","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2211.11337v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18314v1","updated":"2025-01-30T12:43:47Z","published":"2025-01-30T12:43:47Z","title":"AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment","summary":"  Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA, the first large-scale AGAV quality assessment\ndataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The project page is\navailable at https://agav-rater.github.io.\n","authors":["Yuqin Cao","Xiongkuo Min","Yixuan Gao","Wei Sun","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2501.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08988v2","updated":"2025-01-30T11:15:36Z","published":"2024-12-12T06:39:49Z","title":"EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing","summary":"  Given a piece of text, a video clip, and a reference audio, the movie dubbing\ntask aims to generate speech that aligns with the video while cloning the\ndesired voice. The existing methods have two primary deficiencies: (1) They\nstruggle to simultaneously hold audio-visual sync and achieve clear\npronunciation; (2) They lack the capacity to express user-defined emotions. To\naddress these problems, we propose EmoDubber, an emotion-controllable dubbing\narchitecture that allows users to specify emotion type and emotional intensity\nwhile satisfying high-quality lip sync and pronunciation. Specifically, we\nfirst design Lip-related Prosody Aligning (LPA), which focuses on learning the\ninherent consistency between lip motion and prosody variation by duration level\ncontrastive learning to incorporate reasonable alignment. Then, we design\nPronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences\nby efficient conformer to improve speech intelligibility. Next, the speaker\nidentity adapting module aims to decode acoustics prior and inject the speaker\nstyle embedding. After that, the proposed Flow-based User Emotion Controlling\n(FUEC) is used to synthesize waveform by flow matching prediction network\nconditioned on acoustics prior. In this process, the FUEC determines the\ngradient direction and guidance scale based on the user's emotion instructions\nby the positive and negative guidance mechanism, which focuses on amplifying\nthe desired emotion while suppressing others. Extensive experimental results on\nthree benchmark datasets demonstrate favorable performance compared to several\nstate-of-the-art methods.\n","authors":["Gaoxiang Cong","Jiadong Pan","Liang Li","Yuankai Qi","Yuxin Peng","Anton van den Hengel","Jian Yang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.08988v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.18157v1","updated":"2025-01-30T05:46:30Z","published":"2025-01-30T05:46:30Z","title":"Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training\n  and Unimodal Deployment","summary":"  Building reliable speech systems often requires combining multiple\nmodalities, like audio and visual cues. While such multimodal solutions\nfrequently lead to improvements in performance and may even be critical in\ncertain cases, they come with several constraints such as increased sensory\nrequirements, computational cost, and modality synchronization, to mention a\nfew. These challenges constrain the direct uses of these multimodal solutions\nin real-world applications. In this work, we develop approaches where the\nlearning happens with all available modalities but the deployment or inference\nis done with just one or reduced modalities. To do so, we propose a Multimodal\nTraining and Unimodal Deployment (MUTUD) framework which includes a Temporally\nAligned Modality feature Estimation (TAME) module that can estimate information\nfrom missing modality using modalities present during inference. This\ninnovative approach facilitates the integration of information across different\nmodalities, enhancing the overall inference process by leveraging the strengths\nof each modality to compensate for the absence of certain modalities during\ninference. We apply MUTUD to various audiovisual speech tasks and show that it\ncan reduce the performance gap between the multimodal and corresponding\nunimodal models to a considerable extent. MUTUD can achieve this while reducing\nthe model size and compute compared to multimodal models, in some cases by\nalmost 80%.\n","authors":["Joanna Hong","Sanjeel Parekh","Honglie Chen","Jacob Donley","Ke Tan","Buye Xu","Anurag Kumar"],"pdf_url":"https://arxiv.org/pdf/2501.18157v1.pdf","comment":null}]},"2025-01-29T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.18056v1","updated":"2025-01-29T23:41:12Z","published":"2025-01-29T23:41:12Z","title":"RL-based Query Rewriting with Distilled LLM for online E-Commerce\n  Systems","summary":"  Query rewriting (QR) is a critical technique in e-commerce search, addressing\nthe lexical gap between user queries and product descriptions to enhance search\nperformance. Existing QR approaches typically fall into two categories:\ndiscriminative models and generative methods leveraging large language models\n(LLMs). Discriminative models often struggle with natural language\nunderstanding and offer limited flexibility in rewriting, while generative\nLLMs, despite producing high-quality rewrites, face high inference latency and\ncost in online settings. These limitations force offline deployment, making\nthem vulnerable to issues like information staleness and semantic drift. To\novercome these challenges, we propose a novel hybrid pipeline for QR that\nbalances efficiency and effectiveness. Our approach combines offline knowledge\ndistillation to create a lightweight but efficient student model with online\nreinforcement learning (RL) to refine query rewriting dynamically using\nreal-time feedback. A key innovation is the use of LLMs as simulated human\nfeedback, enabling scalable reward signals and cost-effective evaluation\nwithout manual annotations. Experimental results on Amazon ESCI dataset\ndemonstrate significant improvements in query relevance, diversity, and\nadaptability, as well as positive feedback from the LLM simulation. This work\ncontributes to advancing LLM capabilities for domain-specific applications,\noffering a robust solution for dynamic and complex e-commerce search\nenvironments.\n","authors":["Duy A. Nguyen","Rishi Kesav Mohan","Van Yang","Pritom Saha Akash","Kevin Chen-Chuan Chang"],"pdf_url":"https://arxiv.org/pdf/2501.18056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17981v1","updated":"2025-01-29T20:36:29Z","published":"2025-01-29T20:36:29Z","title":"Can Generative LLMs Create Query Variants for Test Collections? An\n  Exploratory Study","summary":"  This paper explores the utility of a Large Language Model (LLM) to\nautomatically generate queries and query variants from a description of an\ninformation need. Given a set of information needs described as backstories, we\nexplore how similar the queries generated by the LLM are to those generated by\nhumans. We quantify the similarity using different metrics and examine how the\nuse of each set would contribute to document pooling when building test\ncollections. Our results show potential in using LLMs to generate query\nvariants. While they may not fully capture the wide variety of human-generated\nvariants, they generate similar sets of relevant documents, reaching up to\n71.1% overlap at a pool depth of 100.\n","authors":["Marwah Alaofi","Luke Gallagher","Mark Sanderson","Falk Scholer","Paul Thomas"],"pdf_url":"https://arxiv.org/pdf/2501.17981v1.pdf","comment":"Published in the proceedings of SIGIR'23"},{"id":"http://arxiv.org/abs/2501.17969v1","updated":"2025-01-29T20:11:35Z","published":"2025-01-29T20:11:35Z","title":"LLMs can be Fooled into Labelling a Document as Relevant (best café\n  near me; this paper is perfectly relevant)","summary":"  LLMs are increasingly being used to assess the relevance of information\nobjects. This work reports on experiments to study the labelling of short texts\n(i.e., passages) for relevance, using multiple open-source and proprietary\nLLMs. While the overall agreement of some LLMs with human judgements is\ncomparable to human-to-human agreement measured in previous research, LLMs are\nmore likely to label passages as relevant compared to human judges, indicating\nthat LLM labels denoting non-relevance are more reliable than those indicating\nrelevance.\n  This observation prompts us to further examine cases where human judges and\nLLMs disagree, particularly when the human judge labels the passage as\nnon-relevant and the LLM labels it as relevant. Results show a tendency for\nmany LLMs to label passages that include the original query terms as relevant.\nWe, therefore, conduct experiments to inject query words into random and\nirrelevant passages, not unlike the way we inserted the query \"best caf\\'e near\nme\" into this paper. The results show that LLMs are highly influenced by the\npresence of query words in the passages under assessment, even if the wider\npassage has no relevance to the query. This tendency of LLMs to be fooled by\nthe mere presence of query words demonstrates a weakness in our current\nmeasures of LLM labelling: relying on overall agreement misses important\npatterns of failures. There is a real risk of bias in LLM-generated relevance\nlabels and, therefore, a risk of bias in rankers trained on those labels.\n  We also investigate the effects of deliberately manipulating LLMs by\ninstructing them to label passages as relevant, similar to the instruction\n\"this paper is perfectly relevant\" inserted above. We find that such\nmanipulation influences the performance of some LLMs, highlighting the critical\nneed to consider potential vulnerabilities when deploying LLMs in real-world\napplications.\n","authors":["Marwah Alaofi","Paul Thomas","Falk Scholer","Mark Sanderson"],"pdf_url":"https://arxiv.org/pdf/2501.17969v1.pdf","comment":"Published in the proceedings of SIGIR-AP'24"},{"id":"http://arxiv.org/abs/2501.17822v1","updated":"2025-01-29T18:14:51Z","published":"2025-01-29T18:14:51Z","title":"Aggregation Schemes for Single-Vector WSI Representation Learning in\n  Digital Pathology","summary":"  A crucial step to efficiently integrate Whole Slide Images (WSIs) in\ncomputational pathology is assigning a single high-quality feature vector,\ni.e., one embedding, to each WSI. With the existence of many pre-trained deep\nneural networks and the emergence of foundation models, extracting embeddings\nfor sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,\ngiven their high resolution and gigapixel nature, inputting them into existing\nGPUs as a single image is not feasible. As a result, WSIs are usually split\ninto many patches. Feeding each patch to a pre-trained model, each WSI can then\nbe represented by a set of patches, hence, a set of embeddings. Hence, in such\na setup, WSI representation learning reduces to set representation learning\nwhere for each WSI we have access to a set of patch embeddings. To obtain a\nsingle embedding from a set of patch embeddings for each WSI, multiple\nset-based learning schemes have been proposed in the literature. In this paper,\nwe evaluate the WSI search performance of multiple recently developed\naggregation techniques (mainly set representation learning techniques)\nincluding simple average or max pooling operations, Deep Sets, Memory networks,\nFocal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse\nand binary Fisher Vector on four different primary sites including bladder,\nbreast, kidney, and Colon from TCGA. Further, we benchmark the search\nperformance of these methods against the median of minimum distances of patch\nembeddings, a non-aggregating approach used for WSI retrieval.\n","authors":["Sobhan Hemati","Ghazal Alabtah","Saghir Alfasly","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2501.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17788v1","updated":"2025-01-29T17:26:47Z","published":"2025-01-29T17:26:47Z","title":"WARP: An Efficient Engine for Multi-Vector Retrieval","summary":"  We study the efficiency of multi-vector retrieval methods like ColBERT and\nits recent variant XTR. We introduce WARP, a retrieval engine that drastically\nimproves the efficiency of XTR-based ColBERT retrievers through three key\ninnovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2)\nimplicit decompression to bypass costly vector reconstruction, and (3) a\ntwo-stage reduction process for efficient scoring. Combined with optimized C++\nkernels and specialized inference runtimes, WARP reduces end-to-end latency by\n41x compared to XTR's reference implementation and thereby achieves a 3x\nspeedup over PLAID from the the official ColBERT implementation.\n  We study the efficiency of multi-vector retrieval methods like ColBERT and\nits recent variant XTR. We introduce WARP, a retrieval engine that drastically\nimproves the efficiency of XTR-based ColBERT retrievers through three key\ninnovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2)\nimplicit decompression during retrieval, and (3) a two-stage reduction process\nfor efficient scoring. Thanks also to highly-optimized C++ kernels and to the\nadoption of specialized inference runtimes, WARP can reduce end-to-end query\nlatency relative to XTR's reference implementation by 41x. And it thereby\nachieves a 3x speedup over the official ColBERTv2 PLAID engine, while\npreserving retrieval quality.\n","authors":["Jan Luca Scheerer","Matei Zaharia","Christopher Potts","Gustavo Alonso","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2501.17788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17998v2","updated":"2025-01-29T17:17:56Z","published":"2024-12-23T21:42:31Z","title":"WavePulse: Real-time Content Analytics of Radio Livestreams","summary":"  Radio remains a pervasive medium for mass information dissemination, with\nAM/FM stations reaching more Americans than either smartphone-based social\nnetworking or live television. Increasingly, radio broadcasts are also streamed\nonline and accessed over the Internet. We present WavePulse, a framework that\nrecords, documents, and analyzes radio content in real-time. While our\nframework is generally applicable, we showcase the efficacy of WavePulse in a\ncollaborative project with a team of political scientists focusing on the 2024\nPresidential Elections. We use WavePulse to monitor livestreams of 396 news\nradio stations over a period of three months, processing close to 500,000 hours\nof audio streams. These streams were converted into time-stamped, diarized\ntranscripts and analyzed to track answer key political science questions at\nboth the national and state levels. Our analysis revealed how local issues\ninteracted with national trends, providing insights into information flow. Our\nresults demonstrate WavePulse's efficacy in capturing and analyzing content\nfrom radio livestreams sourced from the Web. Code and dataset can be accessed\nat \\url{https://wave-pulse.io}.\n","authors":["Govind Mittal","Sarthak Gupta","Shruti Wagle","Chirag Chopra","Anthony J DeMattee","Nasir Memon","Mustaque Ahamad","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2412.17998v2.pdf","comment":"To appear at The Web Conference (WWW) 2025. 20 Pages, 24 figures.\n  Access code and dataset at https://wave-pulse.io"},{"id":"http://arxiv.org/abs/2405.11517v3","updated":"2025-01-29T16:26:36Z","published":"2024-05-19T11:12:10Z","title":"On the Convergence of No-Regret Dynamics in Information Retrieval Games\n  with Proportional Ranking Functions","summary":"  Publishers who publish their content on the web act strategically, in a\nbehavior that can be modeled within the online learning framework. Regret, a\ncentral concept in machine learning, serves as a canonical measure for\nassessing the performance of learning agents within this framework. We prove\nthat any proportional content ranking function with a concave activation\nfunction induces games in which no-regret learning dynamics converge. Moreover,\nfor proportional ranking functions, we prove the equivalence of the concavity\nof the activation function, the social concavity of the induced games and the\nconcavity of the induced games. We also study the empirical trade-offs between\npublishers' and users' welfare, under different choices of the activation\nfunction, using a state-of-the-art no-regret dynamics algorithm. Furthermore,\nwe demonstrate how the choice of the ranking function and changes in the\necosystem structure affect these welfare measures, as well as the dynamics'\nconvergence rate.\n","authors":["Omer Madmon","Idan Pipano","Itamar Reinman","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2405.11517v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17670v1","updated":"2025-01-29T14:20:42Z","published":"2025-01-29T14:20:42Z","title":"Distinguished Quantized Guidance for Diffusion-based Sequence\n  Recommendation","summary":"  Diffusion models (DMs) have emerged as promising approaches for sequential\nrecommendation due to their strong ability to model data distributions and\ngenerate high-quality items. Existing work typically adds noise to the next\nitem and progressively denoises it guided by the user's interaction sequence,\ngenerating items that closely align with user interests. However, we identify\ntwo key issues in this paradigm. First, the sequences are often heterogeneous\nin length and content, exhibiting noise due to stochastic user behaviors. Using\nsuch sequences as guidance may hinder DMs from accurately understanding user\ninterests. Second, DMs are prone to data bias and tend to generate only the\npopular items that dominate the training dataset, thus failing to meet the\npersonalized needs of different users. To address these issues, we propose\nDistinguished Quantized Guidance for Diffusion-based Sequence Recommendation\n(DiQDiff), which aims to extract robust guidance to understand user interests\nand generate distinguished items for personalized user interests within DMs. To\nextract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)\nto quantize sequences into semantic vectors (e.g., collaborative signals and\ncategory interests) using a codebook, which can enrich the guidance to better\nunderstand user interests. To generate distinguished items, DiQDiff\npersonalizes the generation through Contrastive Discrepancy Maximization (CDM),\nwhich maximizes the distance between denoising trajectories using contrastive\nloss to prevent biased generation for different users. Extensive experiments\nare conducted to compare DiQDiff with multiple baseline models across four\nwidely-used datasets. The superior recommendation performance of DiQDiff\nagainst leading approaches demonstrates its effectiveness in sequential\nrecommendation tasks.\n","authors":["Wenyu Mao","Shuchang Liu","Haoyang Liu","Haozhe Liu","Xiang Li","Lanatao Hu"],"pdf_url":"https://arxiv.org/pdf/2501.17670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17630v1","updated":"2025-01-29T13:08:17Z","published":"2025-01-29T13:08:17Z","title":"Uncertainty Quantification and Decomposition for LLM-based\n  Recommendation","summary":"  Despite the widespread adoption of large language models (LLMs) for\nrecommendation, we demonstrate that LLMs often exhibit uncertainty in their\nrecommendations. To ensure the trustworthy use of LLMs in generating\nrecommendations, we emphasize the importance of assessing the reliability of\nrecommendations generated by LLMs. We start by introducing a novel framework\nfor estimating the predictive uncertainty to quantitatively measure the\nreliability of LLM-based recommendations. We further propose to decompose the\npredictive uncertainty into recommendation uncertainty and prompt uncertainty,\nenabling in-depth analyses of the primary source of uncertainty. Through\nextensive experiments, we (1) demonstrate predictive uncertainty effectively\nindicates the reliability of LLM-based recommendations, (2) investigate the\norigins of uncertainty with decomposed uncertainty measures, and (3) propose\nuncertainty-aware prompting for a lower predictive uncertainty and enhanced\nrecommendation. Our source code and model weights are available at\nhttps://github.com/WonbinKweon/UNC_LLM_REC_WWW2025\n","authors":["Wonbin Kweon","Sanghwan Jang","SeongKu Kang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2501.17630v1.pdf","comment":"WWW 2025"},{"id":"http://arxiv.org/abs/2405.05576v2","updated":"2025-01-29T10:59:47Z","published":"2024-05-09T06:52:24Z","title":"LayerPlexRank: Exploring Node Centrality and Layer Influence through\n  Algebraic Connectivity in Multiplex Networks","summary":"  As the calculation of centrality in complex networks becomes increasingly\nvital across technological, biological, and social systems, precise and\nscalable ranking methods are essential for understanding these networks. This\npaper introduces LayerPlexRank, an algorithm that simultaneously assesses node\ncentrality and layer influence in multiplex networks using algebraic\nconnectivity metrics. This method enhances the robustness of the ranking\nalgorithm by effectively assessing structural changes across layers using\nrandom walk, considering the overall connectivity of the graph. We substantiate\nthe utility of LayerPlexRank with theoretical analyses and empirical\nvalidations on varied real-world datasets, contrasting it with established\ncentrality measures.\n","authors":["Hao Ren","Jiaojiao Jiang"],"pdf_url":"https://arxiv.org/pdf/2405.05576v2.pdf","comment":"Published in Proceedings of the 33rd ACM International Conference on\n  Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2501.17449v1","updated":"2025-01-29T07:13:27Z","published":"2025-01-29T07:13:27Z","title":"Cross-Language Approach for Quranic QA","summary":"  Question answering systems face critical limitations in languages with\nlimited resources and scarce data, making the development of robust models\nespecially challenging. The Quranic QA system holds significant importance as\nit facilitates a deeper understanding of the Quran, a Holy text for over a\nbillion people worldwide. However, these systems face unique challenges,\nincluding the linguistic disparity between questions written in Modern Standard\nArabic and answers found in Quranic verses written in Classical Arabic, and the\nsmall size of existing datasets, which further restricts model performance. To\naddress these challenges, we adopt a cross-language approach by (1) Dataset\nAugmentation: expanding and enriching the dataset through machine translation\nto convert Arabic questions into English, paraphrasing questions to create\nlinguistic diversity, and retrieving answers from an English translation of the\nQuran to align with multilingual training requirements; and (2) Language Model\nFine-Tuning: utilizing pre-trained models such as BERT-Medium, RoBERTa-Base,\nDeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, and Falcon to address the\nspecific requirements of Quranic QA. Experimental results demonstrate that this\ncross-language approach significantly improves model performance, with\nRoBERTa-Base achieving the highest MAP@10 (0.34) and MRR (0.52), while\nDeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24). These\nfindings underscore the effectiveness of cross-language strategies in\novercoming linguistic barriers and advancing Quranic QA systems\n","authors":["Islam Oshallah","Mohamed Basem","Ali Hamdi","Ammar Mohammed"],"pdf_url":"https://arxiv.org/pdf/2501.17449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04276v2","updated":"2025-01-29T06:51:35Z","published":"2024-12-05T15:59:05Z","title":"Graph-Sequential Alignment and Uniformity: Toward Enhanced\n  Recommendation Systems","summary":"  Graph-based and sequential methods are two popular recommendation paradigms,\neach excelling in its domain but lacking the ability to leverage signals from\nthe other. To address this, we propose a novel method that integrates both\napproaches for enhanced performance. Our framework uses Graph Neural Network\n(GNN)-based and sequential recommenders as separate submodules while sharing a\nunified embedding space optimized jointly. To enable positive knowledge\ntransfer, we design a loss function that enforces alignment and uniformity both\nwithin and across submodules. Experiments on three real-world datasets\ndemonstrate that the proposed method significantly outperforms using either\napproach alone and achieves state-of-the-art results. Our implementations are\npublicly available at https://github.com/YuweiCao-UIC/GSAU.git.\n","authors":["Yuwei Cao","Liangwei Yang","Zhiwei Liu","Yuqing Liu","Chen Wang","Yueqing Liang","Hao Peng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2412.04276v2.pdf","comment":"Accepted to The Web Conference 2025"},{"id":"http://arxiv.org/abs/2501.17409v1","updated":"2025-01-29T04:22:29Z","published":"2025-01-29T04:22:29Z","title":"Value Function Decomposition in Markov Recommendation Process","summary":"  Recent advances in recommender systems have shown that user-system\ninteraction essentially formulates long-term optimization problems, and online\nreinforcement learning can be adopted to improve recommendation performance.\nThe general solution framework incorporates a value function that estimates the\nuser's expected cumulative rewards in the future and guides the training of the\nrecommendation policy. To avoid local maxima, the policy may explore potential\nhigh-quality actions during inference to increase the chance of finding better\nfuture rewards. To accommodate the stepwise recommendation process, one widely\nadopted approach to learning the value function is learning from the difference\nbetween the values of two consecutive states of a user. However, we argue that\nthis paradigm involves an incorrect approximation in the stochastic process.\nSpecifically, between the current state and the next state in each training\nsample, there exist two separate random factors from the stochastic policy and\nthe uncertain user environment. Original temporal difference (TD) learning\nunder these mixed random factors may result in a suboptimal estimation of the\nlong-term rewards. As a solution, we show that these two factors can be\nseparately approximated by decomposing the original temporal difference loss.\nThe disentangled learning framework can achieve a more accurate estimation with\nfaster learning and improved robustness against action exploration. As\nempirical verification of our proposed method, we conduct offline experiments\nwith online simulated environments built based on public datasets.\n","authors":["Xiaobei Wang","Shuchang Liu","Qingpeng Cai","Xiang Li","Lantao Hu","Han li","Guangming Xie"],"pdf_url":"https://arxiv.org/pdf/2501.17409v1.pdf","comment":"14 pages, 9 figures"}],"Multi Media":[{"id":"http://arxiv.org/abs/2408.08093v2","updated":"2025-01-29T05:19:41Z","published":"2024-08-15T11:36:18Z","title":"When Video Coding Meets Multimodal Large Language Models: A Unified\n  Paradigm for Video Coding","summary":"  Existing codecs are designed to eliminate intrinsic redundancies to create a\ncompact representation for compression. However, strong external priors from\nMultimodal Large Language Models (MLLMs) have not been explicitly explored in\nvideo compression. Herein, we introduce a unified paradigm for Cross-Modality\nVideo Coding (CMVC), which is a pioneering approach to explore multimodality\nrepresentation and video generative models in video coding. Specifically, on\nthe encoder side, we disentangle a video into spatial content and motion\ncomponents, which are subsequently transformed into distinct modalities to\nachieve very compact representation by leveraging MLLMs. During decoding,\npreviously encoded components and video generation models are leveraged to\ncreate multiple encoding-decoding modes that optimize video reconstruction\nquality for specific decoding requirements, including Text-Text-to-Video (TT2V)\nmode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)\nmode to achieve superb perceptual consistency. In addition, we propose an\nefficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)\ntuning to guarantee perceptual quality, which allows the generated motion cues\nto behave smoothly. Experiments on benchmarks indicate that TT2V achieves\neffective semantic reconstruction, while IT2V exhibits competitive perceptual\nconsistency. These results highlight potential directions for future research\nin video coding.\n","authors":["Pingping Zhang","Jinlong Li","Kecheng Chen","Meng Wang","Long Xu","Haoliang Li","Nicu Sebe","Sam Kwong","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17375v1","updated":"2025-01-29T02:04:03Z","published":"2025-01-29T02:04:03Z","title":"Self-Guided Virtual Reality Therapy for Anxiety: A Systematic Review","summary":"  Virtual reality (VR) technology can be used to treat anxiety symptoms and\ndisorders. However, most VR interventions for anxiety have been therapist\nguided rather than self-guided. This systematic review aimed to examine the\neffectiveness and user experience (i.e., usability, acceptability, safety, and\nattrition rates) of self-guided VR therapy interventions in people with any\nanxiety condition as well as provide future research directions. Peer-reviewed\njournal articles reporting on self-guided VR interventions for anxiety were\nsought from the Cochrane Library, IEEE Explore Digital Library, PsycINFO,\nPubMED, Scopus, and Web of Science databases. Study data from the eligible\narticles were extracted, tabulated, and addressed with a narrative synthesis. A\ntotal of 21 articles met the inclusion criteria. The findings revealed that\nself-guided VR interventions for anxiety can provide an effective treatment of\nsocial anxiety disorder, public speaking anxiety, and specific phobias. User\nexperiences outcomes of safety, usability, and acceptability were generally\npositive and the average attrition rate was low. However, there was a lack of\nstandardised assessments to measure user experiences. Self-guided VR for\nanxiety can provide an engaging approach for effectively and safely treating\ncommon anxiety conditions. Nevertheless, more experimental studies are required\nto examine their use in underrepresented anxiety populations, their long-term\ntreatment effects beyond 12 months, and compare their effectiveness against\nother self-help interventions for anxiety (e.g., internet interventions and\nbibliotherapy).\n","authors":["Winona Graham","Russell Drinkwater","Joshua Kelson","Muhammad Ashad Kabir"],"pdf_url":"https://arxiv.org/pdf/2501.17375v1.pdf","comment":"40 pages, 1 figure, 4 tables"}]},"2025-01-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.18636v1","updated":"2025-01-28T17:01:31Z","published":"2025-01-28T17:01:31Z","title":"SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of\n  Large Language Model","summary":"  The indexing-retrieval-generation paradigm of retrieval-augmented generation\n(RAG) has been highly successful in solving knowledge-intensive tasks by\nintegrating external knowledge into large language models (LLMs). However, the\nincorporation of external and unverified knowledge increases the vulnerability\nof LLMs because attackers can perform attack tasks by manipulating knowledge.\nIn this paper, we introduce a benchmark named SafeRAG designed to evaluate the\nRAG security. First, we classify attack tasks into silver noise, inter-context\nconflict, soft ad, and white Denial-of-Service. Next, we construct RAG security\nevaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We\nthen utilize the SafeRAG dataset to simulate various attack scenarios that RAG\nmay encounter. Experiments conducted on 14 representative RAG components\ndemonstrate that RAG exhibits significant vulnerability to all attack tasks and\neven the most apparent attack task can easily bypass existing retrievers,\nfilters, or advanced LLMs, resulting in the degradation of RAG service quality.\nCode is available at: https://github.com/IAAR-Shanghai/SafeRAG.\n","authors":["Xun Liang","Simin Niu","Zhiyu Li","Sensen Zhang","Hanyu Wang","Feiyu Xiong","Jason Zhaoxin Fan","Bo Tang","Shichao Song","Mengwei Wang","Jiawei Yang"],"pdf_url":"https://arxiv.org/pdf/2501.18636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17039v1","updated":"2025-01-28T16:03:52Z","published":"2025-01-28T16:03:52Z","title":"Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block\n  Representations with Large Language Models","summary":"  In recent years, large language models (LLMs) have demonstrated exceptional\npower in various domains, including information retrieval. Most of the previous\npractices involve leveraging these models to create a single embedding for each\nquery, each passage, or each document individually, a strategy exemplified and\nused by the Retrieval-Augmented Generation (RAG) framework. While this method\nhas proven effective, we argue that it falls short in fully capturing the\nnuanced intricacies of document-level texts due to its reliance on a relatively\ncoarse-grained representation. To address this limitation, we introduce a\nnovel, fine-grained approach aimed at enhancing the accuracy of relevance\nscoring for long documents. Our methodology firstly segments a long document\ninto blocks, each of which is embedded using an LLM, for matching with the\nquery representation. When calculating the relevance score, we aggregate the\nquery-block relevance scores through a weighted sum method, yielding a\ncomprehensive score for the query with the entire document. Despite its\napparent simplicity, our experimental findings reveal that this approach\noutperforms standard representation methods and achieves a significant\nreduction in embedding generation latency. Moreover, by carefully optimizing\npairwise loss functions, superior performances have been achieved.\n","authors":["Minghan Li","Eric Gaussier","Guodong Zhou"],"pdf_url":"https://arxiv.org/pdf/2501.17039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09560v2","updated":"2025-01-28T13:17:29Z","published":"2024-12-12T18:46:38Z","title":"Foundational Large Language Models for Materials Research","summary":"  Materials discovery and development are critical for addressing global\nchallenges. Yet, the exponential growth in materials science literature\ncomprising vast amounts of textual data has created significant bottlenecks in\nknowledge extraction, synthesis, and scientific reasoning. Large Language\nModels (LLMs) offer unprecedented opportunities to accelerate materials\nresearch through automated analysis and prediction. Still, their effective\ndeployment requires domain-specific adaptation for understanding and solving\ndomain-relevant tasks. Here, we present LLaMat, a family of foundational models\nfor materials science developed through continued pretraining of LLaMA models\non an extensive corpus of materials literature and crystallographic data.\nThrough systematic evaluation, we demonstrate that LLaMat excels in\nmaterials-specific NLP and structured information extraction while maintaining\ngeneral linguistic capabilities. The specialized LLaMat-CIF variant\ndemonstrates unprecedented capabilities in crystal structure generation,\npredicting stable crystals with high coverage across the periodic table.\nIntriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,\nwe observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific\nperformance across diverse materials science tasks, including structured\ninformation extraction from text and tables, more particularly in crystal\nstructure generation, a potential adaptation rigidity in overtrained LLMs.\nAltogether, the present work demonstrates the effectiveness of domain\nadaptation towards developing practically deployable LLM copilots for materials\nresearch. Beyond materials science, our findings reveal important\nconsiderations for domain adaptation of LLMs, such as model selection, training\nmethodology, and domain-specific performance, which may influence the\ndevelopment of specialized scientific AI systems.\n","authors":["Vaibhav Mishra","Somaditya Singh","Dhruv Ahlawat","Mohd Zaki","Vaibhav Bihani","Hargun Singh Grover","Biswajit Mishra","Santiago Miret"," Mausam","N. M. Anoop Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.09560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16902v1","updated":"2025-01-28T12:40:37Z","published":"2025-01-28T12:40:37Z","title":"Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks","summary":"  Recent advancements in dense retrieval have introduced vision-language model\n(VLM)-based retrievers, such as DSE and ColPali, which leverage document\nscreenshots embedded as vectors to enable effective search and offer a\nsimplified pipeline over traditional text-only methods. In this study, we\npropose three pixel poisoning attack methods designed to compromise VLM-based\nretrievers and evaluate their effectiveness under various attack settings and\nparameter configurations. Our empirical results demonstrate that injecting even\na single adversarial screenshot into the retrieval corpus can significantly\ndisrupt search results, poisoning the top-10 retrieved documents for 41.9% of\nqueries in the case of DSE and 26.4% for ColPali. These vulnerability rates\nnotably exceed those observed with equivalent attacks on text-only retrievers.\nMoreover, when targeting a small set of known queries, the attack success rate\nraises, achieving complete success in certain cases. By exposing the\nvulnerabilities inherent in vision-language models, this work highlights the\npotential risks associated with their deployment.\n","authors":["Shengyao Zhuang","Ekaterina Khramtsova","Xueguang Ma","Bevan Koopman","Jimmy Lin","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2501.16902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16888v1","updated":"2025-01-28T12:18:09Z","published":"2025-01-28T12:18:09Z","title":"Secure Federated Graph-Filtering for Recommender Systems","summary":"  Recommender systems often rely on graph-based filters, such as normalized\nitem-item adjacency matrices and low-pass filters. While effective, the\ncentralized computation of these components raises concerns about privacy,\nsecurity, and the ethical use of user data. This work proposes two\ndecentralized frameworks for securely computing these critical graph components\nwithout centralizing sensitive information. The first approach leverages\nlightweight Multi-Party Computation and distributed singular vector\ncomputations to privately compute key graph filters. The second extends this\nframework by incorporating low-rank approximations, enabling a trade-off\nbetween communication efficiency and predictive performance. Empirical\nevaluations on benchmark datasets demonstrate that the proposed methods achieve\ncomparable accuracy to centralized state-of-the-art systems while ensuring data\nconfidentiality and maintaining low communication costs. Our results highlight\nthe potential for privacy-preserving decentralized architectures to bridge the\ngap between utility and user data protection in modern recommender systems.\n","authors":["Julien Nicolas","César Sabater","Mohamed Maouche","Sonia Ben Mokhtar","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2501.16888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14848v2","updated":"2025-01-28T06:00:44Z","published":"2024-06-21T03:33:51Z","title":"Leveraging Passage Embeddings for Efficient Listwise Reranking with\n  Large Language Models","summary":"  Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. The Code is\navailable at https://github.com/liuqi6777/pe_rank.\n","authors":["Qi Liu","Bo Wang","Nan Wang","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2406.14848v2.pdf","comment":"Accepted by WWW 2025"},{"id":"http://arxiv.org/abs/2501.16722v1","updated":"2025-01-28T05:59:29Z","published":"2025-01-28T05:59:29Z","title":"Hypergraph Diffusion for High-Order Recommender Systems","summary":"  Recommender systems rely on Collaborative Filtering (CF) to predict user\npreferences by leveraging patterns in historical user-item interactions. While\ntraditional CF methods primarily focus on learning compact vector embeddings\nfor users and items, graph neural network (GNN)-based approaches have emerged\nas a powerful alternative, utilizing the structure of user-item interaction\ngraphs to enhance recommendation accuracy. However, existing GNN-based models,\nsuch as LightGCN and UltraGCN, often struggle with two major limitations: an\ninability to fully account for heterophilic interactions, where users engage\nwith diverse item categories, and the over-smoothing problem in multi-layer\nGNNs, which hinders their ability to model complex, high-order relationships.\nTo address these gaps, we introduce WaveHDNN, an innovative wavelet-enhanced\nhypergraph diffusion framework. WaveHDNN integrates a Heterophily-aware\nCollaborative Encoder, designed to capture user-item interactions across\ndiverse categories, with a Multi-scale Group-wise Structure Encoder, which\nleverages wavelet transforms to effectively model localized graph structures.\nAdditionally, cross-view contrastive learning is employed to maintain robust\nand consistent representations. Experiments on benchmark datasets validate the\nefficacy of WaveHDNN, demonstrating its superior ability to capture both\nheterophilic and localized structural information, leading to improved\nrecommendation performance.\n","authors":["Darnbi Sakong","Thanh Trung Huynh","Jun Jo"],"pdf_url":"https://arxiv.org/pdf/2501.16722v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2501.15183v2","updated":"2025-01-28T03:45:40Z","published":"2025-01-25T11:45:49Z","title":"Generating Negative Samples for Multi-Modal Recommendation","summary":"  Multi-modal recommender systems (MMRS) have gained significant attention due\nto their ability to leverage information from various modalities to enhance\nrecommendation quality. However, existing negative sampling techniques often\nstruggle to effectively utilize the multi-modal data, leading to suboptimal\nperformance. In this paper, we identify two key challenges in negative sampling\nfor MMRS: (1) producing cohesive negative samples contrasting with positive\nsamples and (2) maintaining a balanced influence across different modalities.\nTo address these challenges, we propose NegGen, a novel framework that utilizes\nmulti-modal large language models (MLLMs) to generate balanced and contrastive\nnegative samples. We design three different prompt templates to enable NegGen\nto analyze and manipulate item attributes across multiple modalities, and then\ngenerate negative samples that introduce better supervision signals and ensure\nmodality balance. Furthermore, NegGen employs a causal learning module to\ndisentangle the effect of intervened key features and irrelevant item\nattributes, enabling fine-grained learning of user preferences. Extensive\nexperiments on real-world datasets demonstrate the superior performance of\nNegGen compared to state-of-the-art methods in both negative sampling and\nmulti-modal recommendation.\n","authors":["Yanbiao Ji","Yue Ding","Dan Luo","Chang Liu","Jing Tong","Shaokai Wu","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2501.15183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16672v1","updated":"2025-01-28T03:13:16Z","published":"2025-01-28T03:13:16Z","title":"VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic\n  Health Records","summary":"  Methods to ensure factual accuracy of text generated by large language models\n(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence\nsystem that combines retrieval-augmented generation and LLM-as-a-Judge to\nverify whether LLM-generated text is factually supported by a patient's medical\nhistory based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course\nnarratives from discharge summaries into a set of simple statements with\nclinician annotations for whether each statement is supported by the patient's\nEHR clinical notes. Whereas highest agreement between clinicians was 88.5%,\nVeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinican ground truth, suggesting that VeriFact\nexceeds the average clinician's ability to fact-check text against a patient's\nmedical record. VeriFact may accelerate the development of LLM-based EHR\napplications by removing current evaluation bottlenecks.\n","authors":["Philip Chung","Akshay Swaminathan","Alex J. Goodell","Yeasul Kim","S. Momsen Reincke","Lichy Han","Ben Deverett","Mohammad Amin Sadeghi","Abdel-Badih Ariss","Marc Ghanem","David Seong","Andrew A. Lee","Caitlin E. Coombes","Brad Bradshaw","Mahir A. Sufian","Hyo Jung Hong","Teresa P. Nguyen","Mohammad R. Rasouli","Komal Kamra","Mark A. Burbridge","James C. McAvoy","Roya Saffary","Stephen P. Ma","Dev Dash","James Xie","Ellen Y. Wang","Clifford A. Schmiesing","Nigam Shah","Nima Aghaeepour"],"pdf_url":"https://arxiv.org/pdf/2501.16672v1.pdf","comment":"62 pages, 5 figures, 1 table, pre-print manuscript"}],"Multi Media":[{"id":"http://arxiv.org/abs/2501.17011v1","updated":"2025-01-28T15:17:36Z","published":"2025-01-28T15:17:36Z","title":"MIDI-GPT: A Controllable Generative Model for Computer-Assisted\n  Multitrack Music Composition","summary":"  We present and release MIDI-GPT, a generative system based on the Transformer\narchitecture that is designed for computer-assisted music composition\nworkflows. MIDI-GPT supports the infilling of musical material at the track and\nbar level, and can condition generation on attributes including: instrument\ntype, musical style, note density, polyphony level, and note duration. In order\nto integrate these features, we employ an alternative representation for\nmusical material, creating a time-ordered sequence of musical events for each\ntrack and concatenating several tracks into a single sequence, rather than\nusing a single time-ordered sequence where the musical events corresponding to\ndifferent tracks are interleaved. We also propose a variation of our\nrepresentation allowing for expressiveness. We present experimental results\nthat demonstrate that MIDI-GPT is able to consistently avoid duplicating the\nmusical material it was trained on, generate music that is stylistically\nsimilar to the training dataset, and that attribute controls allow enforcing\nvarious constraints on the generated material. We also outline several\nreal-world applications of MIDI-GPT, including collaborations with industry\npartners that explore the integration and evaluation of MIDI-GPT into\ncommercial products, as well as several artistic works produced using it.\n","authors":["Philippe Pasquier","Jeff Ens","Nathan Fradet","Paul Triana","Davide Rizzotti","Jean-Baptiste Rolland","Maryam Safi"],"pdf_url":"https://arxiv.org/pdf/2501.17011v1.pdf","comment":"AAAI 25"},{"id":"http://arxiv.org/abs/2501.08137v2","updated":"2025-01-28T09:14:14Z","published":"2025-01-14T14:15:10Z","title":"Audio-Visual Deepfake Detection With Local Temporal Inconsistencies","summary":"  This paper proposes an audio-visual deepfake detection approach that aims to\ncapture fine-grained temporal inconsistencies between audio and visual\nmodalities. To achieve this, both architectural and data synthesis strategies\nare introduced. From an architectural perspective, a temporal distance map,\ncoupled with an attention mechanism, is designed to capture these\ninconsistencies while minimizing the impact of irrelevant temporal\nsubsequences. Moreover, we explore novel pseudo-fake generation techniques to\nsynthesize local inconsistencies. Our approach is evaluated against\nstate-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating\nits effectiveness in detecting audio-visual deepfakes.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2501.08137v2.pdf","comment":"Accepted in ICASSP 2025"},{"id":"http://arxiv.org/abs/2501.16780v1","updated":"2025-01-28T08:05:22Z","published":"2025-01-28T08:05:22Z","title":"AVE Speech Dataset: A Comprehensive Benchmark for Multi-Modal Speech\n  Recognition Integrating Audio, Visual, and Electromyographic Signals","summary":"  The global aging population faces considerable challenges, particularly in\ncommunication, due to the prevalence of hearing and speech impairments. To\naddress these, we introduce the AVE speech dataset, a comprehensive multi-modal\nbenchmark for speech recognition tasks. The dataset includes a 100-sentence\nMandarin Chinese corpus with audio signals, lip-region video recordings, and\nsix-channel electromyography (EMG) data, collected from 100 participants. Each\nsubject read the entire corpus ten times, with each sentence averaging\napproximately two seconds in duration, resulting in over 55 hours of\nmulti-modal speech data per modality. Experiments demonstrate that combining\nthese modalities significantly improves recognition performance, particularly\nin cross-subject and high-noise environments. To our knowledge, this is the\nfirst publicly available sentence-level dataset integrating these three\nmodalities for large-scale Mandarin speech recognition. We expect this dataset\nto drive advancements in both acoustic and non-acoustic speech recognition\nresearch, enhancing cross-modal learning and human-machine interaction.\n","authors":["Dongliang Zhou","Yakun Zhang","Jinghan Wu","Xingyu Zhang","Liang Xie","Erwei Yin"],"pdf_url":"https://arxiv.org/pdf/2501.16780v1.pdf","comment":null}]},"2025-01-27T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.16450v1","updated":"2025-01-27T19:14:52Z","published":"2025-01-27T19:14:52Z","title":"360Brew: A Decoder-only Foundation Model for Personalized Ranking and\n  Recommendation","summary":"  Ranking and recommendation systems are the foundation for numerous online\nexperiences, ranging from search results to personalized content delivery.\nThese systems have evolved into complex, multilayered architectures that\nleverage vast datasets and often incorporate thousands of predictive models.\nThe maintenance and enhancement of these models is a labor intensive process\nthat requires extensive feature engineering. This approach not only exacerbates\ntechnical debt but also hampers innovation in extending these systems to\nemerging problem domains. In this report, we present our research to address\nthese challenges by utilizing a large foundation model with a textual interface\nfor ranking and recommendation tasks. We illustrate several key advantages of\nour approach: (1) a single model can manage multiple predictive tasks involved\nin ranking and recommendation, (2) decoder models with textual interface due to\ntheir comprehension of reasoning capabilities, can generalize to new\nrecommendation surfaces and out-of-domain problems, and (3) by employing\nnatural language interfaces for task definitions and verbalizing member\nbehaviors and their social connections, we eliminate the need for feature\nengineering and the maintenance of complex directed acyclic graphs of model\ndependencies. We introduce our research pre-production model, 360Brew V1.0, a\n150B parameter, decoder-only model that has been trained and fine-tuned on\nLinkedIn's data and tasks. This model is capable of solving over 30 predictive\ntasks across various segments of the LinkedIn platform, achieving performance\nlevels comparable to or exceeding those of current production systems based on\noffline metrics, without task-specific fine-tuning. Notably, each of these\ntasks is conventionally addressed by dedicated models that have been developed\nand maintained over multiple years by teams of a similar or larger size than\nour own.\n","authors":["Hamed Firooz","Maziar Sanjabi","Adrian Englhardt","Aman Gupta","Ben Levine","Dre Olgiati","Gungor Polatkan","Iuliia Melnychuk","Karthik Ramgopal","Kirill Talanine","Kutta Srinivasan","Luke Simon","Natesh Sivasubramoniapillai","Necip Fazil Ayan","Qingquan Song","Samira Sriram","Souvik Ghosh","Tao Song","Vignesh Kothapalli","Xiaoling Zhai","Ya Xu","Yu Wang","Yun Dai"],"pdf_url":"https://arxiv.org/pdf/2501.16450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16303v1","updated":"2025-01-27T18:45:07Z","published":"2025-01-27T18:45:07Z","title":"RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based\n  Video Event Retrieval","summary":"  Retrieving events from videos using text queries has become increasingly\nchallenging due to the rapid growth of multimedia content. Existing methods for\ntext-based video event retrieval often focus heavily on object-level\ndescriptions, overlooking the crucial role of contextual information. This\nlimitation is especially apparent when queries lack sufficient context, such as\nmissing location details or ambiguous background elements. To address these\nchallenges, we propose a novel system called RAPID (Retrieval-Augmented\nParallel Inference Drafting), which leverages advancements in Large Language\nModels (LLMs) and prompt-based learning to semantically correct and enrich user\nqueries with relevant contextual information. These enriched queries are then\nprocessed through parallel retrieval, followed by an evaluation step to select\nthe most relevant results based on their alignment with the original query.\nThrough extensive experiments on our custom-developed dataset, we demonstrate\nthat RAPID significantly outperforms traditional retrieval methods,\nparticularly for contextually incomplete queries. Our system was validated for\nboth speed and accuracy through participation in the Ho Chi Minh City AI\nChallenge 2024, where it successfully retrieved events from over 300 hours of\nvideo. Further evaluation comparing RAPID with the baseline proposed by the\ncompetition organizers demonstrated its superior effectiveness, highlighting\nthe strength and robustness of our approach.\n","authors":["Long Nguyen","Huy Nguyen","Bao Khuu","Huy Luu","Huy Le","Tuan Nguyen","Tho Quan"],"pdf_url":"https://arxiv.org/pdf/2501.16303v1.pdf","comment":"Under review at SoICT'24"},{"id":"http://arxiv.org/abs/2501.16276v1","updated":"2025-01-27T18:10:34Z","published":"2025-01-27T18:10:34Z","title":"URAG: Implementing a Unified Hybrid RAG for Precise Answers in\n  University Admission Chatbots -- A Case Study at HCMUT","summary":"  With the rapid advancement of Artificial Intelligence, particularly in\nNatural Language Processing, Large Language Models (LLMs) have become pivotal\nin educational question-answering systems, especially university admission\nchatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other\nadvanced techniques have been developed to enhance these systems by integrating\nspecific university data, enabling LLMs to provide informed responses on\nadmissions and academic counseling. However, these enhanced RAG techniques\noften involve high operational costs and require the training of complex,\nspecialized modules, which poses challenges for practical deployment.\nAdditionally, in the educational context, it is crucial to provide accurate\nanswers to prevent misinformation, a task that LLM-based systems find\nchallenging without appropriate strategies and methods. In this paper, we\nintroduce the Unified RAG (URAG) Framework, a hybrid approach that\nsignificantly improves the accuracy of responses, particularly for critical\nqueries. Experimental results demonstrate that URAG enhances our in-house,\nlightweight model to perform comparably to state-of-the-art commercial models.\nMoreover, to validate its practical applicability, we conducted a case study at\nour educational institution, which received positive feedback and acclaim. This\nstudy not only proves the effectiveness of URAG but also highlights its\nfeasibility for real-world implementation in educational settings.\n","authors":["Long Nguyen","Tho Quan"],"pdf_url":"https://arxiv.org/pdf/2501.16276v1.pdf","comment":"Under review at SoICT'24"},{"id":"http://arxiv.org/abs/2409.04432v2","updated":"2025-01-27T18:03:08Z","published":"2024-09-06T17:54:43Z","title":"A Survey on Knowledge Organization Systems of Research Fields: Resources\n  and Challenges","summary":"  Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.\n","authors":["Angelo Salatino","Tanay Aggarwal","Andrea Mannocci","Francesco Osborne","Enrico Motta"],"pdf_url":"https://arxiv.org/pdf/2409.04432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16214v1","updated":"2025-01-27T17:06:56Z","published":"2025-01-27T17:06:56Z","title":"Provence: efficient and robust context pruning for retrieval-augmented\n  generation","summary":"  Retrieval-augmented generation improves various aspects of large language\nmodels (LLMs) generation, but suffers from computational overhead caused by\nlong contexts as well as the propagation of irrelevant retrieved information\ninto generated responses. Context pruning deals with both aspects, by removing\nirrelevant parts of retrieved contexts before LLM generation. Existing context\npruning approaches are however limited, and do not provide a universal model\nthat would be both efficient and robust in a wide range of scenarios, e.g.,\nwhen contexts contain a variable amount of relevant information or vary in\nlength, or when evaluated on various domains. In this work, we close this gap\nand introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),\nan efficient and robust context pruner for Question Answering, which\ndynamically detects the needed amount of pruning for a given context and can be\nused out-of-the-box for various domains. The three key ingredients of Provence\nare formulating the context pruning task as sequence labeling, unifying context\npruning capabilities with context reranking, and training on diverse data. Our\nexperimental results show that Provence enables context pruning with negligible\nto no drop in performance, in various domains and settings, at almost no cost\nin a standard RAG pipeline. We also conduct a deeper analysis alongside various\nablations to provide insights into training context pruners for future work.\n","authors":["Nadezhda Chirkova","Thibault Formal","Vassilina Nikoulina","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2501.16214v1.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2501.16171v1","updated":"2025-01-27T16:13:50Z","published":"2025-01-27T16:13:50Z","title":"Separate This, and All of these Things Around It: Music Source\n  Separation via Hyperellipsoidal Queries","summary":"  Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.\n","authors":["Karn N. Watcharasupat","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2501.16171v1.pdf","comment":"Submitted to the 2025 International Joint Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2405.17890v3","updated":"2025-01-27T15:12:02Z","published":"2024-05-28T07:12:06Z","title":"SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation","summary":"  Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR.\n","authors":["Wujiang Xu","Qitian Wu","Zujie Liang","Jiaojiao Han","Xuying Ning","Yunxiao Shi","Wenfang Lin","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.17890v3.pdf","comment":"International Conference on Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2501.16112v1","updated":"2025-01-27T15:04:00Z","published":"2025-01-27T15:04:00Z","title":"Survey: Understand the challenges of MachineLearning Experts using Named\n  EntityRecognition Tools","summary":"  This paper presents a survey based on Kasunic's survey research methodology\nto identify the criteria used by Machine Learning (ML) experts to evaluate\nNamed Entity Recognition (NER) tools and frameworks. Comparison and selection\nof NER tools and frameworks is a critical step in leveraging NER for\nInformation Retrieval to support the development of Clinical Practice\nGuidelines. In addition, this study examines the main challenges faced by ML\nexperts when choosing suitable NER tools and frameworks. Using Nunamaker's\nmethodology, the article begins with an introduction to the topic,\ncontextualizes the research, reviews the state-of-the-art in science and\ntechnology, and identifies challenges for an expert survey on NER tools and\nframeworks. This is followed by a description of the survey's design and\nimplementation. The paper concludes with an evaluation of the survey results\nand the insights gained, ending with a summary and conclusions.\n","authors":["Florian Freund","Philippe Tamla","Matthias Hemmje"],"pdf_url":"https://arxiv.org/pdf/2501.16112v1.pdf","comment":"20 Pages, 13 Figures, 6th International Conference on Natural\n  Language Processing, Information Retrieval and AI (NIAI 2025) January 25 ~\n  26, 2025, Copenhagen, Denmark"},{"id":"http://arxiv.org/abs/2501.16111v1","updated":"2025-01-27T15:03:26Z","published":"2025-01-27T15:03:26Z","title":"Options-Aware Dense Retrieval for Multiple-Choice query Answering","summary":"  Long-context multiple-choice question answering tasks require robust\nreasoning over extensive text sources. Since most of the pre-trained\ntransformer models are restricted to processing only a few hundred words at a\ntime, successful completion of such tasks often relies on the identification of\nevidence spans, such as sentences, that provide supporting evidence for\nselecting the correct answer. Prior research in this domain has predominantly\nutilized pre-trained dense retrieval models, given the absence of supervision\nto fine-tune the retrieval process. This paper proposes a novel method called\nOptions Aware Dense Retrieval (OADR) to address these challenges. ORDA uses an\ninnovative approach to fine-tuning retrieval by leveraging query-options\nembeddings, which aim to mimic the embeddings of the oracle query (i.e., the\nquery paired with the correct answer) for enhanced identification of supporting\nevidence. Through experiments conducted on the QuALITY benchmark dataset, we\ndemonstrate that our proposed model surpasses existing baselines in terms of\nperformance and accuracy.\n","authors":["Manish Singh","Manish Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2501.16111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00080v4","updated":"2025-01-27T14:55:40Z","published":"2024-04-30T16:35:08Z","title":"Recommenadation aided Caching using Combinatorial Multi-armed Bandits","summary":"  We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.\n","authors":["Pavamana K J","Chandramani Kishore Singh"],"pdf_url":"https://arxiv.org/pdf/2405.00080v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16075v1","updated":"2025-01-27T14:26:27Z","published":"2025-01-27T14:26:27Z","title":"PISCO: Pretty Simple Compression for Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy.\n","authors":["Maxime Louis","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2501.16075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15953v1","updated":"2025-01-27T10:57:24Z","published":"2025-01-27T10:57:24Z","title":"Understanding Long Videos via LLM-Powered Entity Relation Graphs","summary":"  The analysis of extended video content poses unique challenges in artificial\nintelligence, particularly when dealing with the complexity of tracking and\nunderstanding visual elements across time. Current methodologies that process\nvideo frames sequentially struggle to maintain coherent tracking of objects,\nespecially when these objects temporarily vanish and later reappear in the\nfootage. A critical limitation of these approaches is their inability to\neffectively identify crucial moments in the video, largely due to their limited\ngrasp of temporal relationships. To overcome these obstacles, we present\nGraphVideoAgent, a cutting-edge system that leverages the power of graph-based\nobject tracking in conjunction with large language model capabilities. At its\ncore, our framework employs a dynamic graph structure that maps and monitors\nthe evolving relationships between visual entities throughout the video\nsequence. This innovative approach enables more nuanced understanding of how\nobjects interact and transform over time, facilitating improved frame selection\nthrough comprehensive contextual awareness. Our approach demonstrates\nremarkable effectiveness when tested against industry benchmarks. In\nevaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2\nimprovement over existing methods while requiring analysis of only 8.2 frames\non average. Similarly, testing on the NExT-QA benchmark yielded a 2.0\nperformance increase with an average frame requirement of 8.1. These results\nunderscore the efficiency of our graph-guided methodology in enhancing both\naccuracy and computational performance in long-form video understanding tasks.\n","authors":["Meng Chu","Yicong Li","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2501.15953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15915v1","updated":"2025-01-27T10:04:49Z","published":"2025-01-27T10:04:49Z","title":"Parametric Retrieval Augmented Generation","summary":"  Retrieval-augmented generation (RAG) techniques have emerged as a promising\nsolution to enhance the reliability of large language models (LLMs) by\naddressing issues like hallucinations, outdated knowledge, and domain\nadaptation. In particular, existing RAG methods append relevant documents\nretrieved from external corpus or databases to the input of LLMs to guide their\ngeneration process, which we refer to as the in-context knowledge injection\nmethod. While this approach is simple and often effective, it has inherent\nlimitations. Firstly, increasing the context length and number of relevant\ndocuments can lead to higher computational overhead and degraded performance,\nespecially in complex reasoning tasks. More importantly, in-context knowledge\ninjection operates primarily at the input level, but LLMs store their internal\nknowledge in their parameters. This gap fundamentally limits the capacity of\nin-context methods. To this end, we introduce Parametric retrieval-augmented\ngeneration (Parametric RAG), a new RAG paradigm that integrates external\nknowledge directly into the parameters of feed-forward networks (FFN) of an LLM\nthrough document parameterization. This approach not only saves online\ncomputational costs by eliminating the need to inject multiple documents into\nthe LLMs' input context, but also deepens the integration of external knowledge\ninto the parametric knowledge space of the LLM. Experimental results\ndemonstrate that Parametric RAG substantially enhances both the effectiveness\nand efficiency of knowledge augmentation in LLMs. Also, it can be combined with\nin-context RAG methods to achieve even better performance.\n  We have open-sourced all the code, data, and models in the following\nanonymized GitHub link: https://github.com/oneal2000/PRAG\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Junxi Yan","Changyue Wang","Hongning Wang","Ziyi Ye","Yujia Zhou","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2501.15915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17191v1","updated":"2025-01-27T09:29:55Z","published":"2025-01-27T09:29:55Z","title":"Aspect-Aware Decomposition for Opinion Summarization","summary":"  Opinion summarization plays a key role in deriving meaningful insights from\nlarge-scale online reviews. To make this process more explainable and grounded,\nwe propose a modular approach guided by review aspects which separates the\ntasks of aspect identification, opinion consolidation, and meta-review\nsynthesis, enabling greater transparency and ease of inspection. We conduct\nextensive experiments across datasets representing scientific research,\nbusiness, and product domains. Results show that our method generates more\ngrounded summaries compared to strong baseline models, as verified through\nautomated and human evaluations. Additionally, our modular approach, which\nincorporates reasoning based on review aspects, produces more informative\nintermediate outputs than knowledge-agnostic decomposed prompting. These\nintermediate outputs can also effectively support humans in summarizing\nopinions from large volumes of reviews.\n","authors":["Miao Li","Jey Han Lau","Eduard Hovy","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2501.17191v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2501.15817v1","updated":"2025-01-27T06:52:50Z","published":"2025-01-27T06:52:50Z","title":"Long-Term Interest Clock: Fine-Grained Time Perception in Streaming\n  Recommendation System","summary":"  User interests manifest a dynamic pattern within the course of a day, e.g., a\nuser usually favors soft music at 8 a.m. but may turn to ambient music at 10\np.m. To model dynamic interests in a day, hour embedding is widely used in\ntraditional daily-trained industrial recommendation systems. However, its\ndiscreteness can cause periodical online patterns and instability in recent\nstreaming recommendation systems. Recently, Interest Clock has achieved\nremarkable performance in streaming recommendation systems. Nevertheless, it\nmodels users' dynamic interests in a coarse-grained manner, merely encoding\nusers' discrete interests of 24 hours from short-term behaviors. In this paper,\nwe propose a fine-grained method for perceiving time information for streaming\nrecommendation systems, named Long-term Interest Clock (LIC). The key idea of\nLIC is adaptively calculating current user interests by taking into\nconsideration the relevance of long-term behaviors around current time (e.g., 8\na.m.) given a candidate item. LIC consists of two modules: (1) Clock-GSU\nretrieves a sub-sequence by searching through long-term behaviors, using query\ninformation from a candidate item and current time, (2) Clock-ESU employs a\ntime-gap-aware attention mechanism to aggregate sub-sequence with the candidate\nitem. With Clock-GSU and Clock-ESU, LIC is capable of capturing users' dynamic\nfine-grained interests from long-term behaviors. We conduct online A/B tests,\nobtaining +0.122% improvements on user active days. Besides, the extended\noffline experiments show improvements as well. Long-term Interest Clock has\nbeen integrated into Douyin Music App's recommendation system.\n","authors":["Yongchun Zhu","Guanyu Jiang","Jingwu Chen","Feng Zhang","Xiao Yang","Zuotao Liu"],"pdf_url":"https://arxiv.org/pdf/2501.15817v1.pdf","comment":"Accepted by WWW2025"},{"id":"http://arxiv.org/abs/2501.15816v1","updated":"2025-01-27T06:49:27Z","published":"2025-01-27T06:49:27Z","title":"AdaF^2M^2: Comprehensive Learning and Responsive Leveraging Features in\n  Recommendation System","summary":"  Feature modeling, which involves feature representation learning and\nleveraging, plays an essential role in industrial recommendation systems.\nHowever, the data distribution in real-world applications usually follows a\nhighly skewed long-tail pattern due to the popularity bias, which easily leads\nto over-reliance on ID-based features, such as user/item IDs and ID sequences\nof interactions. Such over-reliance makes it hard for models to learn features\ncomprehensively, especially for those non-ID meta features, e.g., user/item\ncharacteristics. Further, it limits the feature leveraging ability in models,\ngetting less generalized and more susceptible to data noise. Previous studies\non feature modeling focus on feature extraction and interaction, hardly\nnoticing the problems brought about by the long-tail data distribution. To\nachieve better feature representation learning and leveraging on real-world\ndata, we propose a model-agnostic framework AdaF^2M^2, short for Adaptive\nFeature Modeling with Feature Mask. The feature-mask mechanism helps\ncomprehensive feature learning via multi-forward training with augmented\nsamples, while the adapter applies adaptive weights on features responsive to\ndifferent user/item states. By arming base models with AdaF^2M^2, we conduct\nonline A/B tests on multiple recommendation scenarios, obtaining +1.37% and\n+1.89% cumulative improvements on user active days and app duration\nrespectively. Besides, the extended offline experiments on different models\nshow improvements as well. AdaF$^2$M$^2$ has been widely deployed on both\nretrieval and ranking tasks in multiple applications of Douyin Group,\nindicating its superior effectiveness and universality.\n","authors":["Yongchun Zhu","Jingwu Chen","Ling Chen","Yitan Li","Feng Zhang","Xiao Yang","Zuotao Liu"],"pdf_url":"https://arxiv.org/pdf/2501.15816v1.pdf","comment":"Accepted by DASFAA2025"},{"id":"http://arxiv.org/abs/2402.02803v2","updated":"2025-01-27T04:30:43Z","published":"2024-02-05T08:25:22Z","title":"Large Language Model Distilling Medication Recommendation Model","summary":"  The recommendation of medication is a vital aspect of intelligent healthcare\nsystems, as it involves prescribing the most suitable drugs based on a\npatient's specific health needs. Unfortunately, many sophisticated models\ncurrently in use tend to overlook the nuanced semantics of medical data, while\nonly relying heavily on identities. Furthermore, these models face significant\nchallenges in handling cases involving patients who are visiting the hospital\nfor the first time, as they lack prior prescription histories to draw upon. To\ntackle these issues, we harness the powerful semantic comprehension and\ninput-agnostic characteristics of Large Language Models (LLMs). Our research\naims to transform existing medication recommendation methodologies using LLMs.\nIn this paper, we introduce a novel approach called Large Language Model\nDistilling Medication Recommendation (LEADER). We begin by creating appropriate\nprompt templates that enable LLMs to suggest medications effectively. However,\nthe straightforward integration of LLMs into recommender systems leads to an\nout-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a\nnovel output layer and a refined tuning loss function. Although LLM-based\nmodels exhibit remarkable capabilities, they are plagued by high computational\ncosts during inference, which is impractical for the healthcare sector. To\nmitigate this, we have developed a feature-level knowledge distillation\ntechnique, which transfers the LLM's proficiency to a more compact model.\nExtensive experiments conducted on two real-world datasets, MIMIC-III and\nMIMIC-IV, demonstrate that our proposed model not only delivers effective\nresults but also is efficient. To ease the reproducibility of our experiments,\nwe release the implementation code online.\n","authors":["Qidong Liu","Xian Wu","Xiangyu Zhao","Yuanshao Zhu","Zijian Zhang","Feng Tian","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.02803v2.pdf","comment":null}],"Multi Media":[{"id":"http://arxiv.org/abs/2501.16326v1","updated":"2025-01-27T18:59:02Z","published":"2025-01-27T18:59:02Z","title":"Movement- and Traffic-based User Identification in Commercial Virtual\n  Reality Applications: Threats and Opportunities","summary":"  With the unprecedented diffusion of virtual reality, the number of\napplication scenarios is continuously growing. As commercial and gaming\napplications become pervasive, the need for the secure and convenient\nidentification of users, often overlooked by the research in immersive media,\nis becoming more and more pressing. Networked scenarios such as Cloud gaming or\ncooperative virtual training and teleoperation require both a user-friendly and\nstreamlined experience and user privacy and security. In this work, we\ninvestigate the possibility of identifying users from their movement patterns\nand data traffic traces while playing four commercial games, using a publicly\navailable dataset. If, on the one hand, this paves the way for easy\nidentification and automatic customization of the virtual reality content, it\nalso represents a serious threat to users' privacy due to network\nanalysis-based fingerprinting. Based on this, we analyze the threats and\nopportunities for virtual reality users' security and privacy.\n","authors":["Sara Baldoni","Salim Benhamadi","Federico Chiariotti","Michele Zorzi","Federica Battisti"],"pdf_url":"https://arxiv.org/pdf/2501.16326v1.pdf","comment":"Accepted for publication at IEEE VR 2025"},{"id":"http://arxiv.org/abs/2501.16164v1","updated":"2025-01-27T15:59:58Z","published":"2025-01-27T15:59:58Z","title":"MetaDecorator: Generating Immersive Virtual Tours through Multimodality","summary":"  MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience.\n","authors":["Shuang Xie","Yang Liu","Jeannie S. A. Lee","Haiwei Dong"],"pdf_url":"https://arxiv.org/pdf/2501.16164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12791v2","updated":"2025-01-27T10:40:20Z","published":"2024-12-17T10:52:50Z","title":"Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning","summary":"  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.\n","authors":["Shiping Ge","Qiang Chen","Zhiwei Jiang","Yafeng Yin","Liu Qin","Ziyao Chen","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12791v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.15721v1","updated":"2025-01-27T01:22:11Z","published":"2025-01-27T01:22:11Z","title":"On Parallelism in Music and Language: A Perspective from Symbol\n  Emergence Systems based on Probabilistic Generative Models","summary":"  Music and language are structurally similar. Such structural similarity is\noften explained by generative processes. This paper describes the recent\ndevelopment of probabilistic generative models (PGMs) for language learning and\nsymbol emergence in robotics. Symbol emergence in robotics aims to develop a\nrobot that can adapt to real-world environments and human linguistic\ncommunications and acquire language from sensorimotor information alone (i.e.,\nin an unsupervised manner). This is regarded as a constructive approach to\nsymbol emergence systems. To this end, a series of PGMs have been developed,\nincluding those for simultaneous phoneme and word discovery, lexical\nacquisition, object and spatial concept formation, and the emergence of a\nsymbol system. By extending the models, a symbol emergence system comprising a\nmulti-agent system in which a symbol system emerges is revealed to be modeled\nusing PGMs. In this model, symbol emergence can be regarded as collective\npredictive coding. This paper expands on this idea by combining the theory that\n''emotion is based on the predictive coding of interoceptive signals'' and\n''symbol emergence systems,'' and describes the possible hypothesis of the\nemergence of meaning in music.\n","authors":["Tadahiro Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2501.15721v1.pdf","comment":null}]},"2025-01-26T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.15587v1","updated":"2025-01-26T16:26:38Z","published":"2025-01-26T16:26:38Z","title":"SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized\n  Pipeline for Automated Extraction in the Higher Education Science Domain","summary":"  Recent breakthroughs in large language models (LLMs) exemplified by the\nimpressive mathematical and scientific reasoning capabilities of the o1 model\nhave spotlighted the critical importance of high-quality training data in\nadvancing LLM performance across STEM disciplines. While the mathematics\ncommunity has benefited from a growing body of curated datasets, the scientific\ndomain at the higher education level has long suffered from a scarcity of\ncomparable resources. To address this gap, we present SCP-116K, a new\nlarge-scale dataset of 116,756 high-quality problem-solution pairs,\nautomatically extracted from heterogeneous sources using a streamlined and\nhighly generalizable pipeline. Our approach involves stringent filtering to\nensure the scientific rigor and educational level of the extracted materials,\nwhile maintaining adaptability for future expansions or domain transfers. By\nopenly releasing both the dataset and the extraction pipeline, we seek to\nfoster research on scientific reasoning, enable comprehensive performance\nevaluations of new LLMs, and lower the barrier to replicating the successes of\nadvanced models like o1 in the broader science community. We believe SCP-116K\nwill serve as a critical resource, catalyzing progress in high-level scientific\nreasoning tasks and promoting further innovations in LLM development. The\ndataset and code are publicly available at\nhttps://github.com/AQA6666/SCP-116K-open.\n","authors":["Dakuan Lu","Xiaoyu Tan","Rui Xu","Tianchu Yao","Chao Qu","Wei Chu","Yinghui Xu","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2501.15587v1.pdf","comment":"9 pages, 1 figures"},{"id":"http://arxiv.org/abs/2501.15470v1","updated":"2025-01-26T10:16:42Z","published":"2025-01-26T10:16:42Z","title":"Unveiling the Potential of Multimodal Retrieval Augmented Generation\n  with Planning","summary":"  Multimodal Retrieval Augmented Generation (MRAG) systems, while promising for\nenhancing Multimodal Large Language Models (MLLMs), often rely on rigid,\nsingle-step retrieval methods. This limitation hinders their ability to\neffectively address real-world scenarios that demand adaptive information\nacquisition and query refinement. To overcome this, we introduce the novel task\nof Multimodal Retrieval Augmented Generation Planning (MRAG Planning), focusing\non optimizing MLLM performance while minimizing computational overhead. We\npresent CogPlanner, a versatile framework inspired by human cognitive\nprocesses. CogPlanner iteratively refines queries and selects retrieval\nstrategies, enabling both parallel and sequential modeling approaches. To\nrigorously evaluate MRAG Planning, we introduce CogBench, a new benchmark\nspecifically designed for this task. CogBench facilitates the integration of\nlightweight CogPlanner with resource-efficient MLLMs. Our experimental findings\ndemonstrate that CogPlanner surpasses existing MRAG baselines, achieving\nsignificant improvements in both accuracy and efficiency with minimal\ncomputational overhead.\n","authors":["Xiaohan Yu","Zhihan Yang","Chong Chen"],"pdf_url":"https://arxiv.org/pdf/2501.15470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00490v2","updated":"2025-01-26T10:08:58Z","published":"2024-08-01T11:51:52Z","title":"Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation","summary":"  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.\n","authors":["Chu Zhao","Enneng Yang","Yuliang Liang","Pengxiang Lan","Yuting Liu","Jianzhe Zhao","Guibing Guo","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00490v2.pdf","comment":"14 pages, accepted by WWW2025"},{"id":"http://arxiv.org/abs/2501.15429v1","updated":"2025-01-26T07:10:22Z","published":"2025-01-26T07:10:22Z","title":"An Aspect Performance-aware Hypergraph Neural Network for Review-based\n  Recommendation","summary":"  Online reviews allow consumers to provide detailed feedback on various\naspects of items. Existing methods utilize these aspects to model users'\nfine-grained preferences for specific item features through graph neural\nnetworks. We argue that the performance of items on different aspects is\nimportant for making precise recommendations, which has not been taken into\naccount by existing approaches, due to lack of data. In this paper, we propose\nan aspect performance-aware hypergraph neural network (APH) for the\nreview-based recommendation, which learns the performance of items from the\nconflicting sentiment polarity of user reviews. Specifically, APH\ncomprehensively models the relationships among users, items, aspects, and\nsentiment polarity by systematically constructing an aspect hypergraph based on\nuser reviews. In addition, APH aggregates aspects representing users and items\nby employing an aspect performance-aware hypergraph aggregation method. It\naggregates the sentiment polarities from multiple users by jointly considering\nuser preferences and the semantics of their sentiments, determining the weights\nof sentiment polarities to infer the performance of items on various aspects.\nSuch performances are then used as weights to aggregate neighboring aspects.\nExperiments on six real-world datasets demonstrate that APH improves MSE,\nPrecision@5, and Recall@5 by an average of 2.30%, 4.89%, and 1.60% over the\nbest baseline. The source code and data are available at\nhttps://github.com/dianziliu/APH.\n","authors":["Junrui Liu","Tong Li","Di Wu","Zifang Tang","Yuan Fang","Zhen Yang"],"pdf_url":"https://arxiv.org/pdf/2501.15429v1.pdf","comment":"12 pages, accepted by WSDM'25"},{"id":"http://arxiv.org/abs/2501.15425v1","updated":"2025-01-26T07:04:57Z","published":"2025-01-26T07:04:57Z","title":"An Empirically-parametrized Spatio-Temporal Extended-SIR Model for\n  Combined Dilution and Vaccination Mitigation for Rabies Outbreaks in Wild\n  Jackals","summary":"  The transmission of zoonotic diseases between animals and humans poses an\nincreasing threat. Rabies is a prominent example with various instances\nglobally, facilitated by a surplus of meso-predators (commonly, facultative\nsynanthropic species e.g., golden jackals [Canis aureus, hereafter jackals])\nthanks to the abundance of anthropogenic resources leading to dense populations\nclose to human establishments. To mitigate rabies outbreaks and prevent human\ninfections, authorities target the jackal which is the main rabies vector in\nmany regions, through the dissemination of oral vaccines in known jackals'\nactivity centers, as well as opportunistic culling to reduce population\ndensity. Because dilution (i.e., culling) is not selective towards sick or\nun-vaccinated individuals, these two complementary epizootic intervention\npolicies (EIPs) can interfere with each other. Nonetheless, there is only\nlimited examination of the interactive effectiveness of these EIPs and their\npotential influence on rabies epizootic spread dynamics, highlighting the need\nto understand these measures and the spread of rabies in wild jackals. In this\nstudy, we introduce a novel spatio-temporal extended-SIR\n(susceptible-infected-recovered) model with a graph-based spatial framework for\nevaluating mitigation efficiency. We implement the model in a case study using\na jackal population in northern Israel, and using spatial and movement data\ncollected by Advanced Tracking and Localization of Animals in real-life Systems\n(ATLAS) telemetry. An agent-based simulation approach allows us to explore\nvarious biologically-realistic scenarios, and assess the impact of different\nEIPs configurations. Our model suggests that under biologically-realistic\nunderlying assumptions and scenarios, the effectiveness of both EIPs is not\ninfluenced much by the jackal population size but is sensitive to their\ndispersal between activity centers.\n","authors":["Teddy Lazebnik","Yehuda Samuel","Jonathan Tichon","Roi Lapid","Roni King","Tomer Nissimian","Orr Spiegel"],"pdf_url":"https://arxiv.org/pdf/2501.15425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00326v7","updated":"2025-01-26T06:16:39Z","published":"2023-12-01T03:44:54Z","title":"Agent-OM: Leveraging LLM Agents for Ontology Matching","summary":"  Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.\n","authors":["Zhangcheng Qiang","Weiqing Wang","Kerry Taylor"],"pdf_url":"https://arxiv.org/pdf/2312.00326v7.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.15379v1","updated":"2025-01-26T03:29:18Z","published":"2025-01-26T03:29:18Z","title":"Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented\n  Representations","summary":"  Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative\nuser-interactive tool for applications in domains such as e-commerce and\neducation. Yet, current methodologies predominantly depend on finetuned\nMultimodal Large Language Models (MLLMs), which face two critical limitations:\n(1) Finetuning imposes prohibitive computational overhead and long-term\nmaintenance costs. (2) Finetuning narrows the pretrained knowledge distribution\nof MLLMs, reducing their adaptability to novel scenarios. These issues are\nexacerbated by the inherently dynamic nature of real-world I-TIR systems, where\nqueries and image databases evolve in complexity and diversity, often deviating\nfrom static training distributions. To overcome these constraints, we propose\nDiffusion Augmented Retrieval (DAR), a paradigm-shifting framework that\nbypasses MLLM finetuning entirely. DAR synergizes Large Language Model\n(LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis\nto create contextually enriched intermediate representations. This\ndual-modality approach deciphers nuanced user intent more holistically,\nenabling precise alignment between textual queries and visually relevant\nimages. Rigorous evaluations across four benchmarks reveal DAR's dual\nstrengths: (1) Matches state-of-the-art finetuned I-TIR models on\nstraightforward queries without task-specific training. (2) Scalable\nGeneralization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10\naccuracy) under multi-turn conversational complexity, demonstrating robustness\nto intricate, distributionally shifted interactions. By eliminating finetuning\ndependencies and leveraging generative-augmented representations, DAR\nestablishes a new trajectory for efficient, adaptive, and scalable cross-modal\nretrieval systems.\n","authors":["Zijun Long","Kangheng Liang","Gerardo Aragon-Camarasa","Richard Mccreadie","Paul Henderson"],"pdf_url":"https://arxiv.org/pdf/2501.15379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15378v1","updated":"2025-01-26T03:27:11Z","published":"2025-01-26T03:27:11Z","title":"How to Mitigate Information Loss in Knowledge Graphs for GraphRAG:\n  Leveraging Triple Context Restoration and Query-Driven Feedback","summary":"  Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently\npropelled significant advances in complex reasoning tasks, thanks to their\nbroad domain knowledge and contextual awareness. Unfortunately, current methods\noften assume KGs to be complete, which is impractical given the inherent\nlimitations of KG construction and the potential loss of contextual cues when\nconverting unstructured text into entity-relation triples. In response, this\npaper proposes the Triple Context Restoration and Query-driven Feedback\n(TCR-QF) framework, which reconstructs the textual context underlying each\ntriple to mitigate information loss, while dynamically refining the KG\nstructure by iteratively incorporating query-relevant missing knowledge.\nExperiments on five benchmark question-answering datasets substantiate the\neffectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%\nimprovement in Exact Match and a 15.5% improvement in F1 over its\nstate-of-the-art GraphRAG competitors.\n","authors":["Manzong Huang","Chenyang Bu","Yi He","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2501.15378v1.pdf","comment":null}],"Multi Media":[{"id":"http://arxiv.org/abs/2501.15508v1","updated":"2025-01-26T12:56:08Z","published":"2025-01-26T12:56:08Z","title":"Learning Complex Heterogeneous Multimodal Fake News via Social Latent\n  Network Inference","summary":"  With the diversification of online social platforms, news dissemination has\nbecome increasingly complex, heterogeneous, and multimodal, making the fake\nnews detection task more challenging and crucial. Previous works mainly focus\non obtaining social relationships of news via retweets, limiting the accurate\ndetection when real cascades are inaccessible. Given the proven assessment of\nthe spreading influence of events, this paper proposes a method called HML\n(Complex Heterogeneous Multimodal Fake News Detection method via Latent Network\nInference). Specifically, an improved social latent network inference strategy\nis designed to estimate the maximum likelihood of news influences under the\nsame event. Meanwhile, a novel heterogeneous graph is built based on social\nattributes for multimodal news under different events. Further, to better\naggregate the relationships among heterogeneous multimodal features, this paper\nproposes a self-supervised-based multimodal content learning strategy, to\nenhance, align, fuse and compare heterogeneous modal contents. Based above, a\npersonalized heterogeneous graph representation learning is designed to\nclassify fake news. Extensive experiments demonstrate that the proposed method\noutperforms the SOTA in real social media news datasets.\n","authors":["Mingxin Li","Yuchen Zhang","Haowei Xu","Xianghua Li","Chao Gao","Zhen Wang"],"pdf_url":"https://arxiv.org/pdf/2501.15508v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2501.15438v1","updated":"2025-01-26T07:50:14Z","published":"2025-01-26T07:50:14Z","title":"Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in\n  Hateful Video Detection","summary":"  Detecting hate speech in online content is essential to ensuring safer\ndigital spaces. While significant progress has been made in text and meme\nmodalities, video-based hate speech detection remains under-explored, hindered\nby a lack of annotated datasets and the high cost of video annotation. This gap\nis particularly problematic given the growing reliance on large models, which\ndemand substantial amounts of training data. To address this challenge, we\nleverage meme datasets as both a substitution and an augmentation strategy for\ntraining hateful video detection models. Our approach introduces a\nhuman-assisted reannotation pipeline to align meme dataset labels with video\ndatasets, ensuring consistency with minimal labeling effort. Using two\nstate-of-the-art vision-language models, we demonstrate that meme data can\nsubstitute for video data in resource-scarce scenarios and augment video\ndatasets to achieve further performance gains. Our results consistently\noutperform state-of-the-art benchmarks, showcasing the potential of cross-modal\ntransfer learning for advancing hateful video detection. Dataset and code are\navailable at https://github.com/Social-AI-Studio/CrossModalTransferLearning.\n","authors":["Han Wang","Rui Yang Tan","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2501.15438v1.pdf","comment":"10 pages, 4 figures, THE WEB CONFERENCE 2025"}]}}