<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-31T00:00:00Z">2025-01-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable-Softmax Is Superior for Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken M. Nakanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The maximum element of the vector output by the Softmax function approaches
zero as the input vector size increases. Transformer-based language models rely
on Softmax to compute attention scores, causing the attention distribution to
flatten as the context size grows. This reduces the model's ability to
prioritize key information effectively and potentially limits its length
generalization. To address this problem, we propose Scalable-Softmax (SSMax),
which replaces Softmax in scenarios where the input vector size varies. SSMax
can be seamlessly integrated into existing Transformer-based architectures.
Experimental results in language modeling show that models using SSMax not only
achieve faster loss reduction during pretraining but also significantly improve
performance in long contexts and key information retrieval. Furthermore, an
analysis of attention scores reveals that SSMax enables the model to focus
attention on key information even in long contexts. Additionally, although
models that use SSMax from the beginning of pretraining achieve better length
generalization, those that have already started pretraining can still gain some
of this ability by replacing Softmax in the attention layers with SSMax, either
during or after pretraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ s1: Simple test-time scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling is a promising new approach to language modeling that uses
extra test-time compute to improve performance. Recently, OpenAI's o1 model
showed this capability but did not publicly share its methodology, leading to
many replication efforts. We seek the simplest approach to achieve test-time
scaling and strong reasoning performance. First, we curate a small dataset s1K
of 1,000 questions paired with reasoning traces relying on three criteria we
validate through ablations: difficulty, diversity, and quality. Second, we
develop budget forcing to control test-time compute by forcefully terminating
the model's thinking process or lengthening it by appending "Wait" multiple
times to the model's generation when it tries to end. This can lead the model
to double-check its answer, often fixing incorrect reasoning steps. After
supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and
equipping it with budget forcing, our model s1 exceeds o1-preview on
competition math questions by up to 27% (MATH and AIME24). Further, scaling s1
with budget forcing allows extrapolating beyond its performance without
test-time intervention: from 50% to 57% on AIME24. Our model, data, and code
are open-source at https://github.com/simplescaling/s1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages (9 main), 10 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding-based Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyou Song, Dara Bahri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have recently been shown capable of performing regression
tasks wherein numeric predictions are represented as decoded strings. In this
work, we provide theoretical grounds for this capability and furthermore
investigate the utility of causal auto-regressive sequence models when they are
applied to any feature representation. We find that, despite being trained in
the usual way - for next-token prediction via cross-entropy loss -
decoding-based regression is as performant as traditional approaches for
tabular regression tasks, while being flexible enough to capture arbitrary
distributions, such as in the task of density estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Google DeepMind Technical Report, 25 pages. Code can be found at
  https://github.com/google-research/optformer/tree/main/optformer/decoding_regression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TableMaster: A Recipe to Advance Table Understanding with Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tables serve as a fundamental format for representing structured relational
data. While current language models (LMs) excel at many text-based tasks, they
still face challenges in table understanding due to the complex characteristics
of tabular data, such as their structured nature. In this paper, we aim to
enhance LMs for improved table understanding. We identify four key challenges:
1) difficulty in locating target data, 2) deficiency in table semantics, 3)
numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in
symbolic reasoning. To address these issues, we propose TableMaster, a recipe
and comprehensive framework that integrates multiple solutions to overcome
these obstacles. TableMaster first extracts relevant table content and
verbalizes it with enriched semantic context. Additionally, we introduce
adaptive reasoning, a flexible approach that dynamically adjusts between
textual and symbolic reasoning, tailoring the reasoning process to each query.
Extensive analyses and experiments demonstrate our findings and the
effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an
accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELMA: A Speech-Enabled Language Model for Virtual Assistant
  Interactions <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present and evaluate SELMA, a Speech-Enabled Language Model
for virtual Assistant interactions that integrates audio and text as inputs to
a Large Language Model (LLM). SELMA is designed to handle three primary and two
auxiliary tasks related to interactions with virtual assistants simultaneously
within a single end-to-end model. We employ low-rank adaptation modules for
parameter-efficient training of both the audio encoder and the LLM.
Additionally, we implement a feature pooling strategy enabling the system to
recognize global patterns and improve accuracy on tasks less reliant on
individual sequence elements. Experimental results on Voice Trigger (VT)
detection, Device-Directed Speech Detection (DDSD), and Automatic Speech
Recognition (ASR), demonstrate that our approach both simplifies the typical
input processing pipeline of virtual assistants significantly and also improves
performance compared to dedicated models for each individual task. SELMA yields
relative Equal-Error Rate improvements of 64% on the VT detection task, and 22%
on DDSD, while also achieving word error rates close to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ We're Different, We're the Same: Creative Homogeneity Across LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Wenger, Yoed Kenett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous powerful large language models (LLMs) are now available for use as
writing support tools, idea generators, and beyond. Although these LLMs are
marketed as helpful creative assistants, several works have shown that using an
LLM as a creative partner results in a narrower set of creative outputs.
However, these studies only consider the effects of interacting with a single
LLM, begging the question of whether such narrowed creativity stems from using
a particular LLM -- which arguably has a limited range of outputs -- or from
using LLMs in general as creative assistants. To study this question, we elicit
creative responses from humans and a broad set of LLMs using standardized
creativity tests and compare the population-level diversity of responses. We
find that LLM responses are much more similar to other LLM responses than human
responses are to each other, even after controlling for response structure and
other key variables. This finding of significant homogeneity in creative
outputs across the LLMs we evaluate adds a new dimension to the ongoing
conversation about creativity and LLMs. If today's LLMs behave similarly, using
them as a creative partners -- regardless of the model used -- may drive all
users towards a limited set of "creative" outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Multimodal Models Solve Caption <span class="highlight-title">Generation</span> for Scientific
  Figures? Lessons Learned from SCICAP Challenge 2023 <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the SCICAP datasets launch in 2021, the research community has made
significant progress in generating captions for scientific figures in scholarly
articles. In 2023, the first SCICAP Challenge took place, inviting global teams
to use an expanded SCICAP dataset to develop models for captioning diverse
figure types across various academic fields. At the same time, text generation
models advanced quickly, with many powerful pre-trained large multimodal models
(LMMs) emerging that showed impressive capabilities in various
vision-and-language tasks. This paper presents an overview of the first SCICAP
Challenge and details the performance of various models on its data, capturing
a snapshot of the fields state. We found that professional editors
overwhelmingly preferred figure captions generated by GPT-4V over those from
all other models and even the original captions written by authors. Following
this key finding, we conducted detailed analyses to answer this question: Have
advanced LMMs solved the task of generating captions for scientific figures?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PixelWorld: Towards Perceiving Everything as Pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Lyu, Xueguang Ma, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing foundation models typically process visual input as pixels and
textual input as tokens, a paradigm that contrasts with human perception, where
both modalities are processed in a unified manner. With the rise of embodied
and agentic AI, where inputs primarily come from camera pixels, the need for a
unified perception framework becomes increasingly evident. In this paper, we
propose to unify all modalities (text, tables, code, diagrams, images, etc) as
pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce
PixelWorld, a novel evaluation suite that unifies all the mentioned modalities
into pixel space to gauge the existing models' performance. Our findings show
that (1) PEAP outperforms baseline with token-based input in multimodal
datasets, benefiting from unified input for better disambiguation, (2)
significant declines in reasoning and coding capabilities across all models
when processing pixel-based input, underscoring the need to enhance foundation
models' perceptual abilities, (3) larger models can maintain strong performance
on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer
significant performance degradation, (4) the attention pattern of PEAP is
highly aligned with text token input, (5) PEAP can be accelerated significantly
by exploiting the spatial sparsity. We conclude that the existing frontier
models are competent in pixel perception, however, there is still headroom for
improvement. Our code, dataset will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneity Bias as Differential Sampling Uncertainty in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Messi H. J. Lee, Soyeon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research show that Large Language Models (LLMs) and Vision-Language
Models (VLMs) represent marginalized groups more homogeneously than dominant
groups. However, the mechanisms underlying this homogeneity bias remain
relatively unexplored. We propose that this bias emerges from systematic
differences in the probability distributions from which tokens are sampled at
inference-time. Analyzing three measures of uncertainty in token sampling
distributions-entropy, perplexity, and probability of differentiation-we find
that in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled
more deterministically when generating texts about marginalized groups (i.e.,
Black Americans and women) compared to their dominant group counterparts (i.e.,
White Americans and men). While these findings may help explain homogeneity
bias in certain models, the patterns did not replicate across all VLMs tested,
suggesting multiple mechanisms may contribute to homogeneity bias in AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Guided Speculative Decoding for Efficient LLM <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Reward-Guided Speculative Decoding (RSD), a novel framework
aimed at improving the efficiency of inference in large language models (LLMs).
RSD synergistically combines a lightweight draft model with a more powerful
target model, incorporating a controlled bias to prioritize high-reward
outputs, in contrast to existing speculative decoding methods that enforce
strict unbiasedness. RSD employs a process reward model to evaluate
intermediate decoding steps and dynamically decide whether to invoke the target
model, optimizing the trade-off between computational cost and output quality.
We theoretically demonstrate that a threshold-based mixture strategy achieves
an optimal balance between resource utilization and performance. Extensive
evaluations on challenging reasoning benchmarks, including Olympiad-level
tasks, show that RSD delivers significant efficiency gains against decoding
with the target model only (up to 4.4x fewer FLOPs), while achieving
significant better accuracy than parallel decoding method on average (up to
+3.5). These results highlight RSD as a robust and cost-effective approach for
deploying LLMs in resource-intensive scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Bias in <span class="highlight-title">Self-Supervised</span> Learning For Automatic Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Storey, Naomi Harte, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is used in deep learning to train on large
datasets without the need for expensive labelling of the data. Recently, large
Automatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to
train on over one hundred different languages simultaneously. However, deeper
investigation shows that the bulk of the training data for XLS-R comes from a
small number of languages. Biases learned through SSL have been shown to exist
in multiple domains, but language bias in multilingual SSL ASR has not been
thoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis
(LTH) to identify language-specific subnetworks within XLS-R and test the
performance of these subnetworks on a variety of different languages. We are
able to show that when fine-tuning, XLS-R bypasses traditional linguistic
knowledge and builds only on weights learned from the languages with the
largest data contribution to the pretraining data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Speech and Language Technology Workshop (SLT) 2024
  accessible on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Affective Text <span class="highlight-title">Generation</span> Quality Based on Different
  Quantization Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yarik Menchaca Resendiz, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models exhibit a remarkable capacity in language generation
and comprehension. These advances enable AI systems to produce more human-like
and emotionally engaging text. However, these models rely on a large number of
parameters, requiring significant computational resources for training and
inference. In some scenarios, accessing these resources can be challenging
(e.g., budget or hardware limitations). Techniques like reducing precision bits
can make models more memory-efficient, reducing the computational resources
needed, at the cost of reduced accuracy. This paper addresses the trade-off
between different quantization values, GPU RAM utilization, and text quality in
affective text generation (e.g., "I really enjoy running in the snow-covered
forest"). To evaluate, we use an emotion classifier and ten seed prompts to
generate affective text. We test three setups of precision bits (8, 16, and 32)
across five open-weight language models from two different families. Our
findings demonstrate that bit reductions lead to memory savings, achieving a
reduction of 76%. However, this optimization comes with a trade-off, leading to
a decrease of up to 10 pp in F1 score for larger models and an increase of 10
pp for smaller models, along with roughly double the inference time. In terms
of text quality, larger models at lower quantization levels generally
outperform smaller, higher-precision models -- while requiring similar memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Probing: Evaluating <span class="highlight-title">Knowledge</span> Transfer via Finetuned Task
  Embeddings for Coreference Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatiana Anikina, Arne Binder, David Harbecke, Stalin Varanasi, Leonhard Hennig, Simon Ostermann, Sebastian Möller, Josef van Genabith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we reimagine classical probing to evaluate knowledge transfer
from simple source to more complex target tasks. Instead of probing frozen
representations from a complex source task on diverse simple target probing
tasks (as usually done in probing), we explore the effectiveness of embeddings
from multiple simple source tasks on a single target task. We select
coreference resolution, a linguistically complex problem requiring contextual
understanding, as focus target task, and test the usefulness of embeddings from
comparably simpler tasks tasks such as paraphrase detection, named entity
recognition, and relation extraction. Through systematic experiments, we
evaluate the impact of individual and combined task embeddings.
  Our findings reveal that task embeddings vary significantly in utility for
coreference resolution, with semantic similarity tasks (e.g., paraphrase
detection) proving most beneficial. Additionally, representations from
intermediate layers of fine-tuned models often outperform those from final
layers. Combining embeddings from multiple tasks consistently improves
performance, with attention-based aggregation yielding substantial gains. These
insights shed light on relationships between task-specific representations and
their adaptability to complex downstream tasks, encouraging further exploration
of embedding-level task transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Approach for Machine Translation on <span class="highlight-title">Low-resource</span> Languages:
  A Case Study in Vietnamese-Chinese <span class="chip">SP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tran Ngoc Son, Nguyen Anh Tu, Nguyen Minh Tri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rise of recent neural networks in machine translation, those
networks do not work well if the training data is insufficient. In this paper,
we proposed an approach for machine translation in low-resource languages such
as Vietnamese-Chinese. Our proposed method leveraged the power of the
multilingual pre-trained language model (mBART) and both Vietnamese and Chinese
monolingual corpus. Firstly, we built an early bird machine translation model
using the bilingual training dataset. Secondly, we used TF-IDF technique to
select sentences from the monolingual corpus which are the most related to
domains of the parallel dataset. Finally, the first model was used to
synthesize the augmented training data from the selected monolingual corpus for
the translation model. Our proposed scheme showed that it outperformed 8%
compared to the transformer model. The augmented dataset also pushed the model
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report of VLSP 2022 NMT; The first two authors contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, Jonas Kohler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) is closely linked to their
underlying size, leading to ever-growing networks and hence slower inference.
Speculative decoding has been proposed as a technique to accelerate
autoregressive generation, leveraging a fast draft model to propose candidate
tokens, which are then verified in parallel based on their likelihood under the
target model. While this approach guarantees to reproduce the target output, it
incurs a substantial penalty: many high-quality draft tokens are rejected, even
when they represent objectively valid continuations. Indeed, we show that even
powerful draft models such as GPT-4o, as well as human text cannot achieve high
acceptance rates under the standard verification scheme. This severely limits
the speedup potential of current speculative decoding methods, as an early
rejection becomes overwhelmingly likely when solely relying on alignment of
draft and target.
  We thus ask the following question: Can we adapt verification to recognize
correct, but non-aligned replies? To this end, we draw inspiration from the
LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers
in a versatile way. We carefully design a dataset to elicit the same capability
in the target model by training a compact module on top of the embeddings to
produce ``judgements" of the current continuation. We showcase our strategy on
the Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over
Llama-405B, while maintaining its quality on a large range of benchmarks. These
benefits remain present even in optimized inference frameworks, where our
method reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B
on 2 and 8 H100s respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SETS: Leveraging Self-Verification and Self-Correction for Improved
  Test-Time Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ö Arık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have created new
opportunities to enhance performance on complex reasoning tasks by leveraging
test-time computation. However, conventional approaches such as repeated
sampling with majority voting or reward model scoring, often face diminishing
returns as test-time compute scales, in addition to requiring costly
task-specific reward model training. In this paper, we present Self-Enhanced
Test-Time Scaling (SETS), a novel method that leverages the self-verification
and self-correction capabilities of recent advanced LLMs to overcome these
limitations. SETS integrates sampling, self-verification, and self-correction
into a unified framework, enabling efficient and scalable test-time computation
for improved capabilities at complex tasks. Through extensive experiments on
challenging planning and reasoning benchmarks, compared to the alternatives, we
demonstrate that SETS achieves significant performance improvements and more
favorable test-time scaling laws.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond checkmate: exploring the creative chokepoints in AI text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.
This rapid advancement has spurred research into various aspects of LLMs, their
text generation & reasoning capability, and potential misuse, fueling the
necessity for robust detection methods. While numerous prior research has
focused on detecting LLM-generated text (AI text) and thus checkmating them,
our study investigates a relatively unexplored territory: portraying the
nuanced distinctions between human and AI texts across text segments. Whether
LLMs struggle with or excel at incorporating linguistic ingenuity across
different text segments carries substantial implications for determining their
potential as effective creative assistants to humans. Through an analogy with
the structure of chess games-comprising opening, middle, and end games-we
analyze text segments (introduction, body, and conclusion) to determine where
the most significant distinctions between human and AI texts exist. While AI
texts can approximate the body segment better due to its increased length, a
closer examination reveals a pronounced disparity, highlighting the importance
of this segment in AI text detection. Additionally, human texts exhibit higher
cross-segment differences compared to AI texts. Overall, our research can shed
light on the intricacies of human-AI text distinctions, offering novel insights
for text detection and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, single columns, under review at Nature Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pheromone-based Learning of Optimal <span class="highlight-title">Reasoning</span> Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Chari, Aditya Tiwari, Richard Lian, Suraj Reddy, Brian Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities through chain-of-thought prompting, yet discovering effective
reasoning methods for complex problems remains challenging due to the vast
space of possible intermediate steps. We introduce Ant Colony
Optimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines
ACO with LLMs to discover optimal reasoning paths for complex problems
efficiently. Drawing inspiration from Hebbian learning in neurological systems,
our method employs a collection of distinctly fine-tuned LLM "ants" to traverse
and lay pheromone trails through a centralized tree of thought, with each ant's
movement governed by a weighted combination of existing pheromone trails and
its own specialized expertise. The algorithm evaluates complete reasoning paths
using a mixture-of-experts-based scoring function, with pheromones reinforcing
productive reasoning paths across iterations. Experiments on three challenging
reasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT
performs significantly better than existing chain-of-thought optimization
approaches, suggesting that incorporating biologically inspired collective
search mechanisms into LLM inference can substantially enhance reasoning
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mFollowIR: a Multilingual Benchmark for Instruction Following in
  Retrieval <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Sam Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, Dawn Lawrie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval systems generally focus on web-style queries that are short and
underspecified. However, advances in language models have facilitated the
nascent rise of retrieval models that can understand more complex queries with
diverse intents. However, these efforts have focused exclusively on English;
therefore, we do not yet understand how they work across languages. We
introduce mFollowIR, a multilingual benchmark for measuring
instruction-following ability in retrieval models. mFollowIR builds upon the
TREC NeuCLIR narratives (or instructions) that span three diverse languages
(Russian, Chinese, Persian) giving both query and instruction to the retrieval
models. We make small changes to the narratives and isolate how well retrieval
models can follow these nuanced changes. We present results for both
multilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong
cross-lingual performance with English-based retrievers that trained using
instructions, but find a notable drop in performance in the multilingual
setting, indicating that more work is needed in developing data for
instruction-based multilingual retrievers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisualSpeech: Enhance Prosody with Visual Context in TTS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumin Que, Anton Ragni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Speech (TTS) synthesis faces the inherent challenge of producing
multiple speech outputs with varying prosody from a single text input. While
previous research has addressed this by predicting prosodic information from
both text and speech, additional contextual information, such as visual
features, remains underutilized. This paper investigates the potential of
integrating visual context to enhance prosody prediction. We propose a novel
model, VisualSpeech, which incorporates both visual and textual information for
improved prosody generation. Empirical results demonstrate that visual features
provide valuable prosodic cues beyond the textual input, significantly
enhancing the naturalness and accuracy of the synthesized speech. Audio samples
are available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Robustness of Representation Misdirection for Large
  Language Model Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation Misdirection (RM) and variants are established large language
model (LLM) unlearning methods with state-of-the-art performance. In this
paper, we show that RM methods inherently reduce models' robustness, causing
them to misbehave even when a single non-adversarial forget-token is in the
retain-query. Toward understanding underlying causes, we reframe the unlearning
process as backdoor attacks and defenses: forget-tokens act as backdoor
triggers that, when activated in retain-queries, cause disruptions in RM
models' behaviors, similar to successful backdoor attacks. To mitigate this
vulnerability, we propose Random Noise Augmentation -- a model and method
agnostic approach with theoretical guarantees for improving the robustness of
RM methods. Extensive experiments demonstrate that RNA significantly improves
the robustness of RM models while enhancing the unlearning performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Reasoning</span> with Hidden Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning has become a powerful framework for
improving complex problem-solving capabilities in Multimodal Large Language
Models (MLLMs). However, the verbose nature of textual reasoning introduces
significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as
hidden llama), an efficient reasoning framework that leverages reasoning CoTs
at hidden latent space. We design the Heima Encoder to condense each
intermediate CoT into a compact, higher-level hidden representation using a
single thinking token, effectively minimizing verbosity and reducing the
overall number of tokens required during the reasoning process. Meanwhile, we
design corresponding Heima Decoder with traditional Large Language Models
(LLMs) to adaptively interpret the hidden representations into variable-length
textual sequence, reconstructing reasoning processes that closely resemble the
original CoTs. Experimental results across diverse reasoning MLLM benchmarks
demonstrate that Heima model achieves higher generation efficiency while
maintaining or even better zero-shot task accuracy. Moreover, the effective
reconstruction of multimodal reasoning processes with Heima Decoder validates
both the robustness and interpretability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed Feelings: Cross-Domain Sentiment Classification of Patient
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egil Rønningstad, Lilja Charlotte Storset, Petter Mæhlum, Lilja Øvrelid, Erik Velldal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis of patient feedback from the public health domain can aid
decision makers in evaluating the provided services. The current paper focuses
on free-text comments in patient surveys about general practitioners and
psychiatric healthcare, annotated with four sentence-level polarity classes --
positive, negative, mixed and neutral -- while also attempting to alleviate
data scarcity by leveraging general-domain sources in the form of reviews. For
several different architectures, we compare in-domain and out-of-domain
effects, as well as the effects of training joint multi-domain models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for NoDaLiDa / Baltic-HLT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Low-Resource</span> Sequence Labeling with <span class="highlight-title">Knowledge</span> Fusion and
  Contextual Label Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peichao Lai, Jiaxin Gan, Feiyang Ye, Yilei Wang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence labeling remains a significant challenge in low-resource,
domain-specific scenarios, particularly for character-dense languages like
Chinese. Existing methods primarily focus on enhancing model comprehension and
improving data diversity to boost performance. However, these approaches still
struggle with inadequate model applicability and semantic distribution biases
in domain-specific contexts. To overcome these limitations, we propose a novel
framework that combines an LLM-based knowledge enhancement workflow with a
span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model.
Our workflow employs explanation prompts to generate precise contextual
interpretations of target entities, effectively mitigating semantic biases and
enriching the model's contextual understanding. The KnowFREE model further
integrates extension label features, enabling efficient nested entity
extraction without relying on external knowledge during inference. Experiments
on multiple Chinese domain-specific sequence labeling datasets demonstrate that
our approach achieves state-of-the-art performance, effectively addressing the
challenges posed by low-resource settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Autonomic Microservice Management through Self-Learning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of modern software systems necessitates robust
autonomic self-management capabilities. While Large Language Models (LLMs)
demonstrate potential in this domain, they often face challenges in adapting
their general knowledge to specific service contexts. To address this
limitation, we propose ServiceOdyssey, a self-learning agent system that
autonomously manages microservices without requiring prior knowledge of
service-specific configurations. By leveraging curriculum learning principles
and iterative exploration, ServiceOdyssey progressively develops a deep
understanding of operational environments, reducing dependence on human input
or static documentation. A prototype built with the Sock Shop microservice
demonstrates the potential of this approach for autonomic microservice
management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Impact of Noise in Differentially Private Text Rewriting <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Meisenbacher, Maulik Chevli, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text privatization often leverages the notion of
$\textit{Differential Privacy}$ (DP) to provide formal guarantees in the
rewriting or obfuscation of sensitive textual data. A common and nearly
ubiquitous form of DP application necessitates the addition of calibrated noise
to vector representations of text, either at the data- or model-level, which is
governed by the privacy parameter $\varepsilon$. However, noise addition almost
undoubtedly leads to considerable utility loss, thereby highlighting one major
drawback of DP in NLP. In this work, we introduce a new sentence infilling
privatization technique, and we use this method to explore the effect of noise
in DP text rewriting. We empirically demonstrate that non-DP privatization
techniques excel in utility preservation and can find an acceptable empirical
privacy-utility trade-off, yet cannot outperform DP methods in empirical
privacy protections. Our results highlight the significant impact of noise in
current DP rewriting mechanisms, leading to a discussion of the merits and
challenges of DP in NLP, as well as the opportunities that non-DP methods
present.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 9 tables. Accepted to NAACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Multi-phase Word Embedding Using Conjunctive Propositional
  Clauses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo, Bimal Bhattarai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness
in Machine Learning (ML), particularly within Natural Language Processing
(NLP). It has been utilized to construct word embedding using conjunctive
propositional clauses, thereby significantly enhancing our understanding and
interpretation of machine-derived decisions. The previous approach performed
the word embedding over a sequence of input words to consolidate the
information into a cohesive and unified representation. However, that approach
encounters scalability challenges as the input size increases. In this study,
we introduce a novel approach incorporating two-phase training to discover
contextual embeddings of input sequences. Specifically, this method
encapsulates the knowledge for each input word within the dataset's vocabulary,
subsequently constructing embeddings for a sequence of input words utilizing
the extracted knowledge. This technique not only facilitates the design of a
scalable model but also preserves interpretability. Our experimental findings
revealed that the proposed method yields competitive performance compared to
the previous approaches, demonstrating promising results in contrast to
human-generated benchmarks. Furthermore, we applied the proposed approach to
sentiment analysis on the IMDB dataset, where the TM embedding and the TM
classifier, along with other interpretable classifiers, offered a transparent
end-to-end solution with competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calling a Spade a Heart: Gaslighting Multimodal Large Language Models
  via Negation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhu, Hui yan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, Ee Peng Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have exhibited remarkable
advancements in integrating different modalities, excelling in complex
understanding and generation tasks. Despite their success, MLLMs remain
vulnerable to conversational adversarial inputs, particularly negation
arguments. This paper systematically evaluates state-of-the-art MLLMs across
diverse benchmarks, revealing significant performance drops when negation
arguments are introduced to initially correct responses. We show critical
vulnerabilities in the reasoning and alignment mechanisms of these models.
Proprietary models such as GPT-4o and Claude-3.5-Sonnet demonstrate better
resilience compared to open-source counterparts like Qwen2-VL and LLaVA.
However, all evaluated MLLMs struggle to maintain logical consistency under
negation arguments during conversation. This paper aims to offer valuable
insights for improving the robustness of MLLMs against adversarial inputs,
contributing to the development of more reliable and trustworthy multimodal AI
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Importing Phantoms: Measuring LLM Package <span class="highlight-title">Hallucination</span> Vulnerabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Krishna, Erick Galinkin, Leon Derczynski, Jeffrey Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become an essential tool in the
programmer's toolkit, but their tendency to hallucinate code can be used by
malicious actors to introduce vulnerabilities to broad swathes of the software
supply chain. In this work, we analyze package hallucination behaviour in LLMs
across popular programming languages examining both existing package references
and fictional dependencies. By analyzing this package hallucination behaviour
we find potential attacks and suggest defensive strategies to defend against
these attacks. We discover that package hallucination rate is predicated not
only on model choice, but also programming language, model size, and
specificity of the coding task request. The Pareto optimality boundary between
code generation performance and package hallucination is sparsely populated,
suggesting that coding models are not being optimized for secure code.
Additionally, we find an inverse correlation between package hallucination rate
and the HumanEval coding benchmark, offering a heuristic for evaluating the
propensity of a model to hallucinate packages. Our metrics, findings and
analyses provide a base for future models, securing AI-assisted software
development workflows against package supply chain attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech
  Recognition <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Lee, Solee Im, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysarthric speech recognition often suffers from performance degradation due
to the intrinsic diversity of dysarthric severity and extrinsic disparity from
normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level
Contrastive Learning (DyPCL) method, which leads to obtaining invariant
representations across diverse speakers. We decompose the speech utterance into
phoneme segments for phoneme-level contrastive learning, leveraging dynamic
connectionist temporal classification alignment. Unlike prior studies focusing
on utterance-level embeddings, our granular learning allows discrimination of
subtle parts of speech. In addition, we introduce dynamic curriculum learning,
which progressively transitions from easy negative samples to
difficult-to-distinguishable negative samples based on phonetic similarity of
phoneme. Our approach to training by difficulty levels alleviates the inherent
variability of speakers, better identifying challenging speeches. Evaluated on
the UASpeech dataset, DyPCL outperforms baseline models, achieving an average
22.10\% relative reduction in word error rate (WER) across the overall
dysarthria group.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025, 9pages, 1 page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks on AI-<span class="highlight-title">Generate</span>d Text Detection Models: A Token
  Probability-Based Approach Using Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, text generation tools utilizing Artificial Intelligence (AI)
have occasionally been misused across various domains, such as generating
student reports or creative writings. This issue prompts plagiarism detection
services to enhance their capabilities in identifying AI-generated content.
Adversarial attacks are often used to test the robustness of AI-text generated
detectors. This work proposes a novel textual adversarial attack on the
detection models such as Fast-DetectGPT. The method employs embedding models
for data perturbation, aiming at reconstructing the AI generated texts to
reduce the likelihood of detection of the true origin of the texts.
Specifically, we employ different embedding techniques, including the Tsetlin
Machine (TM), an interpretable approach in machine learning for this purpose.
By combining synonyms and embedding similarity vectors, we demonstrates the
state-of-the-art reduction in detection scores against Fast-DetectGPT.
Particularly, in the XSum dataset, the detection score decreased from 0.4431 to
0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Tensor Field Propagation in Large Language Models: A Novel
  Approach to Contextual Information Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfred Bexley, Lukas Radcliffe, Giles Weatherstone, Joseph Sakau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context propagation remains a central challenge in language model
architectures, particularly in tasks requiring the retention of long-range
dependencies. Conventional attention mechanisms, while effective in many
applications, exhibit limitations in maintaining coherent contextual
representations over extended sequences due to their reliance on discrete token
interactions. A novel approach is introduced through the formulation of
Intrinsic Tensor Field Propagation (ITFP), which models contextual
relationships as continuous tensor fields distributed across token embeddings.
The propagation dynamics are governed through differential equations that
enable a structured flow of contextual information, augmenting the standard
attention mechanism to enhance coherence and recall. A series of experiments
conducted on an open-source transformer-based model demonstrate that ITFP
provides measurable improvements in contextual retention, dependency
resolution, and inference stability across various linguistic structures.
Comparisons with baseline models reveal a reduction in syntactic
inconsistencies and factual errors, while ablation studies indicate that the
choice of propagation depth and integration strength significantly impacts
model performance. Additional evaluations assessing domain generalization
suggest that ITFP effectively adapts across different text genres, reinforcing
its applicability beyond conventional language modeling tasks. Although
computational trade-offs are introduced through the inclusion of tensor field
computations, empirical findings suggest that the benefits in accuracy and
coherence outweigh the increased processing demands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Games as the Pathway to Artificial Superhuman Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Wen, Ziyu Wan, Shao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of large language models (LLMs) toward artificial superhuman
intelligence (ASI) hinges on data reproduction, a cyclical process in which
models generate, curate and retrain on novel data to refine capabilities.
Current methods, however, risk getting stuck in a data reproduction trap:
optimizing outputs within fixed human-generated distributions in a closed loop
leads to stagnation, as models merely recombine existing knowledge rather than
explore new frontiers. In this paper, we propose language games as a pathway to
expanded data reproduction, breaking this cycle through three mechanisms: (1)
\textit{role fluidity}, which enhances data diversity and coverage by enabling
multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward
variety}, embedding multiple feedback criteria that can drive complex
intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving
interaction constraints to foster learnability, thereby injecting continual
novelty. By scaling language games into global sociotechnical ecosystems,
human-AI co-evolution generates unbounded data streams that drive open-ended
exploration. This framework redefines data reproduction not as a closed loop
but as an engine for superhuman intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This position paper argues that language games provide robust
  mechanism for achieving superhuman intelligence in large language models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KBQA-o1: Agentic <span class="highlight-title">Knowledge</span> Base Question Answering with Monte Carlo Tree
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with a large-scale structured knowledge base (KB). Despite
advancements with large language models (LLMs), KBQA still faces challenges in
weak KB awareness, imbalance between effectiveness and efficiency, and high
reliance on annotated data. To address these challenges, we propose KBQA-o1, a
novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a
ReAct-based agent process for stepwise logical form generation with KB
environment exploration. Moreover, it employs MCTS, a heuristic search method
driven by policy and reward models, to balance agentic exploration's
performance and search space. With heuristic exploration, KBQA-o1 generates
high-quality annotations for further improvement by incremental fine-tuning.
Experimental results show that KBQA-o1 outperforms previous low-resource KBQA
methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1
performance to 78.5% compared to 48.5% of the previous sota method with
GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Supernet Training with Orthogonal Softmax for Scalable ASR
  Model Compression <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xu, Eugen Beck, Zijian Yang, Ralf Schlüter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ASR systems are deployed across diverse environments, each with specific
hardware constraints. We use supernet training to jointly train multiple
encoders of varying sizes, enabling dynamic model size adjustment to fit
hardware constraints without redundant training. Moreover, we introduce a novel
method called OrthoSoftmax, which applies multiple orthogonal softmax functions
to efficiently identify optimal subnets within the supernet, avoiding
resource-intensive search. This approach also enables more flexible and precise
subnet selection by allowing selection based on various criteria and levels of
granularity. Our results with CTC on Librispeech and TED-LIUM-v2 show that
FLOPs-aware component-wise selection achieves the best overall performance.
With the same number of training updates from one single job, WERs for all
model sizes are comparable to or slightly better than those of individually
trained models. Furthermore, we analyze patterns in the selected components and
reveal interesting insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language
  Model <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, Liwei Wang, Mingyi Hong, Zhaoran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex reasoning tasks, yet generating reliable reasoning processes remains a
significant challenge. We present a unified probabilistic framework that
formalizes LLM reasoning through a novel graphical model incorporating latent
thinking processes and evaluation signals. Within this framework, we introduce
the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in
two steps. First, it generates high-quality rationales by approximating the
optimal thinking process through reinforcement learning, using a novel reward
shaping mechanism. Second, it enhances the base LLM by maximizing the joint
probability of rationale generation with respect to the model's parameters.
Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$
representing the number of iterations. Empirical evaluations on math and coding
benchmarks demonstrate that our approach consistently improves performance
across different base models without requiring human-annotated thinking
processes. In addition, BRiTE demonstrates superior performance compared to
existing algorithms that bootstrap thinking processes use alternative methods
such as rejection sampling, and can even match or exceed the results achieved
through supervised fine-tuning with human-annotated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Data Augmentation for Large Language Models: A Comprehensive <span class="highlight-title">Survey</span>
  of Methods, Challenges, and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaping Chai, Haoran Xie, Joe S. Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing size and complexity of pre-trained language models have
demonstrated superior performance in many applications, but they usually
require large training datasets to be adequately trained. Insufficient training
sets could unexpectedly make the model overfit and fail to cope with complex
tasks. Large language models (LLMs) trained on extensive corpora have prominent
text generation capabilities, which improve the quality and quantity of data
and play a crucial role in data augmentation. Specifically, distinctive prompt
templates are given in personalised tasks to guide LLMs in generating the
required content. Recent promising retrieval-based techniques further improve
the expressive performance of LLMs in data augmentation by introducing external
knowledge to enable them to produce more grounded-truth data. This survey
provides an in-depth analysis of data augmentation in LLMs, classifying the
techniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based
Augmentation and Hybrid Augmentation. We summarise the post-processing
approaches in data augmentation, which contributes significantly to refining
the augmented data and enabling the model to filter out unfaithful content.
Then, we provide the common tasks and evaluation metrics. Finally, we introduce
existing challenges and future opportunities that could bring further
improvement to data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partially Rewriting a <span class="highlight-title">Transformer</span> in Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Paulo, Nora Belrose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The greatest ambition of mechanistic interpretability is to completely
rewrite deep neural networks in a format that is more amenable to human
understanding, while preserving their behavior and performance. In this paper,
we attempt to partially rewrite a large language model using simple natural
language explanations. We first approximate one of the feedforward networks in
the LLM with a wider MLP with sparsely activating neurons - a transcoder - and
use an automated interpretability pipeline to generate explanations for these
neurons. We then replace the first layer of this sparse MLP with an LLM-based
simulator, which predicts the activation of each neuron given its explanation
and the surrounding context. Finally, we measure the degree to which these
modifications distort the model's final output. With our pipeline, the model's
increase in loss is statistically similar to entirely replacing the sparse MLP
output with the zero vector. We employ the same protocol, this time using a
sparse autoencoder, on the residual stream of the same layer and obtain similar
results. These results suggest that more detailed explanations are needed to
improve performance substantially above the zero ablation baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constitutional Classifiers: Defending against Universal Jailbreaks
  across Thousands of Hours of Red Teaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are vulnerable to universal jailbreaks-prompting
strategies that systematically bypass model safeguards and enable users to
carry out harmful processes that require many model interactions, like
manufacturing illegal substances at scale. To defend against these attacks, we
introduce Constitutional Classifiers: safeguards trained on synthetic data,
generated by prompting LLMs with natural language rules (i.e., a constitution)
specifying permitted and restricted content. In over 3,000 estimated hours of
red teaming, no red teamer found a universal jailbreak that could extract
information from an early classifier-guarded LLM at a similar level of detail
to an unguarded model across most target queries. On automated evaluations,
enhanced classifiers demonstrated robust defense against held-out
domain-specific jailbreaks. These classifiers also maintain deployment
viability, with an absolute 0.38% increase in production-traffic refusals and a
23.7% inference overhead. Our work demonstrates that defending against
universal jailbreaks while maintaining practical deployment viability is
tractable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural Embedding Projection for Contextual Large Language Model
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Enoasmo, Cedric Featherstonehaugh, Xavier Konstantinopoulos, Zacharias Huntington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured embedding transformations offer a promising approach for enhancing
the efficiency and coherence of language model inference. The introduction of
Structural Embedding Projection (SEP) provides a mechanism for refining token
representations through projection matrices that integrate hierarchical and
relational dependencies. The mathematical formulation of SEP enables embedding
spaces to capture structured contextual relationships, thereby improving
semantic fidelity without significantly increasing computational overhead.
Experimental evaluations conducted on a range of linguistic datasets revealed
that SEP contributed to reductions in perplexity and enhanced contextual
coherence, demonstrating its potential to refine language model outputs.
Computational efficiency assessments highlighted variations across different
datasets, suggesting that the integration of structured embeddings introduced
dataset-dependent trade-offs between inference speed and representational
richness. The qualitative analysis of generated responses indicated that SEP
enhanced narrative consistency and topic alignment, leading to improved fluency
in multi-sentence text generation. The modifications to embedding layers
required precise optimization to ensure stable training dynamics, as the
introduction of structured transformations altered the traditional
representation-learning process. The architectural adjustments necessary for
SEP implementation influenced inference latency and memory consumption,
requiring a balance between efficiency gains and additional processing demands.
The impact of SEP on lexical diversity suggested that embedding modifications
influenced the model's vocabulary usage, reflecting a more context-aware
selection of generated tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-Efficient Fine-Tuning of <span class="highlight-title">Transformer</span>s via Token Selection <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Simoulin, Namyong Park, Xiaoyi Liu, Grey Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning provides an effective means to specialize pre-trained models for
various downstream tasks. However, fine-tuning often incurs high memory
overhead, especially for large transformer-based models, such as LLMs. While
existing methods may reduce certain parts of the memory required for
fine-tuning, they still require caching all intermediate activations computed
in the forward pass to update weights during the backward pass. In this work,
we develop TokenTune, a method to reduce memory usage, specifically the memory
to store intermediate activations, in the fine-tuning of transformer-based
models. During the backward pass, TokenTune approximates the gradient
computation by backpropagating through just a subset of input tokens. Thus,
with TokenTune, only a subset of intermediate activations are cached during the
forward pass. Also, TokenTune can be easily combined with existing methods like
LoRA, further reducing the memory cost. We evaluate our approach on pre-trained
transformer models with up to billions of parameters, considering the
performance on multiple downstream tasks such as text classification and
question answering in a few-shot learning setup. Overall, TokenTune achieves
performance on par with full fine-tuning or representative memory-efficient
fine-tuning methods, while greatly reducing the memory footprint, especially
when combined with other methods with complementary memory reduction
mechanisms. We hope that our approach will facilitate the fine-tuning of large
transformers, in specializing them for specific domains or co-training them
with other neural components from a larger system. Our code is available at
https://github.com/facebookresearch/tokentune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the <span class="highlight-title">Reasoning</span> Gap: Small LLMs Can Plan with Generalised
  Strategies <span class="chip">IJCAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Borro, Patricia J Riddle, Michael W Barley, Michael J Witbrock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the reasoning skills of Large Language Models (LLMs)
demonstrate an increase in the ability of LLMs to solve simple planning tasks.
However, as long as the driving force behind improved reasoning capability is
the size and complexity of the model, the financial and computational costs
associated with running them will also increase. This trend raises questions
about continued accessibility and whether these improvements will increase at
the same pace as models continue to grow in size and expense. We propose two
approaches to enhance the reasoning ability of less resource-intensive LLMs.
(1) Provide them with a generalised strategy for solving tasks within a given
domain, generated by a more resource-intensive LLM. (2) Exploit their
cost-effectiveness by iteratively prompting these models to correct errors in
their proposed solutions. Our empirical results from planning and mathematical
reasoning tasks demonstrate that these methods improve the performance of less
resource-intensive LLMs to levels comparable with their more resource-intensive
counterparts, at a fraction of the cost. Additionally, we show that the
utilisation of generalised strategies in our experiments reduced the cost of
the less resource-intensive model by nearly 30 percent on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 page body, 2 page references, 16 page appendix (25 pages total); 2
  figures; submitted to IJCAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Common-Sense Heuristics <span class="chip">IJCAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Borro, Patricia J Riddle, Michael W Barley, Michael J Witbrock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While systems designed for solving planning tasks vastly outperform Large
Language Models (LLMs) in this domain, they usually discard the rich semantic
information embedded within task descriptions. In contrast, LLMs possess
parametrised knowledge across a wide range of topics, enabling them to leverage
the natural language descriptions of planning tasks in their solutions.
However, current research in this direction faces challenges in generating
correct and executable plans. Furthermore, these approaches depend on the LLM
to output solutions in an intermediate language, which must be translated into
the representation language of the planning task. We introduce a novel planning
method, which leverages the parametrised knowledge of LLMs by using their
output as a heuristic for Hill-Climbing Search. This approach is further
enhanced by prompting the LLM to generate a solution estimate to guide the
search. Our method outperforms the task success rate of similar systems within
a common household environment by 22 percentage points, with consistently
executable plans. All actions are encoded in their original representation,
demonstrating that strong results can be achieved without an intermediate
language, thus eliminating the need for a translation step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 page body, 2 page references, 5 page appendix (14 page total); 1
  figure; Submitted to IJCAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are In-Context Bandit Reinforcement Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at in-context learning (ICL), a supervised
learning technique that relies on adding annotated examples to the model
context. We investigate a contextual bandit version of in-context reinforcement
learning (ICRL), where models learn in-context, online, from external reward,
instead of supervised data. We show that LLMs effectively demonstrate such
learning, and provide a detailed study of the phenomena, experimenting with
challenging classification tasks and models of sizes from 500M to 70B
parameters. This includes identifying and addressing the instability of the
process, demonstrating learning with both semantic and abstract labels, and
showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while
also underscoring fundamental limitations in their implicit reasoning about
errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diverse Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training of language models, either through reinforcement learning,
preference optimization or supervised finetuning, tends to sharpen the output
probability distribution and reduce the diversity of generated responses. This
is particularly a problem for creative generative tasks where varied responses
are desired. In this work we introduce Diverse Preference Optimization (DivPO),
an optimization method which learns to generate much more diverse responses
than standard pipelines, while maintaining the quality of the generations. In
DivPO, preference pairs are selected by first considering a pool of responses,
and a measure of diversity among them, and selecting chosen examples as being
more rare but high quality, while rejected examples are more common, but low
quality. DivPO results in generating 45.6% more diverse persona attributes, and
an 74.6% increase in story diversity, while maintaining similar win rates as
standard baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4o as the Gold Standard: A Scalable and General Purpose Approach to
  Filter Language Model <span class="highlight-title">Pretrain</span>ing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models require vast amounts of high-quality training data, but
effective filtering of web-scale datasets remains a significant challenge. This
paper demonstrates that GPT-4o is remarkably effective at identifying
high-quality training data, but its prohibitive cost makes it impractical at
web-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o
accuracy at less than 1\% of the cost. SIEVE can perform up to 500 filtering
operations for the cost of one GPT-4o filtering call. The key to SIEVE is a
seamless integration of GPT-4o and lightweight text classification models,
using active learning to fine-tune these models in the background with a small
number of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a
tiny fraction of the cost. Through different filtering prompts, SIEVE can
efficiently curate high quality data for general or specialized domains from
web-scale corpora -- a valuable capability given the current scarcity of
high-quality domain-specific datasets. Extensive experiments using automatic
and human evaluation metrics show that SIEVE and GPT-4o achieve similar
performance on five highly specific filtering prompts. In addition, when
performing quality filtering on web crawl datasets, we demonstrate SIEVE can
further improve over state-of-the-art quality filtering methods in the
DataComp-LM challenge for selecting LLM pretraining data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafetyAnalyst: Interpretable, transparent, and steerable safety
  moderation for AI behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal AI safety moderation system would be both structurally
interpretable (so its decisions can be reliably explained) and steerable (to
align to safety standards and reflect a community's values), which current
systems fall short on. To address this gap, we present SafetyAnalyst, a novel
AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses
chain-of-thought reasoning to analyze its potential consequences by creating a
structured "harm-benefit tree," which enumerates harmful and beneficial actions
and effects the AI behavior may lead to, along with likelihood, severity, and
immediacy labels that describe potential impact on any stakeholders.
SafetyAnalyst then aggregates all harmful and beneficial effects into a
harmfulness score using fully interpretable weight parameters, which can be
aligned to particular safety preferences. We applied this conceptual framework
to develop, test, and release an open-source LLM prompt safety classification
system, distilled from 18.5 million harm-benefit features generated by frontier
LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we
show that SafetyReporter (average F1=0.81) outperforms existing LLM safety
moderation systems (average F1$<$0.72) on prompt safety classification, while
offering the additional advantages of interpretability, transparency, and
steerability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Slow than Sorry: Introducing Positive Friction for Reliable
  <span class="highlight-title">Dialogue</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert İnan, Anthony Sicilia, Suvodip Dey, Vardhan Dongre, Tejas Srinivasan, Jesse Thomason, Gökhan Tür, Dilek Hakkani-Tür, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While theories of discourse and cognitive science have long recognized the
value of unhurried pacing, recent dialogue research tends to minimize friction
in conversational systems. Yet, frictionless dialogue risks fostering
uncritical reliance on AI outputs, which can obscure implicit assumptions and
lead to unintended consequences. To meet this challenge, we propose integrating
positive friction into conversational AI, which promotes user reflection on
goals, critical thinking on system response, and subsequent re-conditioning of
AI systems. We hypothesize systems can improve goal alignment, modeling of user
mental states, and task success by deliberately slowing down conversations in
strategic moments to ask questions, reveal assumptions, or pause. We present an
ontology of positive friction and collect expert human annotations on
multi-domain and embodied goal-oriented corpora. Experiments on these corpora,
along with simulated interactions using state-of-the-art systems, suggest
incorporating friction not only fosters accountable decision-making, but also
enhances machine understanding of user beliefs and goals, and increases task
success rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexi<span class="highlight-title">GPT</span>: Pruning and Extending Large Language Models with Low-Rank
  Weight Sharing <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Seale Smith, Chi-Heng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, Yen-Chang Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of large language models (LLMs) in natural language
processing (NLP) has created a critical need for techniques that enable
efficient deployment on memory-constrained devices without compromising
performance. We present a method to prune LLMs that selectively prunes model
blocks based on an importance score and replaces them with a low-parameter
replacement strategy. Specifically, we propose a principled metric to replace
each pruned block using a weight-sharing mechanism that leverages unpruned
counterparts from the model and block-specific low-rank adapters. Furthermore,
we facilitate the learning of these replacement blocks with output feature
normalization and an adapter initialization scheme built on low-rank SVD
reconstructions. Empirical evaluations demonstrate substantial performance
gains over existing methods, achieving state-of-the-art performance on 5/6
benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression
rate of 40%. We also demonstrate that our approach can extend smaller models,
boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended
training with minimal additional parameter costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 - Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FACTTRACK: Time-Aware World State Tracking in Story Outlines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Lyu, Kevin Yang, Lingpeng Kong, Daniel Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While accurately detecting and correcting factual contradictions in language
model outputs has become increasingly important as their capabilities improve,
doing so is highly challenging. We propose a novel method, FACTTRACK, for
tracking atomic facts and addressing factual contradictions. Crucially,
FACTTRACK also maintains time-aware validity intervals for each fact, allowing
for change over time. At a high level, FACTTRACK consists of a four-step
pipeline to update a world state data structure for each new event: (1)
decompose the event into directional atomic facts; (2) determine the validity
interval of each atomic fact using the world state; (3) detect contradictions
with existing facts in the world state; and finally (4) add new facts to the
world state and update existing atomic facts. When we apply FACTTRACK to
contradiction detection on structured story outlines, we find that FACTTRACK
using LLaMA2-7B-Chat substantially outperforms a fair baseline using
LLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.
Moreover, when using GPT4, FACTTRACK significantly outperforms the GPT4
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Natural Language to Extensive-Form Game Representations <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17282v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17282v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Deng, Yongzhao Wang, Rahul Savani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework for translating game descriptions in natural
language into extensive-form representations in game theory, leveraging Large
Language Models (LLMs) and in-context learning. Given the varying levels of
strategic complexity in games, such as perfect versus imperfect information,
directly applying in-context learning would be insufficient. To address this,
we introduce a two-stage framework with specialized modules to enhance
in-context learning, enabling it to divide and conquer the problem effectively.
In the first stage, we tackle the challenge of imperfect information by
developing a module that identifies information sets along and the
corresponding partial tree structure. With this information, the second stage
leverages in-context learning alongside a self-debugging module to produce a
complete extensive-form game tree represented using pygambit, the Python API of
a recognized game-theoretic analysis tool called Gambit. Using this python
representation enables the automation of tasks such as computing Nash
equilibria directly from natural language descriptions. We evaluate the
performance of the full framework, as well as its individual components, using
various LLMs on games with different levels of strategic complexity. Our
experimental results show that the framework significantly outperforms baseline
models in generating accurate extensive-form games, with each module playing a
critical role in its success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted as a full paper for AAMAS 2025. This is a
  full version of the AAMAS 2025 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAN: Fourier Analysis Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jinliang Deng, Jing Su, Jun Zhang, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable successes of general-purpose neural networks, such as
MLPs and Transformers, we find that they exhibit notable shortcomings in
modeling and reasoning about periodic phenomena, achieving only marginal
performance within the training domain and failing to generalize effectively to
out-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature and
science. Therefore, neural networks should be equipped with the essential
ability to model and handle periodicity. In this work, we propose FAN, a novel
general-purpose neural network that offers broad applicability similar to MLP
while effectively addressing periodicity modeling challenges. Periodicity is
naturally integrated into FAN's structure and computational processes by
introducing the Fourier Principle. Unlike existing Fourier-based networks,
which possess particular periodicity modeling abilities but are typically
designed for specific tasks, our approach maintains the general-purpose
modeling capability. Therefore, FAN can seamlessly replace MLP in various model
architectures with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the superiority of FAN in periodicity modeling tasks and the
effectiveness and generalizability of FAN across a range of real-world tasks,
e.g., symbolic formula representation, time series forecasting, language
modeling, and image recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have revolutionized vision-language
understanding but remain vulnerable to multimodal jailbreak attacks, where
adversarial inputs are meticulously crafted to elicit harmful or inappropriate
responses. We propose UniGuard, a novel multimodal safety guardrail that
jointly considers the unimodal and cross-modal harmful signals. UniGuard trains
a multimodal guardrail to minimize the likelihood of generating harmful
responses in a toxic corpus. The guardrail can be seamlessly applied to any
input prompt during inference with minimal computational costs. Extensive
experiments demonstrate the generalizability of UniGuard across multiple
modalities, attack strategies, and multiple state-of-the-art MLLMs, including
LLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust
defense mechanism maintains the models' overall vision-language understanding
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts
  on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Xu, Wenlu Fan, Shiqian Lu, Tenghao Li, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of misinformation and fake news in online political discourse poses
significant challenges to democratic processes and public engagement. While
debunking efforts aim to counteract misinformation and foster fact-based
dialogue, these discussions often involve language toxicity and emotional
polarization. We examined over 86 million debunking tweets and more than 4
million Reddit debunking comments to investigate the relationship between
language toxicity, pessimism, and social polarization in debunking efforts.
Focusing on discussions of the 2016 and 2020 U.S. presidential elections and
the QAnon conspiracy theory, our analysis reveals three key findings: (1)
peripheral participants (1-degree users) play a disproportionate role in
shaping toxic discourse, driven by lower community accountability and emotional
expression; (2) platform mechanisms significantly influence polarization, with
Twitter amplifying partisan differences and Reddit fostering higher overall
toxicity due to its structured, community-driven interactions; and (3) a
negative correlation exists between language toxicity and pessimism, with
increased interaction reducing toxicity, especially on Reddit. We show that
platform architecture affects informational complexity of user interactions,
with Twitter promoting concentrated, uniform discourse and Reddit encouraging
diverse, complex communication. Our findings highlight the importance of user
engagement patterns, platform dynamics, and emotional expressions in shaping
polarization in debunking discourse. This study offers insights for
policymakers and platform designers to mitigate harmful effects and promote
healthier online discussions, with implications for understanding
misinformation, hate speech, and political polarization in digital
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Systematic Monolingual NLP <span class="highlight-title">Survey</span>s: GenA of Greek NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09861v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09861v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juli Bakagianni, Kanella Pouli, Maria Gavriilidou, John Pavlopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) research has traditionally been
predominantly focused on English, driven by the availability of resources, the
size of the research community, and market demands. Recently, there has been a
noticeable shift towards multilingualism in NLP, recognizing the need for
inclusivity and effectiveness across diverse languages and cultures.
Monolingual surveys have the potential to complement the broader trend towards
multilingualism in NLP by providing foundational insights and resources,
necessary for effectively addressing the linguistic diversity of global
communication. However, monolingual NLP surveys are extremely rare in the
literature. This study introduces a generalizable methodology for creating
systematic and comprehensive monolingual NLP surveys, aimed at optimizing the
process of constructing such surveys and thoroughly addressing a language's NLP
support. Our approach integrates a structured search protocol to avoid
selection bias and ensure reproducibility, an NLP task taxonomy to organize the
surveyed material coherently, and language resources (LRs) taxonomies to
identify potential benchmarks and highlight opportunities for improving
resource availability (e.g., through better maintenance or licensing). We apply
this methodology to Greek NLP (2012-2023), providing a comprehensive overview
of its current state and challenges. We discuss the progress of Greek NLP and
outline the Greek LRs found, classified by availability and usability,
assessing language support per NLP task. The presented systematic literature
review of Greek NLP serves as an application of our method that showcases the
benefits of monolingual NLP surveys more broadly. Similar applications could be
considered for the myriads of languages whose progress in NLP lags behind that
of well-supported languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>77 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Separate Instructions From Data? And What Do We Even Mean By
  That? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06833v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06833v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned Large Language Models (LLMs) show impressive results in
numerous practical applications, but they lack essential safety features that
are common in other areas of computer science, particularly an explicit
separation of instructions and data. This makes them vulnerable to
manipulations such as indirect prompt injections and generally unsuitable for
safety-critical tasks. Surprisingly, there is currently no established
definition or benchmark to quantify this phenomenon. In this work, we close
this gap by introducing a formal measure for instruction-data separation and an
empirical variant that is calculable from a model's outputs. We also present a
new dataset, SEP, that allows estimating the measure for real-world models. Our
results on various LLMs show that the problem of instruction-data separation is
real: all models fail to achieve high separation, and canonical mitigation
techniques, such as prompt engineering and fine-tuning, either fail to
substantially improve separation or reduce model utility. The source code and
SEP dataset are openly accessible at
https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025, GitHub:
  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 10 pages main
  text, 30 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Autoencoders Reveal Universal Feature Spaces Across Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones, making it difficult
to disentangle and match features across different models. To address this
issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics on SAE feature spaces across different LLMs. Our
experiments reveal significant similarities in SAE feature spaces across
various LLMs, providing new evidence for feature universality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theoretical guarantees on the best-of-n alignment policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simple and effective method for the inference-time alignment of generative
models is the best-of-$n$ policy, where $n$ samples are drawn from a reference
policy, ranked based on a reward function, and the highest ranking one is
selected. A commonly used analytical expression in the literature claims that
the KL divergence between the best-of-$n$ policy and the reference policy is
equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show
that it is an upper bound on the actual KL divergence. We also explore the
tightness of this upper bound in different regimes, and propose a new estimator
for the KL divergence and empirically show that it provides a tight
approximation. We also show that the win rate of the best-of-$n$ policy against
the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the
tightness of this characterization. We conclude with analyzing the tradeoffs
between win rate and KL divergence of the best-of-$n$ alignment policy, which
demonstrate that very good tradeoffs are achievable with $n < 1000$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoElicit: Using Large Language Models for Expert Prior Elicitation in
  Predictive Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17284v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17284v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) acquire a breadth of information across various
domains. However, their computational complexity, cost, and lack of
transparency often hinder their direct application for predictive tasks where
privacy and interpretability are paramount. In fields such as healthcare,
biology, and finance, specialised and interpretable linear models still hold
considerable value. In such domains, labelled data may be scarce or expensive
to obtain. Well-specified prior distributions over model parameters can reduce
the sample complexity of learning through Bayesian inference; however,
eliciting expert priors can be time-consuming. We therefore introduce
AutoElicit to extract knowledge from LLMs and construct priors for predictive
models. We show these priors are informative and can be refined using natural
language. We perform a careful study contrasting AutoElicit with in-context
learning and demonstrate how to perform model selection between the two
methods. We find that AutoElicit yields priors that can substantially reduce
error over uninformative priors, using fewer labels, and consistently
outperform in-context learning. We show that AutoElicit saves over 6 months of
labelling effort when building a new predictive model for urinary tract
infections from sensor recordings of people living with dementia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual
  Preference Data <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Murun Yang, Qiaozhi He, Tong Xiao, Chunliang Zhang, Tongran Liu, Quan Du, Di Yang, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) often fail to align with human
preferences, leading to issues like generating misleading content without
proper visual context (also known as hallucination). A promising solution to
this problem is using human-preference alignment techniques, such as best-of-n
sampling and reinforcement learning. However, these techniques face the
difficulty arising from the scarcity of visual preference data, which is
required to train a visual reward model (VRM). In this work, we continue the
line of research. We present a Robust Visual Reward Model (RoVRM) which
improves human-preference alignment for LVLMs. RoVRM leverages auxiliary
textual preference data through a three-phase progressive training and optimal
transport-based preference data selection to effectively mitigate the scarcity
of visual preference data. We experiment with RoVRM on the commonly used
vision-language tasks based on the LLaVA-1.5-7B and -13B models. Experimental
results demonstrate that RoVRM consistently outperforms traditional VRMs.
Furthermore, our three-phase progressive training and preference data selection
approaches can yield consistent performance gains over ranking-based alignment
techniques, such as direct preference optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cautious Optimizers: Improving Training with One Line of Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhao Liang, Lizhang Chen, Bo Liu, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AdamW has been the default optimizer for transformer pretraining. For many
years, our community searched for faster and more stable optimizers with only
constrained positive outcomes. In this work, we propose a single-line
modification in Pytorch to any momentum-based optimizer, which we rename
cautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that
this modification preserves Adam's Hamiltonian function and it does not break
the convergence guarantee under the Lyapunov analysis. In addition, a whole new
family of optimizers is revealed by our theoretical insight. Among them, we
pick the simplest one for empirical experiments, showing not only speed-up on
Llama and MAE pretraining up to $1.47$ times, but also better results in LLM
post-training tasks. Code is available at
https://github.com/kyleliang919/C-Optim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Banyan: Improved Representation Learning with Explicit Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Opper, N. Siddharth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Banyan, a model that efficiently learns semantic representations
by leveraging explicit hierarchical structure. While transformers excel at
scale, they struggle in low-resource settings. Conversely recent structured
models have shown promise as efficient learners, but lack performance. Banyan
bridges this gap with two key innovations: an entangled hierarchical tree
structure and diagonalized message passing, enabling it to outperform larger
transformer models with just 14 non-embedding parameters. It excels in
low-resource settings, offering a viable alternative for under-represented
languages and highlighting its potential for efficient, interpretable NLP in
resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Chatbot for Asylum-Seeking Migrants in Europe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bettina Fazzinga, Elena Palmieri, Margherita Vestoso, Luca Bolognini, Andrea Galassi, Filippo Furfaro, Paolo Torroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ACME: A Chatbot for asylum-seeking Migrants in Europe. ACME relies
on computational argumentation and aims to help migrants identify the highest
level of protection they can apply for. This would contribute to a more
sustainable migration by reducing the load on territorial commissions, Courts,
and humanitarian organizations supporting asylum applicants. We describe the
background context, system architecture, underlying technologies, and a case
study used to validate the tool with domain experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2024 IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subgraph-Aware Training of Language Models for <span class="highlight-title">Knowledge</span> Graph
  Completion Using Structure-Aware Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12703v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12703v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youmin Ko, Hyemin Yang, Taeuk Kim, Hyunjoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained language models (PLMs) has recently shown a potential
to improve knowledge graph completion (KGC). However, most PLM-based methods
focus solely on encoding textual information, neglecting the long-tailed nature
of knowledge graphs and their various topological structures, e.g., subgraphs,
shortest paths, and degrees. We claim that this is a major obstacle to
achieving higher accuracy of PLMs for KGC. To this end, we propose a
Subgraph-Aware Training framework for KGC (SATKGC) with two ideas: (i)
subgraph-aware mini-batching to encourage hard negative sampling and to
mitigate an imbalance in the frequency of entity occurrences during training,
and (ii) new contrastive learning to focus more on harder in-batch negative
triples and harder positive triples in terms of the structural properties of
the knowledge graph. To the best of our knowledge, this is the first study to
comprehensively incorporate the structural inductive bias of the knowledge
graph into fine-tuning PLMs. Extensive experiments on three KGC benchmarks
demonstrate the superiority of SATKGC. Our code is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markovian <span class="highlight-title">Transformer</span>s for Informative Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18988v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18988v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Viteri, Max Lamparth, Peter Chatain, Clark Barrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language
model's underlying decision process. We address this by making CoT text
causally essential in a "Markovian" language model, factoring next-token
prediction through an intermediate CoT and training it to predict future tokens
independently of the original prompt. We formalize this via an
"informativeness" objective that quantifies how much a trained CoT improves
next-token predictions over a baseline. Using policy gradient, we show that
Llama 3.1 8B achieves a 33.2% absolute accuracy improvement on GSM8K.
Perturbation tests confirm stronger reliance on the CoT, while cross-model
transfers indicate these reasoning traces generalize across interpreters. Our
approach enhances both accuracy and interpretability, potentially extending CoT
reasoning to arbitrarily long contexts and diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Explainability Approach for Human-Aware Robots in
  Multi-Party <span class="highlight-title">Conversation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iveta Bečková, Štefan Pócoš, Giulia Belgiovine, Marco Matarese, Omar Eldardeer, Alessandra Sciutti, Carlo Mazzola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The addressee estimation (understanding to whom somebody is talking) is a
fundamental task for human activity recognition in multi-party conversation
scenarios. Specifically, in the field of human-robot interaction, it becomes
even more crucial to enable social robots to participate in such interactive
contexts. However, it is usually implemented as a binary classification task,
restricting the robot's capability to estimate whether it was addressed
\review{or not, which} limits its interactive skills. For a social robot to
gain the trust of humans, it is also important to manifest a certain level of
transparency and explainability. Explainable artificial intelligence thus plays
a significant role in the current machine learning applications and models, to
provide explanations for their decisions besides excellent performance. In our
work, we a) present an addressee estimation model with improved performance in
comparison with the previous state-of-the-art; b) further modify this model to
include inherently explainable attention-based segments; c) implement the
explainable addressee estimation as part of a modular cognitive architecture
for multi-party conversation in an iCub robot; d) validate the real-time
performance of the explainable model in multi-party human-robot interaction; e)
propose several ways to incorporate explainability and transparency in the
aforementioned architecture; and f) perform an online user study to analyze the
effect of various explanations on how human participants perceive the robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32pp (+6pp sup.mat.) Accepted in Computer Vision and Image
  Understanding Journal on January 23, 2025. This research received funding
  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and
  Development Agency, project no. APVV-21-0105</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Unsupervised Constituency Parsing via Maximizing Semantic
  Information <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Xiangheng He, Yusuke Miyao, Danushka Bollegala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised constituency parsers organize phrases within a sentence into a
tree-shaped syntactic constituent structure that reflects the organization of
sentence semantics. However, the traditional objective of maximizing sentence
log-likelihood (LL) does not explicitly account for the close relationship
between the constituent structure and the semantics, resulting in a weak
correlation between LL values and parsing accuracy. In this paper, we introduce
a novel objective for training unsupervised parsers: maximizing the information
between constituent structures and sentence semantics (SemInfo). We introduce a
bag-of-substrings model to represent the semantics and apply the
probability-weighted information metric to estimate the SemInfo. Additionally,
we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the
SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)
induction, the state-of-the-art method for unsupervised constituency parsing.
Experiments demonstrate that SemInfo correlates more strongly with parsing
accuracy than LL. Our algorithm significantly enhances parsing accuracy by an
average of 7.85 points across five PCFG variants and in four languages,
achieving new state-of-the-art results in three of the four languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Reliability of Self-Explanations in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the reliability of explanations generated by large
language models (LLMs) when prompted to explain their previous output. We
evaluate two kinds of such self-explanations - extractive and counterfactual -
using three state-of-the-art LLMs (2B to 8B parameters) on two different
classification tasks (objective and subjective). Our findings reveal, that,
while these self-explanations can correlate with human judgement, they do not
fully and accurately follow the model's decision process, indicating a gap
between perceived and actual model reasoning. We show that this gap can be
bridged because prompting LLMs for counterfactual explanations can produce
faithful, informative, and easy-to-verify results. These counterfactuals offer
a promising alternative to traditional explainability methods (e.g. SHAP,
LIME), provided that prompts are tailored to specific tasks and checked for
validity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Non peer-reviewed preprint. Presented at Discovery Science 2024.
  Peer-reviewed version published in the Springer Lecture Notes in Computer
  Science (vol 15243)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Addressing <span class="highlight-title">Hallucination</span>s with RAG and NMISS in Italian Healthcare LLM
  Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Paola Priola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I combine detection and mitigation techniques to addresses hallucinations in
Large Language Models (LLMs). Mitigation is achieved in a question-answering
Retrieval-Augmented Generation (RAG) framework while detection is obtained by
introducing the Negative Missing Information Scoring System (NMISS), which
accounts for contextual relevance in responses. While RAG mitigates
hallucinations by grounding answers in external data, NMISS refines the
evaluation by identifying cases where traditional metrics incorrectly flag
contextually accurate responses as hallucinations. I use Italian health news
articles as context to evaluate LLM performance. Results show that Gemma2 and
GPT-4 outperform the other models, with GPT-4 producing answers closely aligned
with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral
benefit significantly from NMISS, highlighting their ability to provide richer
contextual information. This combined approach offers new insights into the
reduction and more accurate assessment of hallucinations in LLMs, with
applications in real-world healthcare tasks and other domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Magic Elevating Depression Detection with a Fusion of Text
  and Audio Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lindy Gan, Yifan Huang, Xiaoyang Gao, Jiaming Tan, Fujun Zhao, Tao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes an innovative multimodal fusion model based on a
teacher-student architecture to enhance the accuracy of depression
classification. Our designed model addresses the limitations of traditional
methods in feature fusion and modality weight allocation by introducing
multi-head attention mechanisms and weighted multimodal transfer learning.
Leveraging the DAIC-WOZ dataset, the student fusion model, guided by textual
and auditory teacher models, achieves significant improvements in
classification accuracy. Ablation experiments demonstrate that the proposed
model attains an F1 score of 99. 1% on the test set, significantly
outperforming unimodal and conventional approaches. Our method effectively
captures the complementarity between textual and audio features while
dynamically adjusting the contributions of the teacher models to enhance
generalization capabilities. The experimental results highlight the robustness
and adaptability of the proposed framework in handling complex multimodal data.
This research provides a novel technical framework for multimodal large model
learning in depression analysis, offering new insights into addressing the
limitations of existing methods in modality fusion and feature extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages,7 figures.1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Hardness of Code in Large Language Models -- A
  Probabilistic Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common practice in large language model (LLM) usage for complex analytical
tasks such as code generation, is to sample a solution for the entire task
within the model's context window. Previous works have shown that subtask
decomposition within the model's context (chain of thought), is beneficial for
solving such tasks. In this work, we point a limitation of LLMs' ability to
perform several sub-tasks within the same context window - an in-context
hardness of composition, pointing to an advantage for distributing a decomposed
problem in a multi-agent system of LLMs. The hardness of composition is
quantified by a generation complexity metric, i.e., the number of LLM
generations required to sample at least one correct solution. We find a gap
between the generation complexity of solving a compositional problem within the
same context relative to distributing it among multiple agents, that increases
exponentially with the solution's length. We prove our results theoretically
and demonstrate them empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verifying Cross-modal Entity <span class="highlight-title">Consist</span>ency in News using Vision-language
  Models <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Tahmasebi, David Ernst, Eric Müller-Budack, Ralph Ewerth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web has become a crucial source of information, but it is also used to
spread disinformation, often conveyed through multiple modalities like images
and text. The identification of inconsistent cross-modal information, in
particular entities such as persons, locations, and events, is critical to
detect disinformation. Previous works either identify out-of-context
disinformation by assessing the consistency of images to the whole document,
neglecting relations of individual entities, or focus on generic entities that
are not relevant to news. So far, only few approaches have addressed the task
of validating entity consistency between images and text in news. However, the
potential of large vision-language models (LVLMs) has not been explored yet. In
this paper, we propose an LVLM-based framework for verifying Cross-modal Entity
Consistency~(LVLM4CEC), to assess whether persons, locations and events in news
articles are consistent across both modalities. We suggest effective prompting
strategies for LVLMs for entity verification that leverage reference images
crawled from web. Moreover, we extend three existing datasets for the task of
entity verification in news providing manual ground-truth data. Our results
show the potential of LVLMs for automating cross-modal entity verification,
showing improved accuracy in identifying persons and events when using evidence
images. Moreover, our method outperforms a baseline for location and event
verification in documents. The datasets and source code are available on GitHub
at https://github.com/TIBHannover/LVLM4CEC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in: European Conference on Information
  Retrieval (ECIR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Derivational ChainBank for Modern Standard Arabic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reham Marzouk, Sondos Krouna, Nizar Habash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the new concept of an Arabic Derivational Chain Bank CHAINBANK
to leverage the relationship between form and meaning in modeling Arabic
derivational morphology. We constructed a knowledge graph network of abstract
patterns and their derivational relations and aligned it with the lemmas of the
CAMELMORPH morphological analyzer database. This process produced chains of
derived words' lemmas linked to their corresponding lemma bases through
derivational relations, encompassing 23,333 derivational connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference <span class="highlight-title">Consist</span>ency Matters: Enhancing Preference Learning in
  Language Models with Automated Self-Curation of Training Corpora <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JoonHo Lee, JuYoun Son, Juree Seok, Wooseok Jang, Yeong-Dae Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inconsistent annotations in training corpora, particularly within preference
learning datasets, pose challenges in developing advanced language models.
These inconsistencies often arise from variability among annotators and
inherent multi-dimensional nature of the preferences. To address these issues,
we introduce a self-curation method that preprocesses annotated datasets by
leveraging proxy models trained directly on them. Our method enhances
preference learning by automatically detecting and selecting consistent
annotations. We validate the proposed approach through extensive
instruction-following tasks, demonstrating performance improvements of up to
33\% across various learning algorithms and proxy capabilities. This work
offers a straightforward and reliable solution to address preference
inconsistencies without relying on heuristics, serving as an initial step
toward the development of more advanced preference learning methodologies. Code
is available at https://github.com/Self-Curation/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Instruction Following in Language Models through Proxy-Based
  Uncertainty Estimation <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JoonHo Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, JuYoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, Seungjai Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing response quality to instructions in language models is vital but
challenging due to the complexity of human language across different contexts.
This complexity often results in ambiguous or inconsistent interpretations,
making accurate assessment difficult. To address this issue, we propose a novel
Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty
estimation for the quality of paired responses based on Bayesian approximation.
Trained with preference datasets, our uncertainty-enabled proxy not only scores
rewards for responses but also evaluates their inherent uncertainty. Empirical
results demonstrate significant benefits of incorporating the proposed proxy
into language model training. Our method boosts the instruction following
capability of language models by refining data curation for training and
improving policy optimization objectives, thereby surpassing existing methods
by a large margin on benchmarks such as Vicuna and MT-bench. These findings
highlight that our proposed approach substantially advances language model
training and paves a new way of harnessing uncertainty within language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Model Uncertainty Function as a Proxy for Multiple-Choice Question
  Item Difficulty? <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Zotos, Hedderik van Rijn, Malvina Nissim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the difficulty of multiple-choice questions would be great help
for educators who must spend substantial time creating and piloting stimuli for
their tests, and for learners who want to practice. Supervised approaches to
difficulty estimation have yielded to date mixed results. In this contribution
we leverage an aspect of generative large models which might be seen as a
weakness when answering questions, namely their uncertainty, and exploit it
towards exploring correlations between two different metrics of uncertainty,
and the actual student response distribution. While we observe some present but
weak correlations, we also discover that the models' behaviour is different in
the case of correct vs wrong answers, and that correlations differ
substantially according to the different question types which are included in
our fine-grained, previously unused dataset of 451 questions from a
Biopsychology course. In discussing our findings, we also suggest potential
avenues to further leverage model uncertainty as an additional proxy for item
difficulty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, Published in The 31st International Conference
  on Computational Linguistics, available in the ACL Anthology:
  https://aclanthology.org/2025.coling-main.749/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AST<span class="highlight-title">Prompt</span>er: Weakly Supervised Automated Language Model Red-Teaming to
  Identify Low-Perplexity Toxic <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amelia F. Hardy, Houjun Liu, Bernard Lange, Duncan Eddy, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional approaches for the automated red-teaming of large language
models (LLMs) aim to identify prompts that elicit toxic outputs from a frozen
language model (the defender). This often results in the prompting model (the
adversary) producing text that is unlikely to arise during autoregression. In
response, we propose a reinforcement learning formulation of LLM red-teaming
designed to discover prompts that both (1) elicit toxic outputs from a defender
and (2) have low perplexity as scored by that defender. These prompts are the
most pertinent in a red-teaming setting because the defender generates them
with high probability. We solve this formulation with an online and weakly
supervised form of Identity Preference Optimization (IPO), attacking models
ranging from 137M to 7.8B parameters. Our policy performs competitively,
producing prompts that induce defender toxicity at a rate of 2-23 times higher
than baseline across model scales. Importantly, these prompts have lower
perplexity than both automatically generated and human-written attacks.
Furthermore, our method creates black-box attacks with 5.4-14 times increased
toxicity. To assess the downstream utility of our method, we use rollouts from
our policy as negative examples for downstream toxicity tuning and demonstrate
improved safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 pages of appendix, 3 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting the Target Word of Game-playing <span class="highlight-title">Conversation</span>s using a
  Low-Rank Dialect Adapter for Decoder Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipankar Srirag, Aditya Joshi, Jacob Eisenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialect adapters that improve the performance of LLMs for NLU tasks on
certain sociolects/dialects/national varieties ('dialects' for the sake of
brevity) have been reported for encoder models. In this paper, we extend the
idea of dialect adapters to decoder models in our architecture called LoRDD.
Using MD-3, a publicly available dataset of word game-playing conversations
between dialectal speakers, our task is Target Word Prediction (TWP) from a
masked conversation. LoRDD combines task adapters and dialect adapters where
the latter employ contrastive learning on pseudo-parallel conversations from
MD-3. Our experiments on Indian English and Nigerian English conversations with
two models (Mistral and Gemma) demonstrate that LoRDD outperforms four
baselines on TWP. Additionally, it significantly reduces the performance gap
with American English, narrowing it to 12% and 5.8% for word similarity, and
25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is
in its promise for dialect adaptation of decoder models using TWP, a simplified
version of the commonly used next-word prediction task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understand, Solve and Translate: Bridging the Multilingual Mathematical
  <span class="highlight-title">Reasoning</span> Gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Ko, Guijin Son, Dasol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate exceptional performance on complex
reasoning tasks. However, despite their strong reasoning capabilities in
high-resource languages (e.g., English and Chinese), a significant performance
gap persists in other languages. To investigate this gap in Korean, we
introduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual
math problems. Through systematic analysis of model behaviors, we identify a
key finding: these performance disparities stem primarily from difficulties in
comprehending non-English inputs, rather than limitations in reasoning
capabilities. Based on these findings, we propose UST (Understand, Solve, and
Translate), a method that strategically uses English as an anchor for reasoning
and solution generation. By fine-tuning the model on 130k synthetically
generated data points, UST achieves a 10.91% improvement on the HRM8K benchmark
and reduces the multilingual performance gap from 11.6% to 0.7%. Additionally,
we show that improvements from UST generalize effectively to different Korean
domains, demonstrating that capabilities acquired from machine-verifiable
content can be generalized to other areas. We publicly release the benchmark,
training dataset, and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 14 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Parallel Program Performance with LLM Optimizers via
  Agent-System Interface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern scientific discovery increasingly relies on high-performance computing
for complex modeling and simulation. A key challenge in improving parallel
program performance is efficiently mapping tasks to processors and data to
memory, a process dictated by intricate, low-level system code known as
mappers. Developing high-performance mappers demands days of manual tuning,
posing a significant barrier for domain scientists without systems expertise.
We introduce a framework that automates mapper development with generative
optimization, leveraging richer feedback beyond scalar performance metrics. Our
approach features the Agent-System Interface, which includes a Domain-Specific
Language (DSL) to abstract away low-level complexity of system code and define
a structured search space, as well as AutoGuide, a mechanism that interprets
raw execution output into actionable feedback. Unlike traditional reinforcement
learning methods such as OpenTuner, which rely solely on scalar feedback, our
method finds superior mappers in far fewer iterations. With just 10 iterations,
it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster
performance. Our approach finds mappers that surpass expert-written mappers by
up to 1.34X speedup across nine benchmarks while reducing tuning time from days
to minutes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simultaneous Reward Distillation and Preference Learning: Get You a
  Language Model Who Can Do Both 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijnan Nath, Changsoo Jung, Ethan Seefried, Nikhil Krishnaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional RLHF-based LLM alignment methods explicitly maximize the expected
rewards from a separate reward model. More recent supervised alignment methods
like Direct Preference Optimization (DPO) circumvent this phase to avoid
problems including model drift and reward overfitting. Although popular due to
its simplicity, DPO and similar direct alignment methods which rely heavily on
the Bradley-Terry-based pairwise preference formulation can still lead to
degenerate policies when challenged by non-deterministic or noisy preference
labels, for example human scoring of two candidate outputs with low confidence.
This paper introduces DRDO (Direct Reward Distillation and
policy-Optimization), which simultaneously models rewards and preferences to
avoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while
learning human preferences with a novel preference likelihood formulation.
Results on the Ultrafeedback and TL;DR datasets demonstrate that DRDO-trained
policies surpass methods such as DPO and e-DPO in terms of expected rewards and
are more robust, on average, to noisy preference signals as well as
out-of-distribution (OOD) settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InvAgent: A Large Language Model based Multi-Agent System for Inventory
  Management in Supply Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinzhu Quan, Zefang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supply chain management (SCM) involves coordinating the flow of goods,
information, and finances across various entities to deliver products
efficiently. Effective inventory management is crucial in today's volatile and
uncertain world. Previous research has demonstrated the superiority of
heuristic methods and reinforcement learning applications in inventory
management. However, the application of large language models (LLMs) as
autonomous agents in multi-agent systems for inventory management remains
underexplored. This study introduces a novel approach using LLMs to manage
multi-agent inventory systems. Leveraging their zero-shot learning
capabilities, our model, InvAgent, enhances resilience and improves efficiency
across the supply chain network. Our contributions include utilizing LLMs for
zero-shot learning to enable adaptive and informed decision-making without
prior training, providing explainability and clarity through chain-of-thought,
and demonstrating dynamic adaptability to varying demand scenarios while
reducing costs and preventing stockouts. Extensive evaluations across different
scenarios highlight the efficiency of our model in SCM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangjie Zheng, Junwei Yang, Siyue Liang, Bin Feng, Zequn Liu, Wei Ju, Zhiping Xiao, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Language Models (MLMs) have achieved remarkable success in many
self-supervised representation learning tasks. MLMs are trained by randomly
masking portions of the input sequences with [MASK] tokens and learning to
reconstruct the original content based on the remaining context. This paper
explores the impact of [MASK] tokens on MLMs. Analytical studies show that
masking tokens can introduce the corrupted semantics problem, wherein the
corrupted context may convey multiple, ambiguous meanings. This problem is also
a key factor affecting the performance of MLMs on downstream tasks. Based on
these findings, we propose a novel enhanced-context MLM, ExLM. Our approach
expands [MASK] tokens in the input context and models the dependencies between
these expanded states. This enhancement increases context capacity and enables
the model to capture richer semantic information, effectively mitigating the
corrupted semantics problem during pre-training. Experimental results
demonstrate that ExLM achieves significant performance improvements in both
text modeling and SMILES modeling tasks. Further analysis confirms that ExLM
enriches semantic representations through context enhancement, and effectively
reduces the semantic multimodality commonly observed in MLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReXTrust: A Model for Fine-Grained <span class="highlight-title">Hallucination</span> Detection in
  AI-<span class="highlight-title">Generate</span>d Radiology Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15264v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15264v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Hardy, Sung Eun Kim, Du Hyun Ro, Pranav Rajpurkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing adoption of AI-generated radiology reports necessitates robust
methods for detecting hallucinations--false or unfounded statements that could
impact patient care. We present ReXTrust, a novel framework for fine-grained
hallucination detection in AI-generated radiology reports. Our approach
leverages sequences of hidden states from large vision-language models to
produce finding-level hallucination risk scores. We evaluate ReXTrust on a
subset of the MIMIC-CXR dataset and demonstrate superior performance compared
to existing approaches, achieving an AUROC of 0.8751 across all findings and
0.8963 on clinically significant findings. Our results show that white-box
approaches leveraging model hidden states can provide reliable hallucination
detection for medical AI systems, potentially improving the safety and
reliability of automated radiology reporting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AIMedHealth 10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Fine-tuning of Large Language Models with Zeroth-order
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04343v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04343v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, Prateek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private stochastic gradient descent (DP-SGD) allows models to
be trained in a privacy-preserving manner, but has proven difficult to scale to
the era of foundation models. We introduce DP-ZO, a private fine-tuning
framework for large language models by privatizing zeroth order optimization
methods. A key insight into the design of our method is that the direction of
the gradient in the zeroth-order optimization we use is random and the only
information from training data is the step size, i.e., a scalar. Therefore, we
only need to privatize the scalar step size, which is memory-efficient. DP-ZO
provides a strong privacy-utility trade-off across different tasks, and model
sizes that are comparable to DP-SGD in $(\varepsilon,\delta)$-DP. Notably,
DP-ZO possesses significant advantages over DP-SGD in memory efficiency, and
obtains higher utility in $\varepsilon$-DP when using the Laplace mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Framework for Semantic Similarity Analysis of Human and
  AI-<span class="highlight-title">Generate</span>d Text Using <span class="highlight-title">Transformer</span> Architectures and Ensemble Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifu Gao, Ziwei Liu, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has made detecting
AI-generated text an increasingly critical challenge. Traditional methods often
fail to capture the nuanced semantic differences between human and
machine-generated content. We therefore propose a novel approach based on
semantic similarity analysis, leveraging a multi-layered architecture that
combines a pre-trained DeBERTa-v3-large model, Bi-directional LSTMs, and linear
attention pooling to capture both local and global semantic patterns. To
enhance performance, we employ advanced input and output augmentation
techniques such as sector-level context integration and wide output
configurations. These techniques enable the model to learn more discriminative
features and generalize across diverse domains. Experimental results show that
this approach works better than traditional methods, proving its usefulness for
AI-generated text detection and other text comparison tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Multilingual Probing in Large Language Models: A
  Cross-Language Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoyang Li, Haiyan Zhao, Qingcheng Zeng, Mengnan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probing techniques for large language models (LLMs) have primarily focused on
English, overlooking the vast majority of the world's languages. In this paper,
we extend these probing methods to a multilingual context, investigating the
behaviors of LLMs across diverse languages. We conduct experiments on several
open-source LLM models, analyzing probing accuracy, trends across layers, and
similarities between probing vectors for multiple languages. Our key findings
reveal: (1) a consistent performance gap between high-resource and low-resource
languages, with high-resource languages achieving significantly higher probing
accuracy; (2) divergent layer-wise accuracy trends, where high-resource
languages show substantial improvement in deeper layers similar to English; and
(3) higher representational similarities among high-resource languages, with
low-resource languages demonstrating lower similarities both among themselves
and with high-resource languages. These results highlight significant
disparities in LLMs' multilingual capabilities and emphasize the need for
improved modeling of low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing User Behavior: The Interplay Between Mobility Patterns
  and Mobile Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne Josiane Kouam, Aline Carneiro Viana, Mariano G. Beiró, Leo Ferres, Luca Pappalardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile devices have become essential for capturing human activity, and
eXtended Data Records (XDRs) offer rich opportunities for detailed user
behavior modeling, which is useful for designing personalized digital services.
Previous studies have primarily focused on aggregated mobile traffic and
mobility analyses, often neglecting individual-level insights. This paper
introduces a novel approach that explores the dependency between traffic and
mobility behaviors at the user level. By analyzing 13 individual features that
encompass traffic patterns and various mobility aspects, we enhance the
understanding of how these behaviors interact. Our advanced user modeling
framework integrates traffic and mobility behaviors over time, allowing for
fine-grained dependencies while maintaining population heterogeneity through
user-specific signatures. Furthermore, we develop a Markov model that infers
traffic behavior from mobility and vice versa, prioritizing significant
dependencies while addressing privacy concerns. Using a week-long XDR dataset
from 1,337,719 users across several provinces in Chile, we validate our
approach, demonstrating its robustness and applicability in accurately
inferring user behavior and matching mobility and traffic profiles across
diverse urban contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mFollowIR: a Multilingual Benchmark for Instruction Following in
  Retrieval <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Sam Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, Dawn Lawrie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval systems generally focus on web-style queries that are short and
underspecified. However, advances in language models have facilitated the
nascent rise of retrieval models that can understand more complex queries with
diverse intents. However, these efforts have focused exclusively on English;
therefore, we do not yet understand how they work across languages. We
introduce mFollowIR, a multilingual benchmark for measuring
instruction-following ability in retrieval models. mFollowIR builds upon the
TREC NeuCLIR narratives (or instructions) that span three diverse languages
(Russian, Chinese, Persian) giving both query and instruction to the retrieval
models. We make small changes to the narratives and isolate how well retrieval
models can follow these nuanced changes. We present results for both
multilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong
cross-lingual performance with English-based retrievers that trained using
instructions, but find a notable drop in performance in the multilingual
setting, indicating that more work is needed in developing data for
instruction-based multilingual retrievers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emancipatory Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our world today is facing a confluence of several mutually reinforcing crises
each of which intersects with concerns of social justice and emancipation. This
paper is a provocation for the role of computer-mediated information access in
our emancipatory struggles. We define emancipatory information retrieval as the
study and development of information access methods that challenge various
forms of human oppression, and situates its activities within broader
collective emancipatory praxis. The term "emancipatory" here signifies the
moral concerns of universal humanization of all peoples and the elimination of
oppression to create the conditions under which we can collectively flourish.
To develop an emancipatory research agenda for IR, in this paper we speculate
about the practices that the community can adopt, enumerate some of the
projects that the field should undertake, and discuss provocations to spark new
ideas and directions for research. We challenge the field of information
retrieval (IR) research to embrace humanistic values and commit to universal
emancipation and social justice as part of our research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions
in unseen domains without the need for additional training or fine-tuning,
making it particularly valuable in data-sparse environments where traditional
models struggle. Recent advancements in large language models (LLMs) have
greatly improved ZCDSR by leveraging rich pretrained representations to
facilitate cross-domain knowledge transfer. However, a key challenge persists:
domain semantic bias, which arises from variations in vocabulary and content
focus across domains. This misalignment leads to inconsistencies in item
embeddings and hinders generalization.
  To address this issue, we propose a novel framework designed to enhance
LLM-based ZCDSR by improving cross-domain alignment at both the item and
sequential levels. At the item level, we introduce a generalization loss that
promotes inter-domain compactness by aligning embeddings of similar items
across domains while maintaining intra-domain diversity to preserve unique item
characteristics. This prevents embeddings from becoming overly generic while
ensuring effective transferability. At the sequential level, we develop a
method for transferring user behavioral patterns by clustering user sequences
in the source domain and applying attention-based aggregation for target domain
inference. This dynamic adaptation of user embeddings allows effective
zero-shot recommendations without requiring target-domain interactions.
  Comprehensive experiments across multiple datasets and domains demonstrate
that our framework significantly improves sequential recommendation performance
in the ZCDSR setting. By mitigating domain bias and enhancing the
transferability of sequential patterns, our method provides a scalable and
robust approach for achieving more effective zero-shot recommendations across
domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Diffusion Model for Recommender System <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuseok Lee, Yaochen Zhu, Hwanjo Yu, Yao Zhou, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based recommender systems (DR) have gained increasing attention for
their advanced generative and denoising capabilities. However, existing DR face
two central limitations: (i) a trade-off between enhancing generative capacity
via noise injection and retaining the loss of personalized information. (ii)
the underutilization of rich item-side information. To address these
challenges, we present a Collaborative Diffusion model for Recommender System
(CDiff4Rec). Specifically, CDiff4Rec generates pseudo-users from item features
and leverages collaborative signals from both real and pseudo personalized
neighbors identified through behavioral similarity, thereby effectively
reconstructing nuanced user preferences. Experimental results on three public
datasets show that CDiff4Rec outperforms competitors by effectively mitigating
the loss of personalized information through the integration of item content
and collaborative signals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW'25 short</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12433v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12433v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Gao, Bo Chen, Weiwen Liu, Xiangyang Li, Yichao Wang, Wanyu Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking is a critical component in recommender systems, playing an
essential role in refining the output of recommendation algorithms. Traditional
reranking models have focused predominantly on accuracy, but modern
applications demand consideration of additional criteria such as diversity and
fairness. Existing reranking approaches often fail to harmonize these diverse
criteria effectively at the model level. Moreover, these models frequently
encounter challenges with scalability and personalization due to their
complexity and the varying significance of different reranking criteria in
diverse scenarios. In response, we introduce a comprehensive reranking
framework enhanced by LLM, designed to seamlessly integrate various reranking
criteria while maintaining scalability and facilitating personalized
recommendations. This framework employs a fully connected graph structure,
allowing the LLM to simultaneously consider multiple aspects such as accuracy,
diversity, and fairness through a coherent Chain-of-Thought (CoT) process. A
customizable input mechanism is also integrated, enabling the tuning of the
language model's focus to meet specific reranking needs. We validate our
approach using three popular public datasets, where our framework demonstrates
superior performance over existing state-of-the-art reranking models in
balancing multiple criteria. The code for this implementation is publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SampleLLM: Optimizing Tabular Data Synthesis in Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Gao, Zhaocheng Du, Xiaopeng Li, Yichao Wang, Xiangyang Li, Huifeng Guo, Ruiming Tang, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data synthesis is crucial in machine learning, yet existing general
methods-primarily based on statistical or deep learning models-are highly
data-dependent and often fall short in recommender systems. This limitation
arises from their difficulty in capturing complex distributions and
understanding feature relationships from sparse and limited data, along with
their inability to grasp semantic feature relations. Recently, Large Language
Models (LLMs) have shown potential in generating synthetic data samples through
few-shot learning and semantic understanding. However, they often suffer from
inconsistent distribution and lack of diversity due to their inherent
distribution disparity with the target dataset. To address these challenges and
enhance tabular data synthesis for recommendation tasks, we propose a novel
two-stage framework named SampleLLM to improve the quality of LLM-based tabular
data synthesis for recommendations by ensuring better distribution alignment.
In the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and
diverse exemplars to generate data that closely aligns with the target dataset
distribution, even when input samples are limited. The second stage uses an
advanced feature attribution-based importance sampling method to refine feature
relationships within the synthesized data, reducing any distribution biases
introduced by the LLM. Experimental results on three recommendation datasets,
two general datasets, and online deployment illustrate that SampleLLM
significantly surpasses existing methods for recommendation tasks and holds
promise for a broader range of tabular data scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verifying Cross-modal Entity <span class="highlight-title">Consist</span>ency in News using Vision-language
  Models <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Tahmasebi, David Ernst, Eric Müller-Budack, Ralph Ewerth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web has become a crucial source of information, but it is also used to
spread disinformation, often conveyed through multiple modalities like images
and text. The identification of inconsistent cross-modal information, in
particular entities such as persons, locations, and events, is critical to
detect disinformation. Previous works either identify out-of-context
disinformation by assessing the consistency of images to the whole document,
neglecting relations of individual entities, or focus on generic entities that
are not relevant to news. So far, only few approaches have addressed the task
of validating entity consistency between images and text in news. However, the
potential of large vision-language models (LVLMs) has not been explored yet. In
this paper, we propose an LVLM-based framework for verifying Cross-modal Entity
Consistency~(LVLM4CEC), to assess whether persons, locations and events in news
articles are consistent across both modalities. We suggest effective prompting
strategies for LVLMs for entity verification that leverage reference images
crawled from web. Moreover, we extend three existing datasets for the task of
entity verification in news providing manual ground-truth data. Our results
show the potential of LVLMs for automating cross-modal entity verification,
showing improved accuracy in identifying persons and events when using evidence
images. Moreover, our method outperforms a baseline for location and event
verification in documents. The datasets and source code are available on GitHub
at https://github.com/TIBHannover/LVLM4CEC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in: European Conference on Information
  Retrieval (ECIR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream
  Allocation in Feed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10381v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10381v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxin Liu, Xiang Gao, Yisha Li, Xin Li, Haiyang Lu, Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of a short video & live stream mixed recommendation scenario,
the live stream recommendation system (RS) decides whether to allocate at most
one live stream into the video feed for each user request. To maximize
long-term user engagement, it is crucial to determine an optimal live stream
policy for accurate live stream allocation. The inappropriate live stream
allocation policy can significantly affect the duration of the usage app and
user retention, which ignores the long-term negative impact of live stream
allocation. Recently, reinforcement learning (RL) has been widely applied in
recommendation systems to capture long-term user engagement. However,
traditional RL algorithms often face divergence and instability problems, which
restricts the application and deployment in the large-scale industrial
recommendation systems, especially in the aforementioned challenging scenario.
To address these challenges, we propose a novel Supervised Learning-enhanced
Multi-Group Actor Critic algorithm (SL-MGAC). Specifically, we introduce a
supervised learning-enhanced actor-critic framework that incorporates variance
reduction techniques, where multi-task reward learning helps restrict
bootstrapping error accumulation during critic learning. Additionally, we
design a multi-group state decomposition module for both actor and critic
networks to reduce prediction variance and improve model stability. We also
propose a novel reward function to prevent overly greedy live stream
allocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy
evaluation (OPE) and online A/B testing. Experimental results demonstrate that
the proposed method not only outperforms baseline methods under the
platform-level constraints but also exhibits enhanced stability in online
recommendation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikiya Takehi, Ellen M. Voorhees, Tetsuya Sakai, Ian Soboroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test collections are information retrieval tools that allow researchers to
quickly and easily evaluate ranking algorithms. While test collections have
become an integral part of IR research, the process of data creation involves
significant effort in manual annotations, which often makes it very expensive
and time-consuming. Thus, test collections could become too small when the
budget is limited, which may lead to unstable evaluations. As a cheaper
alternative, recent studies have proposed the use of large language models
(LLMs) to completely replace human assessors. However, while LLMs may seem to
somewhat correlate with human judgments, their predictions are not perfect and
often show bias. Thus a complete replacement with LLMs is argued to be too
risky and not fully reliable.
  Thus, in this paper, we propose LLM-Assisted Relevance Assessments (LARA), an
effective method to balance manual annotations with LLM annotations, which
helps to build a rich and reliable test collection even under a low budget. We
use the LLM's predicted relevance probabilities to select the most profitable
documents to manually annotate under a budget constraint. With theoretical
reasoning, LARA effectively guides the human annotation process by actively
learning to calibrate the LLM's predicted relevance probabilities. Then, using
the calibration model learned from the limited manual annotations, LARA
debiases the LLM predictions to annotate the remaining non-assessed data.
Empirical evaluations on TREC-7 Ad Hoc, TREC-8 Ad Hoc, TREC Robust 2004, and
TREC-COVID datasets show that LARA outperforms alternative solutions under
almost any budget constraint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating with Fairness: A Modality-Diffused Counterfactual Framework
  for Incomplete Multimodal Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Li, Shoujin Wang, Qi Zhang, Shui Yu, Fang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete scenario is a prevalent, practical, yet challenging setting in
Multimodal Recommendations (MMRec), where some item modalities are missing due
to various factors. Recently, a few efforts have sought to improve the
recommendation accuracy by exploring generic structures from incomplete data.
However, two significant gaps persist: 1) the difficulty in accurately
generating missing data due to the limited ability to capture modality
distributions; and 2) the critical but overlooked visibility bias, where items
with missing modalities are more likely to be disregarded due to the
prioritization of items' multimodal data over user preference alignment. This
bias raises serious concerns about the fair treatment of items. To bridge these
two gaps, we propose a novel Modality-Diffused Counterfactual (MoDiCF)
framework for incomplete multimodal recommendations. MoDiCF features two key
modules: a novel modality-diffused data completion module and a new
counterfactual multimodal recommendation module. The former, equipped with a
particularly designed multimodal generative framework, accurately generates and
iteratively refines missing data from learned modality-specific distribution
spaces. The latter, grounded in the causal perspective, effectively mitigates
the negative causal effects of visibility bias and thus assures fairness in
recommendations. Both modules work collaboratively to address the two
aforementioned significant gaps for generating more accurate and fair results.
Extensive experiments on three real-world datasets demonstrate the superior
performance of MoDiCF in terms of both recommendation accuracy and fairness.
The code and processed datasets are released at
https://github.com/JinLi-i/MoDiCF.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Rank Adapting Models for Sparse Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Chen, Joshua Engels, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) decompose language model representations into a
sparse set of linear latent vectors. Recent works have improved SAEs using
language model gradients, but these techniques require many expensive backward
passes during training and still cause a significant increase in cross entropy
loss when SAE reconstructions are inserted into the model. In this work, we
improve on these limitations by taking a fundamentally different approach: we
use low-rank adaptation (LoRA) to finetune the language model itself around a
previously trained SAE. We analyze our method across SAE sparsity, SAE width,
language model size, LoRA rank, and model layer on the Gemma Scope family of
SAEs. In these settings, our method reduces the cross entropy loss gap by 30%
to 55% when SAEs are inserted during the forward pass. We also find that
compared to end-to-end (e2e) SAEs, our approach achieves the same downstream
cross entropy loss 3$\times$ to 20$\times$ faster on Gemma-2-2B and 2$\times$
to 10$\times$ faster on Llama-3.2-1B. We further show that our technique
improves downstream metrics and can adapt multiple SAEs at once. Our results
demonstrate that improving model interpretability is not limited to post-hoc
SAE training; Pareto improvements can also be achieved by directly optimizing
the model itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/matchten/LoRA-Models-for-SAEs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingdan Shi, Ren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning seeks to systematically remove specified data from a
trained model, effectively achieving a state as though the data had never been
encountered during training. While metrics such as Unlearning Accuracy (UA) and
Membership Inference Attack (MIA) provide a baseline for assessing unlearning
performance, they fall short of evaluating the completeness and reliability of
forgetting. This is because the ground truth labels remain potential candidates
within the scope of uncertainty quantification, leaving gaps in the evaluation
of true forgetting. In this paper, we identify critical limitations in existing
unlearning metrics and propose enhanced evaluation metrics inspired by
conformal prediction. Our metrics can effectively capture the extent to which
ground truth labels are excluded from the prediction set. Furthermore, we
observe that many existing machine unlearning methods do not achieve
satisfactory forgetting performance when evaluated with our new metrics. To
address this, we propose an unlearning framework that integrates conformal
prediction insights into Carlini & Wagner adversarial attack loss. Extensive
experiments on the image classification task demonstrate that our enhanced
metrics offer deeper insights into unlearning effectiveness, and that our
unlearning framework significantly improves the forgetting quality of
unlearning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection Is All You Need: A Feasible Optimal Prior-Free Black-Box
  Approach For Piecewise Stationary Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Argyrios Gerogiannis, Yu-Han Huang, Subhonmesh Bose, Venugopal V. Veeravalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of piecewise stationary bandits without prior knowledge
of the underlying non-stationarity. We propose the first $\textit{feasible}$
black-box algorithm applicable to most common parametric bandit variants. Our
procedure, termed Detection Augmented Bandit (DAB), is modular, accepting any
stationary bandit algorithm as input and augmenting it with a change detector.
DAB achieves optimal regret in the piecewise stationary setting under mild
assumptions. Specifically, we prove that DAB attains the order-optimal regret
bound of $\tilde{\mathcal{O}}(\sqrt{N_T T})$, where $N_T$ denotes the number of
changes over the horizon $T$, if its input stationary bandit algorithm has
order-optimal stationary regret guarantees. Applying DAB to different
parametric bandit settings, we recover recent state-of-the-art results.
Notably, for self-concordant bandits, DAB achieves optimal dynamic regret,
while previous works obtain suboptimal bounds and require knowledge on the
non-stationarity. In simulations on piecewise stationary environments, DAB
outperforms existing approaches across varying number of changes.
Interestingly, despite being theoretically designed for piecewise stationary
environments, DAB is also effective in simulations in drifting environments,
outperforming existing methods designed specifically for this scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vintix: Action Model via In-Context Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Reinforcement Learning (ICRL) represents a promising paradigm for
developing generalist agents that learn at inference time through
trial-and-error interactions, analogous to how large language models adapt
contextually, but with a focus on reward maximization. However, the scalability
of ICRL beyond toy tasks and single-domain settings remains an open challenge.
In this work, we present the first steps toward scaling ICRL by introducing a
fixed, cross-domain model capable of learning behaviors through in-context
reinforcement learning. Our results demonstrate that Algorithm Distillation, a
framework designed to facilitate ICRL, offers a compelling and competitive
alternative to expert distillation to construct versatile action models. These
findings highlight the potential of ICRL as a scalable approach for generalist
decision-making systems. Code to be released at
https://github.com/dunnolab/vintix
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. In review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable-Softmax Is Superior for Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken M. Nakanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The maximum element of the vector output by the Softmax function approaches
zero as the input vector size increases. Transformer-based language models rely
on Softmax to compute attention scores, causing the attention distribution to
flatten as the context size grows. This reduces the model's ability to
prioritize key information effectively and potentially limits its length
generalization. To address this problem, we propose Scalable-Softmax (SSMax),
which replaces Softmax in scenarios where the input vector size varies. SSMax
can be seamlessly integrated into existing Transformer-based architectures.
Experimental results in language modeling show that models using SSMax not only
achieve faster loss reduction during pretraining but also significantly improve
performance in long contexts and key information retrieval. Furthermore, an
analysis of attention scores reveals that SSMax enables the model to focus
attention on key information even in long contexts. Additionally, although
models that use SSMax from the beginning of pretraining achieve better length
generalization, those that have already started pretraining can still gain some
of this ability by replacing Softmax in the attention layers with SSMax, either
during or after pretraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Strategically Reveal, Conceal, and Infer Information? A
  Theoretical and Empirical Analysis in The Chameleon Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa O. Karabag, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model-based (LLM-based) agents have become common in settings
that include non-cooperative parties. In such settings, agents' decision-making
needs to conceal information from their adversaries, reveal information to
their cooperators, and infer information to identify the other agents'
characteristics. To investigate whether LLMs have these information control and
decision-making capabilities, we make LLM agents play the language-based
hidden-identity game, The Chameleon. In the game, a group of non-chameleon
agents who do not know each other aim to identify the chameleon agent without
revealing a secret. The game requires the aforementioned information control
capabilities both as a chameleon and a non-chameleon. The empirical results
show that while non-chameleon LLM agents identify the chameleon, they fail to
conceal the secret from the chameleon, and their winning probability is far
from the levels of even trivial strategies. To formally explain this behavior,
we give a theoretical analysis for a spectrum of strategies, from concealing to
revealing, and provide bounds on the non-chameleons' winning probability. Based
on the empirical results and theoretical analysis of different strategies, we
deduce that LLM-based non-chameleon agents reveal excessive information to
agents of unknown identities. Our results point to a weakness of contemporary
LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic
interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ s1: Simple test-time scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling is a promising new approach to language modeling that uses
extra test-time compute to improve performance. Recently, OpenAI's o1 model
showed this capability but did not publicly share its methodology, leading to
many replication efforts. We seek the simplest approach to achieve test-time
scaling and strong reasoning performance. First, we curate a small dataset s1K
of 1,000 questions paired with reasoning traces relying on three criteria we
validate through ablations: difficulty, diversity, and quality. Second, we
develop budget forcing to control test-time compute by forcefully terminating
the model's thinking process or lengthening it by appending "Wait" multiple
times to the model's generation when it tries to end. This can lead the model
to double-check its answer, often fixing incorrect reasoning steps. After
supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and
equipping it with budget forcing, our model s1 exceeds o1-preview on
competition math questions by up to 27% (MATH and AIME24). Further, scaling s1
with budget forcing allows extrapolating beyond its performance without
test-time intervention: from 50% to 57% on AIME24. Our model, data, and code
are open-source at https://github.com/simplescaling/s1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages (9 main), 10 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cache Me If You Must: Adaptive Key-Value Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient real-world deployments of large language models (LLMs) rely on
Key-Value (KV) caching for processing and generating long outputs, reducing the
need for repetitive computation. For large contexts, Key-Value caches can take
up tens of gigabytes of device memory, as they store vector representations for
each token and layer. Recent work has shown that the cached vectors can be
compressed through quantization, pruning or merging, but these techniques often
compromise quality towards higher compression rates. In this work, we aim to
improve Key & Value compression by exploiting two observations: 1) the inherent
dependencies between keys and values across different layers, and 2)
high-compression mechanisms for internal network states. We propose AQUA-KV, an
adaptive quantization for Key-Value caches that relies on compact adapters to
exploit existing dependencies between Keys and Values, and aims to "optimally"
compress the information that cannot be predicted. AQUA-KV significantly
improves compression rates, while maintaining high accuracy on state-of-the-art
LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5
bits per value with under $1\%$ relative error in perplexity and LongBench
scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a
single GPU within 1-6 hours, even for 70B models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Seyyedali Hosseinalipour, Christopher G. Brinton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) on devices is attracting increasing
interest. Recent works have fused low-rank adaptation (LoRA) techniques with
federated fine-tuning to mitigate challenges associated with device model sizes
and data scarcity. Still, the heterogeneity of computational resources remains
a critical bottleneck: while higher-rank modules generally enhance performance,
varying device capabilities constrain LoRA's feasible rank range. Existing
approaches attempting to resolve this issue either lack analytical
justification or impose additional computational overhead, leaving a wide gap
for an efficient and theoretically-grounded solution. To address these
challenges, we propose federated sketching LoRA (FSLoRA), which leverages a
sketching mechanism to enable devices to selectively update submatrices of
global LoRA modules maintained by the server. By adjusting the sketching
ratios, which determine the ranks of the submatrices on the devices, FSLoRA
flexibly adapts to device-specific communication and computational constraints.
We provide a rigorous convergence analysis of FSLoRA that characterizes how the
sketching ratios affect the convergence rate. Through comprehensive experiments
on multiple datasets and LLM models, we demonstrate FSLoRA's superior
performance compared to various baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding-based Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyou Song, Dara Bahri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have recently been shown capable of performing regression
tasks wherein numeric predictions are represented as decoded strings. In this
work, we provide theoretical grounds for this capability and furthermore
investigate the utility of causal auto-regressive sequence models when they are
applied to any feature representation. We find that, despite being trained in
the usual way - for next-token prediction via cross-entropy loss -
decoding-based regression is as performant as traditional approaches for
tabular regression tasks, while being flexible enough to capture arbitrary
distributions, such as in the task of density estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Google DeepMind Technical Report, 25 pages. Code can be found at
  https://github.com/google-research/optformer/tree/main/optformer/decoding_regression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using gradient of Lagrangian function to compute efficient channels for
  the ideal observer <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weimin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is widely accepted that the Bayesian ideal observer (IO) should be used to
guide the objective assessment and optimization of medical imaging systems. The
IO employs complete task-specific information to compute test statistics for
making inference decisions and performs optimally in signal detection tasks.
However, the IO test statistic typically depends non-linearly on the image data
and cannot be analytically determined. The ideal linear observer, known as the
Hotelling observer (HO), can sometimes be used as a surrogate for the IO.
However, when image data are high dimensional, HO computation can be difficult.
Efficient channels that can extract task-relevant features have been
investigated to reduce the dimensionality of image data to approximate IO and
HO performance. This work proposes a novel method for generating efficient
channels by use of the gradient of a Lagrangian-based loss function that was
designed to learn the HO. The generated channels are referred to as the
Lagrangian-gradient (L-grad) channels. Numerical studies are conducted that
consider binary signal detection tasks involving various backgrounds and
signals. It is demonstrated that channelized HO (CHO) using L-grad channels can
produce significantly better signal detection performance compared to the CHO
using PLS channels. Moreover, it is shown that the proposed L-grad method can
achieve significantly lower computation time compared to the PLS method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SPIE Medical Imaging 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SELMA: A Speech-Enabled Language Model for Virtual Assistant
  Interactions <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present and evaluate SELMA, a Speech-Enabled Language Model
for virtual Assistant interactions that integrates audio and text as inputs to
a Large Language Model (LLM). SELMA is designed to handle three primary and two
auxiliary tasks related to interactions with virtual assistants simultaneously
within a single end-to-end model. We employ low-rank adaptation modules for
parameter-efficient training of both the audio encoder and the LLM.
Additionally, we implement a feature pooling strategy enabling the system to
recognize global patterns and improve accuracy on tasks less reliant on
individual sequence elements. Experimental results on Voice Trigger (VT)
detection, Device-Directed Speech Detection (DDSD), and Automatic Speech
Recognition (ASR), demonstrate that our approach both simplifies the typical
input processing pipeline of virtual assistants significantly and also improves
performance compared to dedicated models for each individual task. SELMA yields
relative Equal-Error Rate improvements of 64% on the VT detection task, and 22%
on DDSD, while also achieving word error rates close to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixing the Double Penalty in Data-Driven Weather Forecasting Through a
  Modified Spherical Harmonic <span class="highlight-title">Loss</span> Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Subich, Syed Zahid Husain, Leo Separovic, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in data-driven weather forecasting models have delivered
deterministic models that outperform the leading operational forecast systems
based on traditional, physics-based models. However, these data-driven models
are typically trained with a mean squared error loss function, which causes
smoothing of fine scales through a "double penalty" effect. We develop a
simple, parameter-free modification to this loss function that avoids this
problem by separating the loss attributable to decorrelation from the loss
attributable to spectral amplitude errors. Fine-tuning the GraphCast model with
this new loss function results in sharp deterministic weather forecasts, an
increase of the model's effective resolution from 1,250km to 160km,
improvements to ensemble spread, and improvements to predictions of tropical
cyclone strength and surface wind extremes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising
  Diffusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sören Christensen, Claudia Strauch, Lukas Trottner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new class of generative diffusion models that, unlike
conventional denoising diffusion models, achieve a time-homogeneous structure
for both the noising and denoising processes, allowing the number of steps to
adaptively adjust based on the noise level. This is accomplished by
conditioning the forward process using Doob's $h$-transform, which terminates
the process at a suitable sampling distribution at a random time. The model is
particularly well suited for generating data with lower intrinsic dimensions,
as the termination criterion simplifies to a first-hitting rule. A key feature
of the model is its adaptability to the target data, enabling a variety of
downstream tasks using a pre-trained unconditional generative model. These
tasks include natural conditioning through appropriate initialization of the
denoising process and classification of noisy data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoSTI: <span class="highlight-title">Consist</span>ency Models for (a faster) Spatio-Temporal Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time Series Imputation (MTSI) is crucial for many applications,
such as healthcare monitoring and traffic management, where incomplete data can
compromise decision-making. Existing state-of-the-art methods, like Denoising
Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;
however, they suffer from significant computational costs and are notably
time-consuming due to their iterative nature. In this work, we propose CoSTI,
an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI
employs Consistency Training to achieve comparable imputation quality to DDPMs
while drastically reducing inference times, making it more suitable for
real-time applications. We evaluate CoSTI across multiple datasets and missing
data scenarios, demonstrating up to a 98% reduction in imputation time with
performance on par with diffusion-based models. This work bridges the gap
between efficiency and accuracy in generative imputation tasks, providing a
scalable solution for handling missing data in critical spatio-temporal
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ We're Different, We're the Same: Creative Homogeneity Across LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Wenger, Yoed Kenett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous powerful large language models (LLMs) are now available for use as
writing support tools, idea generators, and beyond. Although these LLMs are
marketed as helpful creative assistants, several works have shown that using an
LLM as a creative partner results in a narrower set of creative outputs.
However, these studies only consider the effects of interacting with a single
LLM, begging the question of whether such narrowed creativity stems from using
a particular LLM -- which arguably has a limited range of outputs -- or from
using LLMs in general as creative assistants. To study this question, we elicit
creative responses from humans and a broad set of LLMs using standardized
creativity tests and compare the population-level diversity of responses. We
find that LLM responses are much more similar to other LLM responses than human
responses are to each other, even after controlling for response structure and
other key variables. This finding of significant homogeneity in creative
outputs across the LLMs we evaluate adds a new dimension to the ongoing
conversation about creativity and LLMs. If today's LLMs behave similarly, using
them as a creative partners -- regardless of the model used -- may drive all
users towards a limited set of "creative" outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Energy <span class="highlight-title">Loss</span> Phenomenon in RLHF: A New Perspective on Mitigating
  Reward Hacking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchun Miao, Sen Zhang, Liang Ding, Yuqi Zhang, Lefei Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work identifies the Energy Loss Phenomenon in Reinforcement Learning
from Human Feedback (RLHF) and its connection to reward hacking. Specifically,
energy loss in the final layer of a Large Language Model (LLM) gradually
increases during the RL process, with an excessive increase in energy loss
characterizing reward hacking. Beyond empirical analysis, we further provide a
theoretical foundation by proving that, under mild conditions, the increased
energy loss reduces the upper bound of contextual relevance in LLMs, which is a
critical aspect of reward hacking as the reduced contextual relevance typically
indicates overfitting to reward model-favored patterns in RL. To address this
issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the
increase in energy loss in the LLM's final layer during reward calculation to
prevent excessive energy loss, thereby mitigating reward hacking. We
theoretically show that EPPO can be conceptually interpreted as an
entropy-regularized RL algorithm, which provides deeper insights into its
effectiveness. Extensive experiments across various LLMs and tasks demonstrate
the commonality of the energy loss phenomenon, as well as the effectiveness of
\texttt{EPPO} in mitigating reward hacking and improving RLHF performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Implicit Solution Formula for Efficiently Solving Hamilton-Jacobi
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yesom Park, Stanley Osher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an implicit solution formula for the Hamilton-Jacobi
partial differential equation (HJ PDE). The formula is derived using the method
of characteristics and is shown to coincide with the Hopf and Lax formulas in
the case where either the Hamiltonian or the initial function is convex. It
provides a simple and efficient numerical approach for computing the viscosity
solution of HJ PDEs, bypassing the need for the Legendre transform of the
Hamiltonian or the initial condition, and the explicit computation of
individual characteristic trajectories. A deep learning-based methodology is
proposed to learn this implicit solution formula, leveraging the mesh-free
nature of deep learning to ensure scalability for high-dimensional problems.
Building upon this framework, an algorithm is developed that approximates the
characteristic curves piecewise linearly for state-dependent Hamiltonians.
Extensive experimental results demonstrate that the proposed method delivers
highly accurate solutions, even for nonconvex Hamiltonians, and exhibits
remarkable scalability, achieving computational efficiency for problems up to
40 dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An All-digital 65-nm Tsetlin Machine Image Classification Accelerator
  with 8.6 nJ per MNIST Frame at 60.3k Frames per Second 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Svein Anders Tunheim, Yujin Zheng, Lei Jiao, Rishad Shafik, Alex Yakovlev, Ole-Christoffer Granmo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an all-digital programmable machine learning accelerator chip for
image classification, underpinning on the Tsetlin machine (TM) principles. The
TM is a machine learning algorithm founded on propositional logic, utilizing
sub-pattern recognition expressions called clauses. The accelerator implements
the coalesced TM version with convolution, and classifies booleanized images of
28$\times$28 pixels with 10 categories. A configuration with 128 clauses is
used in a highly parallel architecture. Fast clause evaluation is obtained by
keeping all clause weights and Tsetlin automata (TA) action signals in
registers. The chip is implemented in a 65 nm low-leakage CMOS technology, and
occupies an active area of 2.7mm$^2$. At a clock frequency of 27.8 MHz, the
accelerator achieves 60.3k classifications per second, and consumes 8.6 nJ per
classification. The latency for classifying a single image is 25.4 $\mu$s which
includes system timing overhead. The accelerator achieves 97.42%, 84.54% and
82.55% test accuracies for the datasets MNIST, Fashion-MNIST and
Kuzushiji-MNIST, respectively, matching the TM software models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PUATE: Semiparametric Efficient Average Treatment Effect Estimation from
  Treated (Positive) and Unlabeled Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of average treatment effects (ATEs), defined as the difference
in expected outcomes between treatment and control groups, is a central topic
in causal inference. This study develops semiparametric efficient estimators
for ATE estimation in a setting where only a treatment group and an unknown
group-comprising units for which it is unclear whether they received the
treatment or control-are observable. This scenario represents a variant of
learning from positive and unlabeled data (PU learning) and can be regarded as
a special case of ATE estimation with missing data. For this setting, we derive
semiparametric efficiency bounds, which provide lower bounds on the asymptotic
variance of regular estimators. We then propose semiparametric efficient ATE
estimators whose asymptotic variance aligns with these efficiency bounds. Our
findings contribute to causal inference with missing data and weakly supervised
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covering Multiple Objectives with a Small Set of Solutions Using
  Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Maus, Kyurae Kim, Yimeng Zeng, Haydn Thomas Jones, Fangping Wan, Marcelo Der Torossian Torres, Cesar de la Fuente-Nunez, Jacob R. Gardner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-objective black-box optimization, the goal is typically to find
solutions that optimize a set of T black-box objective functions, $f_1$, ...,
$f_T$, simultaneously. Traditional approaches often seek a single
Pareto-optimal set that balances trade-offs among all objectives. In this work,
we introduce a novel problem setting that departs from this paradigm: finding a
smaller set of K solutions, where K < T, that collectively "covers" the T
objectives. A set of solutions is defined as "covering" if, for each objective
$f_1$, ..., $f_T$, there is at least one good solution. A motivating example
for this problem setting occurs in drug design. For example, we may have T
pathogens and aim to identify a set of K < T antibiotics such that at least one
antibiotic can be used to treat each pathogen. To address this problem, we
propose Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principled
algorithm designed to efficiently find a covering set. We validate our approach
through extensive experiments on challenging high-dimensional tasks, including
applications in peptide and molecular design. Experiments demonstrate MOCOBO's
ability to find high-performing covering sets of solutions. Additionally, we
show that the small sets of K < T solutions found by MOCOBO can match or nearly
match the performance of T individually optimized solutions for the same
objectives. Our results highlight MOCOBO's potential to tackle complex
multi-objective problems in domains where finding at least one high-performing
solution for each objective is critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What is causal about causal models and representations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Hytting Jørgensen, Luigi Gresele, Sebastian Weichwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal Bayesian networks are 'causal' models since they make predictions
about interventional distributions. To connect such causal model predictions to
real-world outcomes, we must determine which actions in the world correspond to
which interventions in the model. For example, to interpret an action as an
intervention on a treatment variable, the action will presumably have to a)
change the distribution of treatment in a way that corresponds to the
intervention, and b) not change other aspects, such as how the outcome depends
on the treatment; while the marginal distributions of some variables may change
as an effect. We introduce a formal framework to make such requirements for
different interpretations of actions as interventions precise. We prove that
the seemingly natural interpretation of actions as interventions is circular:
Under this interpretation, every causal Bayesian network that correctly models
the observational distribution is trivially also interventionally valid, and no
action yields empirical data that could possibly falsify such a model. We prove
an impossibility result: No interpretation exists that is non-circular and
simultaneously satisfies a set of natural desiderata. Instead, we examine
non-circular interpretations that may violate some desiderata and show how this
may in turn enable the falsification of causal models. By rigorously examining
how a causal Bayesian network could be a 'causal' model of the world instead of
merely a mathematical object, our formal framework contributes to the
conceptual foundations of causal representation learning, causal discovery, and
causal abstraction, while also highlighting some limitations of existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Value of Prediction in Identifying the Worst-Off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Unai Fischer-Abaigar, Christoph Kern, Juan Carlos Perdomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is increasingly used in government programs to identify and
support the most vulnerable individuals, prioritizing assistance for those at
greatest risk over optimizing aggregate outcomes. This paper examines the
welfare impacts of prediction in equity-driven contexts, and how they compare
to other policy levers, such as expanding bureaucratic capacity. Through
mathematical models and a real-world case study on long-term unemployment
amongst German residents, we develop a comprehensive understanding of the
relative effectiveness of prediction in surfacing the worst-off. Our findings
provide clear analytical frameworks and practical, data-driven tools that
empower policymakers to make principled decisions when designing these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Pauls, Max Zimmer, Berkant Turan, Sassan Saatchi, Philippe Ciais, Sebastian Pokutta, Fabian Gieseke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise in global greenhouse gas emissions, accurate large-scale tree
canopy height maps are essential for understanding forest structure, estimating
above-ground biomass, and monitoring ecological disruptions. To this end, we
present a novel approach to generate large-scale, high-resolution canopy height
maps over time. Our model accurately predicts canopy height over multiple years
given Sentinel-2 time series satellite data. Using GEDI LiDAR data as the
ground truth for training the model, we present the first 10m resolution
temporal canopy height map of the European continent for the period 2019-2022.
As part of this product, we also offer a detailed canopy height map for 2020,
providing more precise estimates than previous studies. Our pipeline and the
resulting temporal height map are publicly available, enabling comprehensive
large-scale monitoring of forests and, hence, facilitating future research and
ecological analyses. For an interactive viewer, see
https://europetreemap.projects.earthengine.app/view/temporalcanopyheight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages main paper, 5 pages references and appendix, 8 figures, 5
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Bias in <span class="highlight-title">Self-Supervised</span> Learning For Automatic Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Storey, Naomi Harte, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is used in deep learning to train on large
datasets without the need for expensive labelling of the data. Recently, large
Automatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to
train on over one hundred different languages simultaneously. However, deeper
investigation shows that the bulk of the training data for XLS-R comes from a
small number of languages. Biases learned through SSL have been shown to exist
in multiple domains, but language bias in multilingual SSL ASR has not been
thoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis
(LTH) to identify language-specific subnetworks within XLS-R and test the
performance of these subnetworks on a variety of different languages. We are
able to show that when fine-tuning, XLS-R bypasses traditional linguistic
knowledge and builds only on weights learned from the languages with the
largest data contribution to the pretraining data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Speech and Language Technology Workshop (SLT) 2024
  accessible on IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, Jonas Kohler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) is closely linked to their
underlying size, leading to ever-growing networks and hence slower inference.
Speculative decoding has been proposed as a technique to accelerate
autoregressive generation, leveraging a fast draft model to propose candidate
tokens, which are then verified in parallel based on their likelihood under the
target model. While this approach guarantees to reproduce the target output, it
incurs a substantial penalty: many high-quality draft tokens are rejected, even
when they represent objectively valid continuations. Indeed, we show that even
powerful draft models such as GPT-4o, as well as human text cannot achieve high
acceptance rates under the standard verification scheme. This severely limits
the speedup potential of current speculative decoding methods, as an early
rejection becomes overwhelmingly likely when solely relying on alignment of
draft and target.
  We thus ask the following question: Can we adapt verification to recognize
correct, but non-aligned replies? To this end, we draw inspiration from the
LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers
in a versatile way. We carefully design a dataset to elicit the same capability
in the target model by training a compact module on top of the embeddings to
produce ``judgements" of the current continuation. We showcase our strategy on
the Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over
Llama-405B, while maintaining its quality on a large range of benchmarks. These
benefits remain present even in optimized inference frameworks, where our
method reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B
on 2 and 8 H100s respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Learning for Combinatorial Multi-armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xutong Liu, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, Carlee-Joe Wong, John C. S. Lui, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combinatorial multi-armed bandit (CMAB) is a fundamental sequential
decision-making framework, extensively studied over the past decade. However,
existing work primarily focuses on the online setting, overlooking the
substantial costs of online interactions and the readily available offline
datasets. To overcome these limitations, we introduce Off-CMAB, the first
offline learning framework for CMAB. Central to our framework is the
combinatorial lower confidence bound (CLCB) algorithm, which combines
pessimistic reward estimations with combinatorial solvers. To characterize the
quality of offline datasets, we propose two novel data coverage conditions and
prove that, under these conditions, CLCB achieves a near-optimal suboptimality
gap, matching the theoretical lower bound up to a logarithmic factor. We
validate Off-CMAB through practical applications, including learning to rank,
large language model (LLM) caching, and social influence maximization, showing
its ability to handle nonlinear reward functions, general feedback models, and
out-of-distribution action samples that excludes optimal or even feasible
actions. Extensive experiments on synthetic and real-world datasets further
highlight the superior performance of CLCB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic User Behavior Sequence <span class="highlight-title">Generation</span> with Large Language Models
  for Smart Homes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, as smart home systems have become more widespread, security
concerns within these environments have become a growing threat. Currently,
most smart home security solutions, such as anomaly detection and behavior
prediction models, are trained using fixed datasets that are precollected.
However, the process of dataset collection is time-consuming and lacks the
flexibility needed to adapt to the constantly evolving smart home environment.
Additionally, the collection of personal data raises significant privacy
concerns for users. Lately, large language models (LLMs) have emerged as a
powerful tool for a wide range of tasks across diverse application domains,
thanks to their strong capabilities in natural language processing, reasoning,
and problem-solving. In this paper, we propose an LLM-based synthetic dataset
generation IoTGen framework to enhance the generalization of downstream smart
home intelligent models. By generating new synthetic datasets that reflect
changes in the environment, smart home intelligent models can be retrained to
overcome the limitations of fixed and outdated data, allowing them to better
align with the dynamic nature of real-world home environments. Specifically, we
first propose a Structure Pattern Perception Compression (SPPC) method tailored
for IoT behavior data, which preserves the most informative content in the data
while significantly reducing token consumption. Then, we propose a systematic
approach to create prompts and implement data generation to automatically
generate IoT synthetic data with normative and reasonable properties, assisting
task models in adaptive training to improve generalization and real-world
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private In-context Learning via Sampling <span class="highlight-title">Few-shot</span> Mixed
  with Zero-shot Outputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Flemings, Haosheng Gan, Hongyi Li, Meisam Razaviyayn, Murali Annavaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) has shown promising improvement in downstream task
adaptation of LLMs by augmenting prompts with relevant input-output examples
(demonstrations). However, the ICL demonstrations can contain privacy-sensitive
information, which can be leaked and/or regurgitated by the LLM output.
Differential Privacy (DP), a widely adopted privacy safeguard, has emerged to
mitigate this privacy leakage, with recent work demonstrating strong
privacy-utility tradeoffs in classification tasks for ICL. However, generation
tasks for ICL are challenging due to the high-dimensional output space of
open-ended generation. To this end, we propose $\texttt{dps-mozo}$,
Differentially Private Sampling by Mixing One-shot with Zero-shot Outputs, a
decoding framework that generates DP text by sampling from the product of
multiple one-shot outputs mixed with a zero-shot output. This mixing
effectively reduces the amount of information that can be leaked by each
demonstration. By utilizing the inherent randomness in sampling from the mixed
distributions, we can achieve DP without adding noise, thereby improving the
privacy-utility tradeoff. Our experimental evaluations show $\texttt{dps-mozo}$
can achieve a strong privacy guarantee, $\epsilon=2$, with minimal utility
degradation compared to non-private few-shot learning, $\textbf{0.3}$% ROUGE-L
F1 score decrease on the SAMSum dataset with Gemma 2 2B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneBatchPAM: A Fast and Frugal K-Medoids Algorithm <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine de Mathelin, Nicolas Enrique Cecchi, François Deheeger, Mathilde Mougeot, Nicolas Vayatis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel k-medoids approximation algorithm to handle
large-scale datasets with reasonable computational time and memory complexity.
We develop a local-search algorithm that iteratively improves the medoid
selection based on the estimation of the k-medoids objective. A single batch of
size m << n provides the estimation, which reduces the required memory size and
the number of pairwise dissimilarities computations to O(mn), instead of O(n^2)
compared to most k-medoids baselines. We obtain theoretical results
highlighting that a batch of size m = O(log(n)) is sufficient to guarantee,
with strong probability, the same performance as the original local-search
algorithm. Multiple experiments conducted on real datasets of various sizes and
dimensions show that our algorithm provides similar performances as
state-of-the-art methods such as FasterPAM and BanditPAM++ with a drastically
reduced running time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of Generative Adversarial Network (GAN) for Synthetic
  Training Data Creation to improve performance of ANN Classifier for
  extracting Built-Up pixels from Landsat Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amritendu Mukherjee, Dipanwita Sinha Mukherjee, Parthasarathy Ramachandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a neural network for pixel based classification task using low
resolution Landsat images is difficult as the size of the training data is
usually small due to less number of available pixels that represent a single
class without any mixing with other classes. Due to this scarcity of training
data, neural network may not be able to attain expected level of accuracy. This
limitation could be overcome using a generative network that aims to generate
synthetic data having the same distribution as the sample data with which it is
trained. In this work, we have proposed a methodology for improving the
performance of ANN classifier to identify built-up pixels in the Landsat$7$
image with the help of developing a simple GAN architecture that could generate
synthetic training pixels when trained using original set of sample built-up
pixels. To ensure that the marginal and joint distributions of all the bands
corresponding to the generated and original set of pixels are
indistinguishable, non-parametric Kolmogorov Smirnov Test and Ball Divergence
based Equality of Distributions Test have been performed respectively. It has
been observed that the overall accuracy and kappa coefficient of the ANN model
for built-up classification have continuously improved from $0.9331$ to
$0.9983$ and $0.8277$ to $0.9958$ respectively, with the inclusion of generated
sets of built-up pixels to the original one.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Physics of Deep Neural Networks: Generalization Capability,
  Beyond the Infinite Width, and Feature Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastiano Ariosto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassing
human performance. Yet their internal processes remain elusive, frequently
described as "black boxes." While performance can be refined experimentally,
achieving a fundamental grasp of their inner workings is still a challenge.
  Statistical Mechanics has long tackled computational problems, and this
thesis applies physics-based insights to understand DNNs via three
complementary approaches.
  First, by averaging over data, we derive an asymptotic bound on
generalization that depends solely on the size of the last layer, rather than
on the total number of parameters -- revealing how deep architectures process
information differently across layers.
  Second, adopting a data-dependent viewpoint, we explore a finite-width
thermodynamic limit beyond the infinite-width regime. This leads to: (i) a
closed-form expression for the generalization error in a finite-width
one-hidden-layer network (regression task); (ii) an approximate partition
function for deeper architectures; and (iii) a link between deep networks in
this thermodynamic limit and Student's t-processes.
  Finally, from a task-explicit perspective, we present a preliminary analysis
of how DNNs interact with a controlled dataset, investigating whether they
truly internalize its structure -- collapsing to the teacher -- or merely
memorize it. By understanding when a network must learn data structure rather
than just memorize, it sheds light on fostering meaningful internal
representations.
  In essence, this thesis leverages the synergy between Statistical Physics and
Machine Learning to illuminate the inner behavior of DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis (200 pages), divided into four separate chapters, each of
  which can be read independently. Some of the material presented has
  previously appeared in works available on arXiv under the following
  identifiers: 2209.04882 and 2201.11022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S-VOTE: Similarity-based Voting for Client Selection in Decentralized
  Federated Learning <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Miguel Sánchez Sánchez, Enrique Tomás Martínez Beltrán, Chao Feng, Gérôme Bovet, Gregorio Martínez Pérez, Alberto Huertas Celdrán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized Federated Learning (DFL) enables collaborative,
privacy-preserving model training without relying on a central server. This
decentralized approach reduces bottlenecks and eliminates single points of
failure, enhancing scalability and resilience. However, DFL also introduces
challenges such as suboptimal models with non-IID data distributions, increased
communication overhead, and resource usage. Thus, this work proposes S-VOTE, a
voting-based client selection mechanism that optimizes resource usage and
enhances model performance in federations with non-IID data conditions. S-VOTE
considers an adaptive strategy for spontaneous local training that addresses
participation imbalance, allowing underutilized clients to contribute without
significantly increasing resource costs. Extensive experiments on benchmark
datasets demonstrate the S-VOTE effectiveness. More in detail, it achieves
lower communication costs by up to 21%, 4-6% faster convergence, and improves
local performance by 9-17% compared to baseline methods in some configurations,
all while achieving a 14-24% energy consumption reduction. These results
highlight the potential of S-VOTE to address DFL challenges in heterogeneous
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IJCNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Pareto Optimality for the Multinomial Logistic Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Zuo, Hanzhang Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a new online learning algorithm for tackling the Multinomial Logit
Bandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorial
nature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-based
method that achieves Pareto optimality by balancing regret minimization and
estimation error of the assortment revenues and the MNL parameters. We develop
theoretical guarantees characterizing the tradeoff between regret and
estimation error for the MNL-Bandit problem through information-theoretic
bounds, and propose a modified UCB algorithm that incorporates forced
exploration to improve parameter estimation accuracy while maintaining low
regret. Our analysis sheds critical insights into how to optimally balance the
collected revenues and the treatment estimation in dynamic assortment
optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concept-Based Explainable Artificial Intelligence: Metrics and
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based explanation methods, such as concept bottleneck models (CBMs),
aim to improve the interpretability of machine learning models by linking their
decisions to human-understandable concepts, under the critical assumption that
such concepts can be accurately attributed to the network's feature space.
However, this foundational assumption has not been rigorously validated, mainly
because the field lacks standardised metrics and benchmarks to assess the
existence and spatial alignment of such concepts. To address this, we propose
three metrics: the concept global importance metric, the concept existence
metric, and the concept location metric, including a technique for visualising
concept activations, i.e., concept activation mapping. We benchmark post-hoc
CBMs to illustrate their capabilities and challenges. Through qualitative and
quantitative experiments, we demonstrate that, in many cases, even the most
important concepts determined by post-hoc CBMs are not present in input images;
moreover, when they are present, their saliency maps fail to align with the
expected regions by either activating across an entire object or misidentifying
relevant concept-specific regions. We analyse the root causes of these
limitations, such as the natural correlation of concepts. Our findings
underscore the need for more careful application of concept-based explanation
techniques especially in settings where spatial interpretability is critical.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages it total, 8 main pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jackpot! Alignment as a Maximal Lottery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF), the standard for aligning
Large Language Models (LLMs) with human values, is known to fail to satisfy
properties that are intuitively desirable, such as respecting the preferences
of the majority \cite{ge2024axioms}. To overcome these issues, we propose the
use of a probabilistic Social Choice rule called \emph{maximal lotteries} as a
replacement for RLHF. We show that a family of alignment techniques, namely
Nash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants,
approximate maximal lottery outcomes and thus inherit its beneficial
properties.
  We confirm experimentally that our proposed methodology handles situations
that arise when working with preferences more robustly than standard RLHF,
including supporting the preferences of the majority, providing principled ways
of handling non-transitivities in the preference data, and robustness to
irrelevant alternatives. This results in systems that better incorporate human
values and respect human intentions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Semantic Segmentation with Diffusion <span class="highlight-title">Pretrain</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Li, Anvar Kurmukov, Mikhail Goncharov, Roman Sokolov, Mikhail Belyaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have shown that learning robust feature
representations is critical for the success of many computer vision tasks,
including medical image segmentation. In particular, both transformer and
convolutional-based architectures have benefit from leveraging pretext tasks
for pretraining. However, the adoption of pretext tasks in 3D medical imaging
has been less explored and remains a challenge, especially in the context of
learning generalizable feature representations.
  We propose a novel pretraining strategy using diffusion models with
anatomical guidance, tailored to the intricacies of 3D medical image data. We
introduce an auxiliary diffusion process to pretrain a model that produce
generalizable feature representations, useful for a variety of downstream
segmentation tasks. We employ an additional model that predicts 3D universal
body-part coordinates, providing guidance during the diffusion process and
improving spatial awareness in generated representations. This approach not
only aids in resolving localization inaccuracies but also enriches the model's
ability to understand complex anatomical structures.
  Empirical validation on a 13-class organ segmentation task demonstrate the
effectiveness of our pretraining technique. It surpasses existing restorative
pretraining methods in 3D medical image segmentation by $7.5\%$, and is
competitive with the state-of-the-art contrastive pretraining approach,
achieving an average Dice coefficient of 67.8 in a non-linear evaluation
scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mFollowIR: a Multilingual Benchmark for Instruction Following in
  Retrieval <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Sam Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, Dawn Lawrie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval systems generally focus on web-style queries that are short and
underspecified. However, advances in language models have facilitated the
nascent rise of retrieval models that can understand more complex queries with
diverse intents. However, these efforts have focused exclusively on English;
therefore, we do not yet understand how they work across languages. We
introduce mFollowIR, a multilingual benchmark for measuring
instruction-following ability in retrieval models. mFollowIR builds upon the
TREC NeuCLIR narratives (or instructions) that span three diverse languages
(Russian, Chinese, Persian) giving both query and instruction to the retrieval
models. We make small changes to the narratives and isolate how well retrieval
models can follow these nuanced changes. We present results for both
multilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong
cross-lingual performance with English-based retrievers that trained using
instructions, but find a notable drop in performance in the multilingual
setting, indicating that more work is needed in developing data for
instruction-based multilingual retrievers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for
  Autonomous Drone FlighT at the Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amogh Joshi, Sourav Sanyal, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear $Q$-Learning Does Not Diverge: Convergence Rates to a Bounded Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Zixuan Xie, Shangtong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $Q$-learning is one of the most fundamental reinforcement learning
algorithms. Previously, it is widely believed that $Q$-learning with linear
function approximation (i.e., linear $Q$-learning) suffers from possible
divergence. This paper instead establishes the first $L^2$ convergence rate of
linear $Q$-learning to a bounded set. Notably, we do not make any modification
to the original linear $Q$-learning algorithm, do not make any Bellman
completeness assumption, and do not make any near-optimality assumption on the
behavior policy. All we need is an $\epsilon$-softmax behavior policy with an
adaptive temperature. The key to our analysis is the general result of
stochastic approximations under Markovian noise with fast-changing transition
functions. As a side product, we also use this general result to establish the
$L^2$ convergence rate of tabular $Q$-learning with an $\epsilon$-softmax
behavior policy, for which we rely on a novel pseudo-contraction property of
the weighted Bellman optimality operator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering in hyperbolic balls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Jaćimović, Aladin Crnkić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The idea of representations of the data in negatively curved manifolds
recently attracted a lot of attention and gave a rise to the new research
direction named {\it hyperbolic machine learning} (ML). In order to unveil the
full potential of this new paradigm, efficient techniques for data analysis and
statistical modeling in hyperbolic spaces are necessary. In the present paper
rigorous mathematical framework for clustering in hyperbolic spaces is
established. First, we introduce the $k$-means clustering in hyperbolic balls,
based on the novel definition of barycenter. Second, we present the
expectation-maximization (EM) algorithm for learning mixtures of novel
probability distributions in hyperbolic balls. In such a way we lay the
foundation of unsupervised learning in hyperbolic spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-agent Multi-armed Bandit with Fully Heavy-tailed Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Wang, Mengfan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study decentralized multi-agent multi-armed bandits in fully heavy-tailed
settings, where clients communicate over sparse random graphs with heavy-tailed
degree distributions and observe heavy-tailed (homogeneous or heterogeneous)
reward distributions with potentially infinite variance. The objective is to
maximize system performance by pulling the globally optimal arm with the
highest global reward mean across all clients. We are the first to address such
fully heavy-tailed scenarios, which capture the dynamics and challenges in
communication and inference among multiple clients in real-world systems. In
homogeneous settings, our algorithmic framework exploits hub-like structures
unique to heavy-tailed graphs, allowing clients to aggregate rewards and reduce
noises via hub estimators when constructing UCB indices; under $M$ clients and
degree distributions with power-law index $\alpha > 1$, our algorithm attains a
regret bound (almost) of order $O(M^{1 -\frac{1}{\alpha}} \log{T})$. Under
heterogeneous rewards, clients synchronize by communicating with neighbors,
aggregating exchanged estimators in UCB indices; With our newly established
information delay bounds on sparse random graphs, we prove a regret bound of
$O(M \log{T})$. Our results improve upon existing work, which only address
time-invariant connected graphs, or light-tailed dynamics in dense graphs and
rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DINAMO: Dynamic and INterpretable Anomaly MOnitoring for Large-Scale
  Particle Physics Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsenii Gavrikov, Julián García Pardiñas, Alberto Garfagnini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring reliable data collection in large-scale particle physics experiments
demands Data Quality Monitoring (DQM) procedures to detect possible detector
malfunctions and preserve data integrity. Traditionally, this
resource-intensive task has been handled by human shifters that struggle with
frequent changes in operational conditions. We present novel, interpretable,
robust, and scalable DQM algorithms designed to automate anomaly detection in
time-dependent settings. Our approach constructs evolving histogram templates
with built-in uncertainties, featuring both a statistical variant - extending
the classical Exponentially Weighted Moving Average (EWMA) - and a machine
learning (ML)-enhanced version that leverages a transformer encoder for
improved adaptability. Experimental validations on synthetic datasets
demonstrate the high accuracy, adaptability, and interpretability of these
methods, with the statistical variant being commissioned in the LHCb experiment
at the Large Hadron Collider, underscoring its real-world impact. The code used
in this study is available at https://github.com/ArseniiGav/DINAMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hourly Short Term Load Forecasting for Residential Buildings and Energy
  Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksei Kychkin, Georgios C. Chasparis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electricity load consumption may be extremely complex in terms of profile
patterns, as it depends on a wide range of human factors, and it is often
correlated with several exogenous factors, such as the availability of
renewable energy and the weather conditions. The first goal of this paper is to
investigate the performance of a large selection of different types of
forecasting models in predicting the electricity load consumption within the
short time horizon of a day or few hours ahead. Such forecasts may be rather
useful for the energy management of individual residential buildings or small
energy communities. In particular, we introduce persistence models, standard
auto-regressive-based machine learning models, and more advanced deep learning
models. The second goal of this paper is to introduce two alternative modeling
approaches that are simpler in structure while they take into account domain
specific knowledge, as compared to the previously mentioned black-box modeling
techniques. In particular, we consider the persistence-based auto-regressive
model (PAR) and the seasonal persistence-based regressive model (SPR), priorly
introduced by the authors. In this paper, we specifically tailor these models
to accommodate the generation of hourly forecasts. The introduced models and
the induced comparative analysis extend prior work of the authors which was
restricted to day-ahead forecasts. We observed a 15-30% increase in the
prediction accuracy of the newly introduced hourly-based forecasting models
over existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast exact recovery of noisy matrix from few entries: the infinity norm
  approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        BaoLinh Tran, Van Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The matrix recovery (completion) problem, a central problem in data science
and theoretical computer science, is to recover a matrix $A$ from a relatively
small sample of entries.
  While such a task is impossible in general, it has been shown that one can
recover $A$ exactly in polynomial time, with high probability, from a random
subset of entries, under three (basic and necessary) assumptions: (1) the rank
of $A$ is very small compared to its dimensions (low rank), (2) $A$ has
delocalized singular vectors (incoherence), and (3) the sample size is
sufficiently large.
  There are many different algorithms for the task, including convex
optimization by Candes, Tao and Recht (2009), alternating projection by Hardt
and Wooters (2014) and low rank approximation with gradient descent by
Keshavan, Montanari and Oh (2009, 2010).
  In applications, it is more realistic to assume that data is noisy. In this
case, these approaches provide an approximate recovery with small root mean
square error. However, it is hard to transform such approximate recovery to an
exact one.
  Recently, results by Abbe et al. (2017) and Bhardwaj et al. (2023) concerning
approximation in the infinity norm showed that we can achieve exact recovery
even in the noisy case, given that the ground matrix has bounded precision.
Beyond the three basic assumptions above, they required either the condition
number of $A$ is small (Abbe et al.) or the gap between consecutive singular
values is large (Bhardwaj et al.).
  In this paper, we remove these extra spectral assumptions. As a result, we
obtain a simple algorithm for exact recovery in the noisy case, under only
three basic assumptions. This is the first such algorithm. To analyse the
algorithm, we introduce a contour integration argument which is totally
different from all previous methods and may be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Through the Looking Glass: LLM-Based Analysis of AR/VR Android
  Applications Privacy Policies <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulaziz Alghamdi, David Mohaisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \begin{abstract} This paper comprehensively analyzes privacy policies in
AR/VR applications, leveraging BERT, a state-of-the-art text classification
model, to evaluate the clarity and thoroughness of these policies. By comparing
the privacy policies of AR/VR applications with those of free and premium
websites, this study provides a broad perspective on the current state of
privacy practices within the AR/VR industry. Our findings indicate that AR/VR
applications generally offer a higher percentage of positive segments than free
content but lower than premium websites. The analysis of highlighted segments
and words revealed that AR/VR applications strategically emphasize critical
privacy practices and key terms. This enhances privacy policies' clarity and
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; appeared in ICMLA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ \underline{E2}Former: A Linear-time \underline{E}fficient and
  \underline{E}quivariant Trans\underline{former} for Scalable Molecular
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunyang Li, Lin Huang, Zhihao Ding, Chu Wang, Xinran Wei, Han Yang, Zun Wang, Chang Liu, Yu Shi, Peiran Jin, Jia Zhang, Mark Gerstein, Tao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant Graph Neural Networks (EGNNs) have demonstrated significant
success in modeling microscale systems, including those in chemistry, biology
and materials science. However, EGNNs face substantial computational challenges
due to the high cost of constructing edge features via spherical tensor
products, making them impractical for large-scale systems. To address this
limitation, we introduce E2Former, an equivariant and efficient transformer
architecture that incorporates the Wigner $6j$ convolution (Wigner $6j$ Conv).
By shifting the computational burden from edges to nodes, the Wigner $6j$ Conv
reduces the complexity from $O(|\mathcal{E}|)$ to $ O(| \mathcal{V}|)$ while
preserving both the model's expressive power and rotational equivariance. We
show that this approach achieves a 7x-30x speedup compared to conventional
$\mathrm{SO}(3)$ convolutions. Furthermore, our empirical results demonstrate
that the derived E2Former mitigates the computational challenges of existing
approaches without compromising the ability to capture detailed geometric
information. This development could suggest a promising direction for scalable
and efficient molecular modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strassen Attention: Unlocking Compositional Abilities in <span class="highlight-title">Transformer</span>s
  Based on a New Lower Bound Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kozachinskiy, Felipe Urrutia, Hector Jimenez, Tomasz Steifer, Germán Pizarro, Matías Fuentes, Francisco Meza, Cristian Buc, Cristóbal Rojas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method to evaluate the theoretical limits of Transformers,
allowing us to prove the first lower bounds against one-layer softmax
Transformers with infinite precision. We establish those bounds for three tasks
that require advanced reasoning. The first task, Match3 (Sanford et al., 2023),
requires looking at all triples of positions. The second and third tasks
address compositionality-based reasoning: one is composition of functions (Peng
et al., 2024) and the other is composition of binary relations. We formally
prove the inability of one-layer softmax Transformers to solve any of these
tasks. In an attempt to overcome these limitations, we introduce Strassen
attention and prove that with this mechanism a one-layer Transformer can in
principle solve all these tasks. We also show that it enjoys sub-cubic
running-time complexity, making it more scalable than similar previously
proposed mechanisms, such as higher-order attention (Sanford et al., 2023). To
complement our theoretical findings, we experimentally studied Strassen
attention and compared it against standard (Vaswani et al, 2017), higher-order
attention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021).
Our results help to disentangle all these attention mechanisms, highlighting
their strengths and limitations. In particular, Strassen attention outperforms
standard attention significantly on all the tasks. Altogether, understanding
the theoretical limitations can guide research towards scalable attention
mechanisms that improve the reasoning abilities of Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A single-loop SPIDER-type stochastic subgradient method for
  expectation-constrained nonconvex nonsmooth optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Yangyang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world problems, such as those with fairness constraints, involve
complex expectation constraints and large datasets, necessitating the design of
efficient stochastic methods to solve them. Most existing research focuses on
cases with no {constraint} or easy-to-project constraints or deterministic
constraints. In this paper, we consider nonconvex nonsmooth stochastic
optimization problems with expectation constraints, for which we build a novel
exact penalty model. We first show the relationship between the penalty model
and the original problem. Then on solving the penalty problem, we present a
single-loop SPIDER-type stochastic subgradient method, which utilizes the
subgradients of both the objective and constraint functions, as well as the
constraint function value at each iteration. Under certain regularity
conditions (weaker than Slater-type constraint qualification or strong
feasibility assumed in existing works), we establish an iteration complexity
result of $O(\epsilon^{-4})$ to reach a near-$\epsilon$ stationary point of the
penalized problem in expectation, matching the lower bound for such tasks.
Building on the exact penalization, an $(\epsilon,\epsilon)$-KKT point of the
original problem is obtained. For a few scenarios, our complexity of either the
{objective} sample subgradient or the constraint sample function values can be
lower than the state-of-the-art results by a factor of $\epsilon^{-2}$.
Moreover, on solving two fairness-constrained problems, our method is
significantly (up to 466 times) faster than the state-of-the-art algorithms,
including switching subgradient method and inexact proximal point methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Key word: stochastic, subgradient, expectation constraints, weakly
  convex, fairness constrained classification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning While Repositioning in On-Demand Vehicle Sharing Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansheng Jiang, Chunlin Sun, Zuo-Jun Max Shen, Shunan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a network inventory problem motivated by one-way, on-demand
vehicle sharing services. Due to uncertainties in both demand and returns, as
well as a fixed number of rental units across an $n$-location network, the
service provider must periodically reposition vehicles to match supply with
demand spatially while minimizing costs. The optimal repositioning policy under
a general $n$-location network is intractable without knowing the optimal value
function. We introduce the best base-stock repositioning policy as a
generalization of the classical inventory control policy to $n$ dimensions, and
establish its asymptotic optimality in two distinct limiting regimes under
general network structures. We present reformulations to efficiently compute
this best base-stock policy in an offline setting with pre-collected data.
  In the online setting, we show that a natural Lipschitz-bandit approach
achieves a regret guarantee of $\widetilde{O}(T^{\frac{n}{n+1}})$, which
suffers from the exponential dependence on $n$. We illustrate the challenges of
learning with censored data in networked systems through a regret lower bound
analysis and by demonstrating the suboptimality of alternative algorithmic
approaches. Motivated by these challenges, we propose an Online Gradient
Repositioning algorithm that relies solely on censored demand. Under a mild
cost-structure assumption, we prove that it attains an optimal regret of
$O(n^{2.5} \sqrt{T})$, which matches the regret lower bound in $T$ and achieves
only polynomial dependence on $n$. The key algorithmic innovation involves
proposing surrogate costs to disentangle intertemporal dependencies and
leveraging dual solutions to find the gradient of policy change. Numerical
experiments demonstrate the effectiveness of our proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Sheaf Laplacian Optimizing Restriction Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Di Nino, Sergio Barbarossa, Paolo Di Lorenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to propose a novel framework to infer the sheaf
Laplacian, including the topology of a graph and the restriction maps, from a
set of data observed over the nodes of a graph. The proposed method is based on
sheaf theory, which represents an important generalization of graph signal
processing. The learning problem aims to find the sheaf Laplacian that
minimizes the total variation of the observed data, where the variation over
each edge is also locally minimized by optimizing the associated restriction
maps. Compared to alternative methods based on semidefinite programming, our
solution is significantly more numerically efficient, as all its fundamental
steps are resolved in closed form. The method is numerically tested on data
consisting of vectors defined over subspaces of varying dimensions at each
node. We demonstrate how the resulting graph is influenced by two key factors:
the cross-correlation and the dimensionality difference of the data residing on
the graph's nodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proc. 58th Annual Asilomar Conference on Signals, Systems, and
  Computers (Asilomar), Pacific Grove, CA, Oct. 27 - Oct. 30, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RIGNO: A Graph-based framework for robust and accurate operator learning
  for PDEs on arbitrary domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Mousavi, Shizheng Wen, Levi Lingsch, Maximilian Herde, Bogdan Raonić, Siddhartha Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning the solution operators of PDEs on arbitrary domains is challenging
due to the diversity of possible domain shapes, in addition to the often
intricate underlying physics. We propose an end-to-end graph neural network
(GNN) based neural operator to learn PDE solution operators from data on point
clouds in arbitrary domains. Our multi-scale model maps data between
input/output point clouds by passing it through a downsampled regional mesh.
Many novel elements are also incorporated to ensure resolution invariance and
temporal continuity. Our model, termed RIGNO, is tested on a challenging suite
of benchmarks, composed of various time-dependent and steady PDEs defined on a
diverse set of domains. We demonstrate that RIGNO is significantly more
accurate than neural operator baselines and robustly generalizes to unseen
spatial resolutions and time instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Reasoning</span> with Hidden Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning has become a powerful framework for
improving complex problem-solving capabilities in Multimodal Large Language
Models (MLLMs). However, the verbose nature of textual reasoning introduces
significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as
hidden llama), an efficient reasoning framework that leverages reasoning CoTs
at hidden latent space. We design the Heima Encoder to condense each
intermediate CoT into a compact, higher-level hidden representation using a
single thinking token, effectively minimizing verbosity and reducing the
overall number of tokens required during the reasoning process. Meanwhile, we
design corresponding Heima Decoder with traditional Large Language Models
(LLMs) to adaptively interpret the hidden representations into variable-length
textual sequence, reconstructing reasoning processes that closely resemble the
original CoTs. Experimental results across diverse reasoning MLLM benchmarks
demonstrate that Heima model achieves higher generation efficiency while
maintaining or even better zero-shot task accuracy. Moreover, the effective
reconstruction of multimodal reasoning processes with Heima Decoder validates
both the robustness and interpretability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Variational Perspective on Generative Protein Fitness Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler, Michael Krauthammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of protein fitness optimization is to discover new protein variants
with enhanced fitness for a given use. The vast search space and the sparsely
populated fitness landscape, along with the discrete nature of protein
sequences, pose significant challenges when trying to determine the gradient
towards configurations with higher fitness. We introduce Variational Latent
Generative Protein Optimization (VLGPO), a variational perspective on fitness
optimization. Our method embeds protein sequences in a continuous latent space
to enable efficient sampling from the fitness distribution and combines a
(learned) flow matching prior over sequence mutations with a fitness predictor
to guide optimization towards sequences with high fitness. VLGPO achieves
state-of-the-art results on two different protein benchmarks of varying
complexity. Moreover, the variational design with explicit prior and likelihood
functions offers a flexible plug-and-play framework that can be easily
customized to suit various protein design tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Early Stopping: Refine, Then Calibrate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugène Berta, David Holzmüller, Michael I. Jordan, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning classifiers often produce probabilistic predictions that are
critical for accurate and interpretable decision-making in various domains. The
quality of these predictions is generally evaluated with proper losses like
cross-entropy, which decompose into two components: calibration error assesses
general under/overconfidence, while refinement error measures the ability to
distinguish different classes. In this paper, we provide theoretical and
empirical evidence that these two errors are not minimized simultaneously
during training. Selecting the best training epoch based on validation loss
thus leads to a compromise point that is suboptimal for both calibration error
and, most importantly, refinement error. To address this, we introduce a new
metric for early stopping and hyperparameter tuning that makes it possible to
minimize refinement error during training. The calibration error is minimized
after training, using standard techniques. Our method integrates seamlessly
with any architecture and consistently improves performance across diverse
classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position: Curvature Matrices Should Be Democratized via Linear Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Dangel, Runa Eschenhagen, Weronika Ormaniec, Andres Fernandez, Lukas Tatzel, Agustinus Kristiadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured large matrices are prevalent in machine learning. A particularly
important class is curvature matrices like the Hessian, which are central to
understanding the loss landscape of neural nets (NNs), and enable second-order
optimization, uncertainty quantification, model pruning, data attribution, and
more. However, curvature computations can be challenging due to the complexity
of automatic differentiation, and the variety and structural assumptions of
curvature proxies, like sparsity and Kronecker factorization. In this position
paper, we argue that linear operators -- an interface for performing
matrix-vector products -- provide a general, scalable, and user-friendly
abstraction to handle curvature matrices. To support this position, we
developed $\textit{curvlinops}$, a library that provides curvature matrices
through a unified linear operator interface. We demonstrate with
$\textit{curvlinops}$ how this interface can hide complexity, simplify
applications, be extensible and interoperable with other libraries, and scale
to large NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comunication Framework for Compositional <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Elberg, Mircea Petrache, Denis Parra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositionality and compositional generalization--the ability to understand
novel combinations of known concepts--are central characteristics of human
language and are hypothesized to be essential for human cognition. In machine
learning, the emergence of this property has been studied in a communication
game setting, where independent agents (a sender and a receiver) converge to a
shared encoding policy from a set of states to a space of discrete messages,
where the receiver can correctly reconstruct the states observed by the sender
using only the sender's messages. The use of communication games in generation
tasks is still largely unexplored, with recent methods for compositional
generation focusing mainly on the use of supervised guidance (either through
class labels or text). In this work, we take the first steps to fill this gap,
and we present a self-supervised generative communication game-based framework
for creating compositional encodings in learned representations from
pre-trained encoder-decoder models. In an Iterated Learning (IL) protocol
involving a sender and a receiver, we apply alternating pressures for
compression and diversity of encoded discrete messages, so that the protocol
converges to an efficient but unambiguous encoding. Approximate message entropy
regularization is used to favor compositional encodings. Our framework is based
on rigorous justifications and proofs of defining and balancing the concepts of
Eficiency, Unambiguity and Non-Holisticity in encoding. We test our method on
the compositional image dataset Shapes3D, demonstrating robust performance in
both reconstruction and compositionality metrics, surpassing other tested
discrete message frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Non-Local Molecular Interactions via Equivariant Local
  Representations and Charge Equilibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Fuchs, Michał Sanocki, Julija Zavadlav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Network (GNN) potentials relying on chemical locality offer
near-quantum mechanical accuracy at significantly reduced computational costs.
By propagating local information to distance particles, Message-passing neural
networks (MPNNs) extend the locality concept to model interactions beyond their
local neighborhood. Still, this locality precludes modeling long-range effects,
such as charge transfer, electrostatic interactions, and dispersion effects,
which are critical to adequately describe many real-world systems. In this
work, we propose the Charge Equilibration Layer for Long-range Interactions
(CELLI) to address the challenging modeling of non-local interactions and the
high computational cost of MPNNs. This novel architecture generalizes the
fourth-generation high-dimensional neural network (4GHDNN) concept, integrating
the charge equilibration (Qeq) method into a model-agnostic building block for
modern equivariant GNN potentials. A series of benchmarks show that CELLI can
extend the strictly local Allegro architecture to model highly non-local
interactions and charge transfer. Our architecture generalizes to diverse
datasets and large structures, achieving an accuracy comparable to MPNNs at
about twice the computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Foundations without Foundations -- Why semi-mechanistic models are
  essential for regulatory biology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luka Kovačević, Thomas Gaudelet, James Opzoomer, Hagen Triendl, John Whittaker, Caroline Uhler, Lindsay Edwards, Jake P. Taylor-King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite substantial efforts, deep learning has not yet delivered a
transformative impact on elucidating regulatory biology, particularly in the
realm of predicting gene expression profiles. Here, we argue that genuine
"foundation models" of regulatory biology will remain out of reach unless
guided by frameworks that integrate mechanistic insight with principled
experimental design. We present one such ground-up, semi-mechanistic framework
that unifies perturbation-based experimental designs across both in vitro and
in vivo CRISPR screens, accounting for differentiating and non-differentiating
cellular systems. By revealing previously unrecognised assumptions in published
machine learning methods, our approach clarifies links with popular techniques
such as variational autoencoders and structural causal models. In practice,
this framework suggests a modified loss function that we demonstrate can
improve predictive performance, and further suggests an error analysis that
informs batching strategies. Ultimately, since cellular regulation emerges from
innumerable interactions amongst largely uncharted molecular components, we
contend that systems-level understanding cannot be achieved through structural
biology alone. Instead, we argue that real progress will require a
first-principles perspective on how experiments capture biological phenomena,
how data are generated, and how these processes can be reflected in more
faithful modelling architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSyDUCK: Training-Free Steganography for Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgia Channing, Aqib Mahfuz, Mark van der Wilk, Philip Torr, Fabio Pizzati, Christian Schroeder de Witt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in AI-generated steganography highlight its potential for
safeguarding the privacy of vulnerable democratic actors, including aid
workers, journalists, and whistleblowers operating in oppressive regimes. In
this work, we address current limitations and establish the foundations for
large-throughput generative steganography. We introduce a novel approach that
enables secure and efficient steganography within latent diffusion models. We
show empirically that our methods perform well across a variety of open-source
latent diffusion models, particularly in generative image and video tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locality-aware Surrogates for Gradient-based Black-box Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Momeni, Stefan Uhlich, Arun Venkitaraman, Chia-Yu Hsieh, Andrea Bonetti, Ryoga Matsuo, Eisaku Ohbuchi, Lorenzo Servadei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In physics and engineering, many processes are modeled using
non-differentiable black-box simulators, making the optimization of such
functions particularly challenging. To address such cases, inspired by the
Gradient Theorem, we propose locality-aware surrogate models for active
model-based black-box optimization. We first establish a theoretical connection
between gradient alignment and the minimization of a Gradient Path Integral
Equation (GradPIE) loss, which enforces consistency of the surrogate's
gradients in local regions of the design space. Leveraging this theoretical
insight, we develop a scalable training algorithm that minimizes the GradPIE
loss, enabling both offline and online learning while maintaining computational
efficiency. We evaluate our approach on three real-world tasks - spanning
automated in silico experiments such as coupled nonlinear oscillators, analog
circuits, and optical systems - and demonstrate consistent improvements in
optimization efficiency under limited query budgets. Our results offer
dependable solutions for both offline and online optimization tasks where
reliable gradient estimation is needed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A theoretical framework for overfitting in energy-based modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Catania, Aurélien Decelle, Cyril Furtlehner, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the impact of limited data on training pairwise energy-based
models for inverse problems aimed at identifying interaction networks.
Utilizing the Gaussian model as testbed, we dissect training trajectories
across the eigenbasis of the coupling matrix, exploiting the independent
evolution of eigenmodes and revealing that the learning timescales are tied to
the spectral decomposition of the empirical covariance matrix. We see that
optimal points for early stopping arise from the interplay between these
timescales and the initial conditions of training. Moreover, we show that
finite data corrections can be accurately modeled through asymptotic random
matrix theory calculations and provide the counterpart of generalized
cross-validation in the energy based model context. Our analytical framework
extends to binary-variable maximum-entropy pairwise models with minimal
variations. These findings offer strategies to control overfitting in
discrete-variable models through empirical shrinkage corrections, improving the
management of overfitting in energy-based generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures (including appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Training Scaling for Chemical Exploration in Drug Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgan Thomas, Albert Bou, Gianni De Fabritiis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chemical language models for molecular design have the potential to find
solutions to multi-parameter optimization problems in drug discovery via
reinforcement learning (RL). A key requirement to achieve this is the capacity
to "search" chemical space to identify all molecules of interest. Here, we
propose a challenging new benchmark to discover dissimilar molecules that
possess similar bioactivity, a common scenario in drug discovery, but a hard
problem to optimize. We show that a population of RL agents can solve the
benchmark, while a single agent cannot. We also find that cooperative
strategies are not significantly better than independent agents. Moreover, the
performance on the benchmark scales log-linearly with the number of independent
agents, showing a test-time training scaling law for chemical language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the inductive bias of infinite-depth ResNets and the bottleneck rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enric Boix-Adsera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We compute the minimum-norm weights of a deep linear ResNet, and find that
the inductive bias of this architecture lies between minimizing nuclear norm
and rank. This implies that, with appropriate hyperparameters, deep nonlinear
ResNets have an inductive bias towards minimizing bottleneck rank.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Multi-Label Contrastive Learning by Leveraging Label
  Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Chen, Shen-Huan Lyu, Tian-Shuang Wu, Yanyan Wang, Bin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-label learning, leveraging contrastive learning to learn better
representations faces a key challenge: selecting positive and negative samples
and effectively utilizing label information. Previous studies selected positive
and negative samples based on the overlap between labels and used them for
label-wise loss balancing. However, these methods suffer from a complex
selection process and fail to account for the varying importance of different
labels. To address these problems, we propose a novel method that improves
multi-label contrastive learning through label distribution. Specifically, when
selecting positive and negative samples, we only need to consider whether there
is an intersection between labels. To model the relationships between labels,
we introduce two methods to recover label distributions from logical labels,
based on Radial Basis Function (RBF) and contrastive loss, respectively. We
evaluate our method on nine widely used multi-label datasets, including image
and vector datasets. The results demonstrate that our method outperforms
state-of-the-art methods in six evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Metric for the Balance of Information in Graph Learning <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex O. Davies, Nirav S. Ajmeri, Telmo de Menezes e Silva Filho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning on molecules makes use of information from both the molecular
structure and the features attached to that structure. Much work has been
conducted on biasing either towards structure or features, with the aim that
bias bolsters performance. Identifying which information source a dataset
favours, and therefore how to approach learning that dataset, is an open issue.
Here we propose Noise-Noise Ratio Difference (NNRD), a quantitative metric for
whether there is more useful information in structure or features. By employing
iterative noising on features and structure independently, leaving the other
intact, NNRD measures the degradation of information in each. We employ NNRD
over a range of molecular tasks, and show that it corresponds well to a loss of
information, with intuitive results that are more expressive than simple
performance aggregates. Our future work will focus on expanding data domains,
tasks and types, as well as refining our choice of baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the 4th Annual AAAI Workshop on AI to Accelerate
  Science and Engineering (AI2ASE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burcu Küçükoğlu, Sander Dalm, Marcel van Gerven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of credit assignment in reinforcement learning (RL) when
dealing with high-dimensional data is influenced by the success of
representation learning via deep neural networks, and has implications for the
sample efficiency of deep RL algorithms. Input decorrelation has been
previously introduced as a method to speed up optimization in neural networks,
and has proven impactful in both efficient deep learning and as a method for
effective representation learning for deep RL algorithms. We propose a novel
approach to online decorrelation in deep RL based on the decorrelated
backpropagation algorithm that seamlessly integrates the decorrelation process
into the RL training pipeline. Decorrelation matrices are added to each layer,
which are updated using a separate decorrelation learning rule that minimizes
the total decorrelation loss across all layers, in parallel to minimizing the
usual RL loss. We used our approach in combination with the soft actor-critic
(SAC) method, which we refer to as decorrelated soft actor-critic (DSAC).
Experiments on the Atari 100k benchmark with DSAC shows, compared to the
regular SAC baseline, faster training in five out of the seven games tested and
improved reward performance in two games with around 50% reduction in
wall-clock time, while maintaining performance levels on the other games. These
results demonstrate the positive impact of network-wide decorrelation in deep
RL for speeding up its sample efficiency through more effective credit
assignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyun Li, Wenjie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world scenarios, reward signal for agents are exceedingly
sparse, making it challenging to learn an effective reward function for reward
shaping. To address this issue, our approach performs reward shaping not only
by utilizing non-zero-reward transitions but also by employing the
Semi-Supervised Learning (SSL) technique combined with a novel data
augmentation to learn trajectory space representations from the majority of
transitions, zero-reward transitions, thereby improving the efficacy of reward
shaping. Experimental results in Atari and robotic manipulation demonstrate
that our method effectively generalizes reward shaping to sparse reward
scenarios, achieving up to four times better performance in reaching higher
best scores compared to curiosity-driven methods. The proposed double entropy
data augmentation enhances performance, showcasing a 15.8\% increase in best
score over other augmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Huang, Hai Yang, Yuan Chen, Jiaxun Ye, Dapeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training across
distributed clients without data sharing, but its high computational and
communication demands strain resource-constrained devices. While existing
methods use dynamic pruning to improve efficiency by periodically adjusting
sparse model topologies while maintaining sparsity, these approaches suffer
from issues such as greedy adjustments, unstable topologies, and communication
inefficiency, resulting in less robust models and suboptimal performance under
data heterogeneity and partial client availability. To address these
challenges, we propose Federated Robust pruning via combinatorial Thompson
Sampling (FedRTS), a novel framework designed to develop robust sparse models.
FedRTS enhances robustness and performance through its Thompson Sampling-based
Adjustment (TSAdj) mechanism, which uses probabilistic decisions informed by
stable, farsighted information instead of deterministic decisions reliant on
unstable and myopic information in previous methods. Extensive experiments
demonstrate that FedRTS achieves state-of-the-art performance in computer
vision and natural language processing tasks while reducing communication
costs, particularly excelling in scenarios with heterogeneous data
distributions and partial client participation. Our codes are available at:
https://github.com/Little0o0/FedRTS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Theoretical Justification for Asymmetric Actor-Critic Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaspard Lambrechts, Damien Ernst, Aditya Mahajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning for partially observable environments, many
successful algorithms were developed within the asymmetric learning paradigm.
This paradigm leverages additional state information available at training time
for faster learning. Although the proposed learning objectives are usually
theoretically sound, these methods still lack a theoretical justification for
their potential benefits. We propose such a justification for asymmetric
actor-critic algorithms with linear function approximators by adapting a
finite-time convergence analysis to this setting. The resulting finite-time
bound reveals that the asymmetric critic eliminates an error term arising from
aliasing in the agent state.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 29 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Principal Components for Neural Network Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhan Phan, Thu Nguyen, Pål Halvorsen, Michael A. Riegler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal Component Analysis (PCA) is a commonly used tool for dimension
reduction and denoising. Therefore, it is also widely used on the data prior to
training a neural network. However, this approach can complicate the
explanation of explainable AI (XAI) methods for the decision of the model. In
this work, we analyze the potential issues with this approach and propose
Principal Components-based Initialization (PCsInit), a strategy to incorporate
PCA into the first layer of a neural network via initialization of the first
layer in the network with the principal components, and its two variants
PCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct
and straightforward as for neural networks and are simpler than using PCA prior
to training a neural network on the principal components. Moreover, as will be
illustrated in the experiments, such training strategies can also allow further
improvement of training via backpropagation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain-inspired sparse training enables <span class="highlight-title">Transformer</span>s and LLMs to perform
  as fully connected 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Zhang, Jialin Zhao, Wenjing Wu, Ziheng Liao, Umberto Michieli, Carlo Vittorio Cannistraci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to enlarge our current knowledge on application of
brain-inspired network science principles for training artificial neural
networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can
reduce the computational demands in ANNs, but faces difficulties to keep peak
performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a
brain-inspired method for growing connectivity in DST. CHT leverages a
gradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%
connectivity or lower) advantage across various tasks compared to fully
connected networks. Yet, CHT suffers two main drawbacks: (i) its time
complexity is O(Nd^3) - N node network size, d node degree - hence it can apply
only to ultra-sparse networks. (ii) it selects top link prediction scores,
which is inappropriate for the early training epochs, when the network presents
unreliable connections. We propose a GPU-friendly approximation of the CH link
predictor, which reduces the computational complexity to O(N^3), enabling a
fast implementation of CHT in large-scale models. We introduce the
Cannistraci-Hebb training soft rule (CHTs), which adopts a strategy for
sampling connections in both link removal and regrowth, balancing the
exploration and exploitation of network topology. To improve performance, we
integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results
show that, using 1% of connections, CHTs outperforms fully connected networks
in MLP on visual classification tasks, compressing some networks to < 30%
nodes. Using 5% of the connections, CHTss outperforms fully connected networks
in two Transformer-based machine translation tasks. Using 30% of the
connections, CHTss achieves superior performance compared to other dynamic
sparse training methods in language modeling, and it surpasses the fully
connected counterpart in zero-shot evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the
  Squared <span class="highlight-title">Loss</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijeet Mulgund, Chirag Pabbaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paradigm of weak-to-strong generalization constitutes the training of a
strong AI model on data labeled by a weak AI model, with the goal that the
strong model nevertheless outperforms its weak supervisor on the target task of
interest. For the setting of real-valued regression with the squared loss,
recent work quantitatively characterizes the gain in performance of the strong
model over the weak model in terms of the misfit between the strong and weak
model. We generalize such a characterization to learning tasks whose loss
functions correspond to arbitrary Bregman divergences when the strong class is
convex. This extends the misfit-based characterization of performance gain in
weak-to-strong generalization to classification tasks, as the cross-entropy
loss can be expressed in terms of a Bregman divergence. In most practical
scenarios, however, the strong model class may not be convex. We therefore
weaken this assumption and study weak-to-strong generalization for convex
combinations of $k$ strong models in the strong class, in the concrete setting
of classification. This allows us to obtain a similar misfit-based
characterization of performance gain, upto an additional error term that
vanishes as $k$ gets large. Our theoretical findings are supported by thorough
experiments on synthetic as well as real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Collapse Beyond the Unconstrainted Features Model: Landscape,
  Dynamics, and Generalization in the Mean-Field Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diyuan Wu, Marco Mondelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Collapse is a phenomenon where the last-layer representations of a
well-trained neural network converge to a highly structured geometry. In this
paper, we focus on its first (and most basic) property, known as NC1: the
within-class variability vanishes. While prior theoretical studies establish
the occurrence of NC1 via the data-agnostic unconstrained features model, our
work adopts a data-specific perspective, analyzing NC1 in a three-layer neural
network, with the first two layers operating in the mean-field regime and
followed by a linear layer. In particular, we establish a fundamental
connection between NC1 and the loss landscape: we prove that points with small
empirical loss and gradient norm (thus, close to being stationary)
approximately satisfy NC1, and the closeness to NC1 is controlled by the
residual loss and gradient norm. We then show that (i) gradient flow on the
mean squared error converges to NC1 solutions with small empirical loss, and
(ii) for well-separated data distributions, both NC1 and vanishing test loss
are achieved simultaneously. This aligns with the empirical observation that
NC1 emerges during training while models attain near-zero test error. Overall,
our results demonstrate that NC1 arises from gradient training due to the
properties of the loss landscape, and they show the co-occurrence of NC1 and
small test error for certain data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning on Reconfigurable Hardware: Overcoming Material
  Variability in Laser Material Processing <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Masinelli, Chang Rajani, Patrik Hoffmann, Kilian Wasmer, David Atienza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring consistent processing quality is challenging in laser processes due
to varying material properties and surface conditions. Although some approaches
have shown promise in solving this problem via automation, they often rely on
predetermined targets or are limited to simulated environments. To address
these shortcomings, we propose a novel real-time reinforcement learning
approach for laser process control, implemented on a Field Programmable Gate
Array to achieve real-time execution. Our experimental results from laser
welding tests on stainless steel samples with a range of surface roughnesses
validated the method's ability to adapt autonomously, without relying on reward
engineering or prior setup information. Specifically, the algorithm learned the
correct power profile for each unique surface characteristic, demonstrating
significant improvements over hand-engineered optimal constant power strategies
-- up to 23% better performance on rougher surfaces and 7% on mixed surfaces.
This approach represents a significant advancement in automating and optimizing
laser processes, with potential applications across multiple industries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2025 IEEE International Conference on Robotics and
  Automation (ICRA), May 19-23, 2025, Atlanta, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional
  Structured Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihwan Park, Jihun Yun, SungYub Kim, Souvik Kundu, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zeroth-order (ZO) optimization has emerged as a promising alternative to
gradient-based backpropagation methods, particularly for black-box optimization
and large language model (LLM) fine-tuning. However, ZO methods suffer from
slow convergence due to high-variance stochastic gradient estimators. While
structured perturbations, such as sparsity and low-rank constraints, have been
explored to mitigate these issues, their effectiveness remains highly
under-explored. In this work, we develop a unified theoretical framework that
analyzes both the convergence and generalization properties of ZO optimization
under structured perturbations. We show that high dimensionality is the primary
bottleneck and introduce the notions of \textit{stable rank} and
\textit{effective overlap} to explain how structured perturbations reduce
gradient noise and accelerate convergence. Using the uniform stability under
our framework, we then provide the first theoretical justification for why
these perturbations enhance generalization. Additionally, through empirical
analysis, we identify that \textbf{block coordinate descent} (BCD) to be an
effective structured perturbation method. Extensive experiments show that,
compared to existing alternatives, memory-efficient ZO (MeZO) with BCD
(\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock
time/iteration by up to $\times\textbf{2.09}$ while yielding similar or better
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\infty$-Video: A Training-Free Approach to Long Video Understanding via
  Continuous-Time Memory Consolidation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saul Santos, António Farinhas, Daniel C. McNamee, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current video-language models struggle with long-video understanding due to
limited context lengths and reliance on sparse frame subsampling, often leading
to information loss. This paper introduces $\infty$-Video, which can process
arbitrarily long videos through a continuous-time long-term memory (LTM)
consolidation mechanism. Our framework augments video Q-formers by allowing
them to process unbounded video contexts efficiently and without requiring
additional training. Through continuous attention, our approach dynamically
allocates higher granularity to the most relevant video segments, forming
"sticky" memories that evolve over time. Experiments with Video-LLaMA and
VideoChat2 demonstrate improved performance in video question-answering tasks,
showcasing the potential of continuous-time LTM mechanisms to enable scalable
and training-free comprehension of long videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathE: Leveraging Entity-Agnostic Paths for Parameter-Efficient
  <span class="highlight-title">Knowledge</span> Graph Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Reklos, Jacopo de Berardinis, Elena Simperl, Albert Meroño-Peñuela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) store human knowledge in the form of entities (nodes)
and relations, and are used extensively in various applications. KG embeddings
are an effective approach to addressing tasks like knowledge discovery, link
prediction, and reasoning. This is often done by allocating and learning
embedding tables for all or a subset of the entities. As this scales linearly
with the number of entities, learning embedding models in real-world KGs with
millions of nodes can be computationally intractable. To address this
scalability problem, our model, PathE, only allocates embedding tables for
relations (which are typically orders of magnitude fewer than the entities) and
requires less than 25% of the parameters of previous parameter efficient
methods. Rather than storing entity embeddings, we learn to compute them by
leveraging multiple entity-relation paths to contextualise individual entities
within triples. Evaluated on four benchmarks, PathE achieves state-of-the-art
performance in relation prediction, and remains competitive in link prediction
on path-rich KGs while training on consumer-grade hardware. We perform ablation
experiments to test our design choices and analyse the sensitivity of the model
to key hyper-parameters. PathE is efficient and cost-effective for relationally
diverse and well-connected KGs commonly found in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FL-APU: A Software Architecture to Ease Practical Implementation of
  Cross-Silo Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Stricker, J. A. Peregrina, D. Bermbach, C. Zirpins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is an upcoming technology that is increasingly
applied in real-world applications. Early applications focused on cross-device
scenarios, where many participants with limited resources train machine
learning (ML) models together, e.g., in the case of Google's GBoard.
Contrarily, cross-silo scenarios have only few participants but with many
resources, e.g., in the healthcare domain. Despite such early efforts, FL is
still rarely used in practice and best practices are, hence, missing. For new
applications, in our case inter-organizational cross-silo applications,
overcoming this lack of role models is a significant challenge.
  In order to ease the use of FL in real-world cross-silo applications, we here
propose a scenario-based architecture for the practical use of FL in the
context of multiple companies collaborating to improve the quality of their ML
models. The architecture emphasizes the collaboration between the participants
and the FL server and extends basic interactions with domain-specific features.
First, it combines governance with authentication, creating an environment
where only trusted participants can join. Second, it offers traceability of
governance decisions and tracking of training processes, which are also crucial
in a production environment. Beyond presenting the architectural design, we
analyze requirements for the real-world use of FL and evaluate the architecture
with a scenario-based analysis method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pivoting Factorization: A Compact Meta Low-Rank Representation of
  Sparsity for Efficient Inference in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Zhao, Yingtao Zhang, Carlo Vittorio Cannistraci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of Large Language Models has driven demand for effective
model compression techniques to reduce memory and computation costs. Low-rank
pruning has gained attention for its tensor coherence and GPU compatibility
across all densities. However, low-rank pruning has struggled to match the
performance of semi-structured pruning, often doubling perplexity (PPL) at
similar densities. In this paper, we propose Pivoting Factorization (PIFA), a
novel lossless meta low-rank representation that unsupervisedly learns a
compact form of any low-rank representation, effectively eliminating redundant
information. PIFA identifies pivot rows (linearly independent rows) and
expresses non-pivot rows as linear combinations, achieving an additional 24.2\%
memory savings and 24.6\% faster inference over low-rank layers at r/d = 0.5,
thereby significantly enhancing performance at the same density. To mitigate
the performance degradation caused by low-rank pruning, we introduce a novel,
retraining-free low-rank reconstruction method that minimizes error
accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework,
significantly outperforms existing low-rank pruning methods and, for the first
time, achieves performance comparable to semi-structured pruning, while
surpassing it in GPU efficiency and compatibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Oversmoothing in GNNs as Consensus in Opinion Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqin Wang, Yulong Yang, Ishan Saha, Christine Allen-Blanchette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to classes of neural networks where the learned representations
become increasingly expressive with network depth, the learned representations
in graph neural networks (GNNs), tend to become increasingly similar. This
phenomena, known as oversmoothing, is characterized by learned representations
that cannot be reliably differentiated leading to reduced predictive
performance. In this paper, we propose an analogy between oversmoothing in GNNs
and consensus or agreement in opinion dynamics. Through this analogy, we show
that the message passing structure of recent continuous-depth GNNs is
equivalent to a special case of opinion dynamics (i.e., linear consensus
models) which has been theoretically proven to converge to consensus (i.e.,
oversmoothing) for all inputs. Using the understanding developed through this
analogy, we design a new continuous-depth GNN model based on nonlinear opinion
dynamics and prove that our model, which we call behavior-inspired message
passing neural network (BIMP) circumvents oversmoothing for general inputs.
Through extensive experiments, we show that BIMP is robust to oversmoothing and
adversarial attack, and consistently outperforms competitive baselines on
numerous benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bias-Correction Decentralized Stochastic Gradient Algorithm with
  Momentum Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Hu, Xi Chen, Weidong Liu, Xiaojun Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed stochastic optimization algorithms can handle large-scale data
simultaneously and accelerate model training. However, the sparsity of
distributed networks and the heterogeneity of data limit these advantages. This
paper proposes a momentum-accelerated distributed stochastic gradient
algorithm, referred to as Exact-Diffusion with Momentum (EDM), which can
correct the bias caused by data heterogeneity and introduces the momentum
method commonly used in deep learning to accelerate the convergence of the
algorithm. We theoretically demonstrate that this algorithm converges to the
neighborhood of the optimum sub-linearly irrelevant to data heterogeneity when
applied to non-convex objective functions and linearly under the
Polyak-{\L}ojasiewicz condition (a weaker assumption than $\mu$-strongly
convexity). Finally, we evaluate the performance of the proposed algorithm by
simulation, comparing it with a range of existing decentralized optimization
algorithms to demonstrate its effectiveness in addressing data heterogeneity
and network sparsity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Policy Gradient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Rio, Merwan Barlier, Igor Colin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the increasing deployment of reinforcement learning in the real
world, involving a large consumption of personal data, we introduce a
differentially private (DP) policy gradient algorithm. We show that, in this
setting, the introduction of Differential Privacy can be reduced to the
computation of appropriate trust regions, thus avoiding the sacrifice of
theoretical properties of the DP-less methods. Therefore, we show that it is
possible to find the right trade-off between privacy noise and trust-region
size to obtain a performant differentially private policy gradient algorithm.
We then outline its performance empirically on various benchmarks. Our results
and the complexity of the tasks addressed represent a significant improvement
over existing DP algorithms in online RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temperature-Annealed Boltzmann Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrik Schopmans, Pascal Friederich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient sampling of unnormalized probability densities such as the
Boltzmann distribution of molecular systems is a longstanding challenge. Next
to conventional approaches like molecular dynamics or Markov chain Monte Carlo,
variational approaches, such as training normalizing flows with the reverse
Kullback-Leibler divergence, have been introduced. However, such methods are
prone to mode collapse and often do not learn to sample the full
configurational space. Here, we present temperature-annealed Boltzmann
generators (TA-BG) to address this challenge. First, we demonstrate that
training a normalizing flow with the reverse Kullback-Leibler divergence at
high temperatures is possible without mode collapse. Furthermore, we introduce
a reweighting-based training objective to anneal the distribution to lower
target temperatures. We apply this methodology to three molecular systems of
increasing complexity and, compared to the baseline, achieve better results in
almost all metrics while requiring up to three times fewer target energy
evaluations. For the largest system, our approach is the only method that
accurately resolves the metastable states of the system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pareto-frontier Entropy Search with Variational Lower Bound Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanori Ishikura, Masayuki Karasuyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study considers multi-objective Bayesian optimization (MOBO) through the
information gain of the Pareto-frontier. To calculate the information gain, a
predictive distribution conditioned on the Pareto-frontier plays a key role,
which is defined as a distribution truncated by the Pareto-frontier. However,
it is usually impossible to obtain the entire Pareto-frontier in a continuous
domain, and therefore, the complete truncation cannot be known. We consider an
approximation of the truncate distribution by using a mixture distribution
consisting of two possible approximate truncation obtainable from a subset of
the Pareto-frontier, which we call over- and under-truncation. Since the
optimal balance of the mixture is unknown beforehand, we propose optimizing the
balancing coefficient through the variational lower bound maximization
framework, by which the approximation error of the information gain can be
minimized. Our empirical evaluation demonstrates the effectiveness of the
proposed method particularly when the number of objective functions is large.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikingSoft: A Spiking Neuron Controller for Bio-inspired Locomotion
  with Soft Snake Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Zhang, Cong Wang, Wei Pan, Cosimo Della Santina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the dynamic coupling of moto-neurons and physical elasticity in
animals, this work explores the possibility of generating locomotion gaits by
utilizing physical oscillations in a soft snake by means of a low-level spiking
neural mechanism. To achieve this goal, we introduce the Double Threshold
Spiking neuron model with adjustable thresholds to generate varied output
patterns. This neuron model can excite the natural dynamics of soft robotic
snakes, and it enables distinct movements, such as turning or moving forward,
by simply altering the neural thresholds. Finally, we demonstrate that our
approach, termed SpikingSoft, naturally pairs and integrates with reinforcement
learning. The high-level agent only needs to adjust the two thresholds to
generate complex movement patterns, thus strongly simplifying the learning of
reactive locomotion. Simulation results demonstrate that the proposed
architecture significantly enhances the performance of the soft snake robot,
enabling it to achieve target objectives with a 21.6% increase in success rate,
a 29% reduction in time to reach the target, and smoother movements compared to
the vanilla reinforcement learning controllers or Central Pattern Generator
controller acting in torque space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8th IEEE-RAS International Conference on Soft Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are In-Context Bandit Reinforcement Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at in-context learning (ICL), a supervised
learning technique that relies on adding annotated examples to the model
context. We investigate a contextual bandit version of in-context reinforcement
learning (ICRL), where models learn in-context, online, from external reward,
instead of supervised data. We show that LLMs effectively demonstrate such
learning, and provide a detailed study of the phenomena, experimenting with
challenging classification tasks and models of sizes from 500M to 70B
parameters. This includes identifying and addressing the instability of the
process, demonstrating learning with both semantic and abstract labels, and
showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while
also underscoring fundamental limitations in their implicit reasoning about
errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Node Classification and Search on the Rubik's Cube Graph with GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Barro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on the application of deep geometric models to solve the
3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation and
defining distance as the model's optimization objective. The distance
approximation task is reformulated as a node classification problem,
effectively addressed using Graph Neural Networks (GNNs). After training the
model on a random subgraph, the predicted classes are used to construct a
heuristic for $A^*$ search. We conclude with experiments comparing our
heuristic to that of the DeepCubeA model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Optimal Algorithms for Group Distributionally Robust Optimization
  and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tasuku Soma, Khashayar Gatmiry, Sharut Gupta, Stefanie Jegelka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributionally robust optimization (DRO) can improve the robustness and
fairness of learning methods. In this paper, we devise stochastic algorithms
for a class of DRO problems including group DRO, subpopulation fairness, and
empirical conditional value at risk (CVaR) optimization. Our new algorithms
achieve faster convergence rates than existing algorithms for multiple DRO
settings. We also provide a new information-theoretic lower bound that implies
our bounds are tight for group DRO. Empirically, too, our algorithms outperform
known methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOAP: Improving and Stabilizing Shampoo using Adam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing evidence of the effectiveness of Shampoo, a higher-order
preconditioning method, over Adam in deep learning optimization tasks. However,
Shampoo's drawbacks include additional hyperparameters and computational
overhead when compared to Adam, which only updates running averages of first-
and second-moment quantities. This work establishes a formal connection between
Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient
approximation of Adam -- showing that Shampoo is equivalent to running
Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to
the design of a simpler and computationally efficient algorithm:
$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the
$\textbf{P}$reconditioner's eigenbasis (SOAP).
  With regards to improving Shampoo's computational efficiency, the most
straightforward approach would be to simply compute Shampoo's
eigendecomposition less frequently. Unfortunately, as our empirical results
show, this leads to performance degradation that worsens with this frequency.
SOAP mitigates this degradation by continually updating the running average of
the second moment, just as Adam does, but in the current (slowly changing)
coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a
rotated space, it introduces only one additional hyperparameter (the
preconditioning frequency) compared to Adam. We empirically evaluate SOAP on
language model pre-training with 360m and 660m sized models. In the large batch
regime, SOAP reduces the number of iterations by over 40% and wall clock time
by over 35% compared to AdamW, with approximately 20% improvements in both
metrics compared to Shampoo. An implementation of SOAP is available at
https://github.com/nikhilvyas/SOAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning multivariate Gaussians with imperfect advice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12700v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12700v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Bhattacharyya, Davin Choo, Philips George John, Themis Gouleakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the problem of distribution learning within the framework of
learning-augmented algorithms. In this setting, we explore the scenario where a
probability distribution is provided as potentially inaccurate advice on the
true, unknown distribution. Our objective is to develop learning algorithms
whose sample complexity decreases as the quality of the advice improves,
thereby surpassing standard learning lower bounds when the advice is
sufficiently accurate.
  Specifically, we demonstrate that this outcome is achievable for the problem
of learning a multivariate Gaussian distribution $N(\boldsymbol{\mu},
\boldsymbol{\Sigma})$ in the PAC learning setting. Classically, in the
advice-free setting, $\tilde{\Theta}(d^2/\varepsilon^2)$ samples are sufficient
and worst case necessary to learn $d$-dimensional Gaussians up to TV distance
$\varepsilon$ with constant probability. When we are additionally given a
parameter $\tilde{\boldsymbol{\Sigma}}$ as advice, we show that
$\tilde{O}(d^{2-\beta}/\varepsilon^2)$ samples suffices whenever $\|
\tilde{\boldsymbol{\Sigma}}^{-1/2} \boldsymbol{\Sigma}
\tilde{\boldsymbol{\Sigma}}^{-1/2} - \boldsymbol{I_d} \|_1 \leq \varepsilon
d^{1-\beta}$ (where $\|\cdot\|_1$ denotes the entrywise $\ell_1$ norm) for any
$\beta > 0$, yielding a polynomial improvement over the advice-free setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAeUron: Interpretable Concept Unlearning in Diffusion Models with
  Sparse Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bartosz Cywiński, Kamil Deja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, while powerful, can inadvertently generate harmful or
undesirable content, raising significant ethical and safety concerns. Recent
machine unlearning approaches offer potential solutions but often lack
transparency, making it difficult to understand the changes they introduce to
the base model. In this work, we introduce SAeUron, a novel method leveraging
features learned by sparse autoencoders (SAEs) to remove unwanted concepts in
text-to-image diffusion models. First, we demonstrate that SAEs, trained in an
unsupervised manner on activations from multiple denoising timesteps of the
diffusion model, capture sparse and interpretable features corresponding to
specific concepts. Building on this, we propose a feature selection method that
enables precise interventions on model activations to block targeted content
while preserving overall performance. Evaluation with the competitive
UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's
state-of-the-art performance. Moreover, we show that with a single SAE, we can
remove multiple concepts simultaneously and that in contrast to other methods,
SAeUron mitigates the possibility of generating unwanted content, even under
adversarial attack. Code and checkpoints are available at:
https://github.com/cywinski/SAeUron.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4o as the Gold Standard: A Scalable and General Purpose Approach to
  Filter Language Model <span class="highlight-title">Pretrain</span>ing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jifan Zhang, Ziyue Luo, Jia Liu, Ness Shroff, Robert Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models require vast amounts of high-quality training data, but
effective filtering of web-scale datasets remains a significant challenge. This
paper demonstrates that GPT-4o is remarkably effective at identifying
high-quality training data, but its prohibitive cost makes it impractical at
web-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o
accuracy at less than 1\% of the cost. SIEVE can perform up to 500 filtering
operations for the cost of one GPT-4o filtering call. The key to SIEVE is a
seamless integration of GPT-4o and lightweight text classification models,
using active learning to fine-tune these models in the background with a small
number of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a
tiny fraction of the cost. Through different filtering prompts, SIEVE can
efficiently curate high quality data for general or specialized domains from
web-scale corpora -- a valuable capability given the current scarcity of
high-quality domain-specific datasets. Extensive experiments using automatic
and human evaluation metrics show that SIEVE and GPT-4o achieve similar
performance on five highly specific filtering prompts. In addition, when
performing quality filtering on web crawl datasets, we demonstrate SIEVE can
further improve over state-of-the-art quality filtering methods in the
DataComp-LM challenge for selecting LLM pretraining data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CORAL: Concept Drift Representation Learning for Co-evolving Time-series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01480v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01480v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunpeng Xu, Lifei Chen, Shengrui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of time series analysis, tackling the phenomenon of concept
drift poses a significant challenge. Concept drift -- characterized by the
evolving statistical properties of time series data, affects the reliability
and accuracy of conventional analysis models. This is particularly evident in
co-evolving scenarios where interactions among variables are crucial. This
paper presents CORAL, a simple yet effective method that models time series as
an evolving ecosystem to learn representations of concept drift. CORAL employs
a kernel-induced self-representation learning to generate a representation
matrix, encapsulating the inherent dynamics of co-evolving time series. This
matrix serves as a key tool for identification and adaptation to concept drift
by observing its temporal variations. Furthermore, CORAL effectively identifies
prevailing patterns and offers insights into emerging trends through pattern
evolution analysis. Our empirical evaluation of CORAL across various datasets
demonstrates its effectiveness in handling the complexities of concept drift.
This approach introduces a novel perspective in the theoretical domain of
co-evolving time series analysis, enhancing adaptability and accuracy in the
face of dynamic data environments, and can be easily integrated into most deep
learning backbones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Loss</span> shaping enhances exact gradient learning with Eventprop in spiking
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01232v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01232v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Nowotny, James P. Turner, James C. Knight
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based machine learning promises more energy-efficient AI on future
neuromorphic hardware. Here, we investigate how the recently discovered
Eventprop algorithm for gradient descent on exact gradients in spiking neural
networks can be scaled up to challenging keyword recognition benchmarks. We
implemented Eventprop in the GPU-enhanced Neural Networks framework and used it
for training recurrent spiking neural networks on the Spiking Heidelberg Digits
and Spiking Speech Commands datasets. We found that learning depended strongly
on the loss function and extended Eventprop to a wider class of loss functions
to enable effective training. We then tested a large number of data
augmentations and regularisations as well as exploring different network
structures; and heterogeneous and trainable timescales. We found that when
combined with two specific augmentations, the right regularisation and a delay
line input, Eventprop networks with one recurrent layer achieved
state-of-the-art performance on Spiking Heidelberg Digits and good accuracy on
Spiking Speech Commands. In comparison to a leading surrogate-gradient-based
SNN training method, our GeNN Eventprop implementation is 3X faster and uses 4X
less memory. This work is a significant step towards a low-power neuromorphic
alternative to current machine learning paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 7 figures, 5 tables. Neuromorphic Computing and Engineering
  (2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fast, Specialized Machine Learning Force Fields: Distilling
  Foundation Models via Energy Hessians <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishan Amin, Sanjeev Raja, Aditi Krishnapriyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The foundation model (FM) paradigm is transforming Machine Learning Force
Fields (MLFFs), leveraging general-purpose representations and scalable
training to perform a variety of computational chemistry tasks. Although MLFF
FMs have begun to close the accuracy gap relative to first-principles methods,
there is still a strong need for faster inference speed. Additionally, while
research is increasingly focused on general-purpose models which transfer
across chemical space, practitioners typically only study a small subset of
systems at a given time. This underscores the need for fast, specialized MLFFs
relevant to specific downstream applications, which preserve test-time physical
soundness while maintaining train-time scalability. In this work, we introduce
a method for transferring general-purpose representations from MLFF foundation
models to smaller, faster MLFFs specialized to specific regions of chemical
space. We formulate our approach as a knowledge distillation procedure, where
the smaller "student" MLFF is trained to match the Hessians of the energy
predictions of the "teacher" foundation model. Our specialized MLFFs can be up
to 20 $\times$ faster than the original foundation model, while retaining, and
in some cases exceeding, its performance and that of undistilled models. We
also show that distilling from a teacher model with a direct force
parameterization into a student model trained with conservative forces (i.e.,
computed as derivatives of the potential energy) successfully leverages the
representations from the large-scale teacher for improved accuracy, while
maintaining energy conservation during test-time molecular dynamics
simulations. More broadly, our work suggests a new paradigm for MLFF
development, in which foundation models are released along with smaller,
specialized simulation "engines" for common chemical subsets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at ICLR 2025. The implementation of
  our method is available at https://github.com/ASK-Berkeley/MLFF-distill</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexi<span class="highlight-title">GPT</span>: Pruning and Extending Large Language Models with Low-Rank
  Weight Sharing <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Seale Smith, Chi-Heng Lin, Shikhar Tuli, Haris Jeelani, Shangqian Gao, Yilin Shen, Hongxia Jin, Yen-Chang Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of large language models (LLMs) in natural language
processing (NLP) has created a critical need for techniques that enable
efficient deployment on memory-constrained devices without compromising
performance. We present a method to prune LLMs that selectively prunes model
blocks based on an importance score and replaces them with a low-parameter
replacement strategy. Specifically, we propose a principled metric to replace
each pruned block using a weight-sharing mechanism that leverages unpruned
counterparts from the model and block-specific low-rank adapters. Furthermore,
we facilitate the learning of these replacement blocks with output feature
normalization and an adapter initialization scheme built on low-rank SVD
reconstructions. Empirical evaluations demonstrate substantial performance
gains over existing methods, achieving state-of-the-art performance on 5/6
benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression
rate of 40%. We also demonstrate that our approach can extend smaller models,
boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended
training with minimal additional parameter costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 - Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wearable Accelerometer Foundation Models for Health via <span class="highlight-title">Knowledge</span>
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salar Abbaspourazad, Anshuman Mishra, Joseph Futoma, Andrew C. Miller, Ian Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern wearable devices can conveniently record various biosignals in the
many different environments of daily living, enabling a rich view of individual
health. However, not all biosignals are the same: high-fidelity biosignals,
such as photoplethysmogram (PPG), contain more physiological information, but
require optical sensors with a high power footprint. Alternatively, a
lower-fidelity biosignal such as accelerometry has a significantly smaller
power footprint and is available in almost any wearable device. While
accelerometry is widely used for activity recognition and fitness, it is less
explored for health biomarkers and diagnosis. Here, we show that an
accelerometry foundation model can predict a wide variety of health targets. To
achieve improved performance, we distill representational knowledge from PPG
encoders to accelerometery encoders using 20 million minutes of unlabeled data,
collected from ~172K participants in the Apple Heart and Movement Study under
informed consent. We observe strong cross-modal alignment on unseen data, e.g.,
99.2% top-1 accuracy for retrieving PPG embeddings from accelerometry
embeddings. We show that distilled accelerometry encoders have significantly
more informative representations compared to self-supervised or supervised
encoders trained directly on accelerometry data, observed by at least 23%-49%
improved performance for predicting heart rate and heart rate variability. We
also show that distilled accelerometry encoders are readily predictive of a
wide array of downstream health targets, i.e., they are generalist foundation
models. We believe accelerometry foundation models for health may unlock new
opportunities for developing digital biomarkers from any wearable device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updated format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and
  Parameter Efficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based reinforcement learning (RL) offers a solution to the data
inefficiency that plagues most model-free RL algorithms. However, learning a
robust world model often demands complex and deep architectures, which are
expensive to compute and train. Within the world model, dynamics models are
particularly crucial for accurate predictions, and various dynamics-model
architectures have been explored, each with its own set of challenges.
Currently, recurrent neural network (RNN) based world models face issues such
as vanishing gradients and difficulty in capturing long-term dependencies
effectively. In contrast, use of transformers suffers from the well-known
issues of self-attention mechanisms, where both memory and computational
complexity scale as $O(n^2)$, with $n$ representing the sequence length.
  To address these challenges we propose a state space model (SSM) based world
model, specifically based on Mamba, that achieves $O(n)$ memory and
computational complexity while effectively capturing long-term dependencies and
facilitating the use of longer training sequences efficiently. We also
introduce a novel sampling method to mitigate the suboptimality caused by an
incorrect world model in the early stages of training, combining it with the
aforementioned technique to achieve a normalised score comparable to other
state-of-the-art model-based RL algorithms using only a 7 million trainable
parameter world model. This model is accessible and can be trained on an
off-the-shelf laptop. Our code is available at
https://github.com/realwenlongwang/Drama.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random features and polynomial rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabián Aguirre-López, Silvio Franz, Mauro Pastore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random features models play a distinguished role in the theory of deep
learning, describing the behavior of neural networks close to their
infinite-width limit. In this work, we present a thorough analysis of the
generalization performance of random features models for generic supervised
learning problems with Gaussian data. Our approach, built with tools from the
statistical mechanics of disordered systems, maps the random features model to
an equivalent polynomial model, and allows us to plot average generalization
curves as functions of the two main control parameters of the problem: the
number of random features $N$ and the size $P$ of the training set, both
assumed to scale as powers in the input dimension $D$. Our results extend the
case of proportional scaling between $N$, $P$ and $D$. They are in accordance
with rigorous bounds known for certain particular learning tasks and are in
quantitative agreement with numerical experiments performed over many order of
magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic
limits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remains
finite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages + appendix, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Universal Certified Robustness with Multi-Norm Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enyi Jiang, David S. Cheung, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing certified training methods can only train models to be robust
against a certain perturbation type (e.g. $l_\infty$ or $l_2$). However, an
$l_\infty$ certifiably robust model may not be certifiably robust against $l_2$
perturbation (and vice versa) and also has low robustness against other
perturbations (e.g. geometric and patch transformation). By constructing a
theoretical framework to analyze and mitigate the tradeoff, we propose the
first multi-norm certified training framework \textbf{CURE}, consisting of
several multi-norm certified training methods, to attain better \emph{union
robustness} when training from scratch or fine-tuning a pre-trained certified
model. Inspired by our theoretical findings, we devise bound alignment and
connect natural training with certified training for better union robustness.
Compared with SOTA-certified training, \textbf{CURE} improves union robustness
to $32.0\%$ on MNIST, $25.8\%$ on CIFAR-10, and $10.6\%$ on TinyImagenet across
different epsilon values. It leads to better generalization on a diverse set of
challenging unseen geometric and patch perturbations to $6.8\%$ and $16.0\%$ on
CIFAR-10. Overall, our contributions pave a path towards \textit{universal
certified robustness}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Potential and limitations of random Fourier features for dequantizing
  quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Sweke, Erik Recio, Sofiene Jerbi, Elies Gil-Fuster, Bryce Fuller, Jens Eisert, Johannes Jakob Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning is arguably one of the most explored applications of
near-term quantum devices. Much focus has been put on notions of variational
quantum machine learning where parameterized quantum circuits (PQCs) are used
as learning models. These PQC models have a rich structure which suggests that
they might be amenable to efficient dequantization via random Fourier features
(RFF). In this work, we establish necessary and sufficient conditions under
which RFF does indeed provide an efficient dequantization of variational
quantum machine learning for regression. We build on these insights to make
concrete suggestions for PQC architecture design, and to identify structures
which are necessary for a regression problem to admit a potential quantum
advantage via PQC based optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 5 figures. Many clarifying figures added to this version.
  Comments and feedback welcome. Now accepted in Quantum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAN: Fourier Analysis Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jinliang Deng, Jing Su, Jun Zhang, Jingjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable successes of general-purpose neural networks, such as
MLPs and Transformers, we find that they exhibit notable shortcomings in
modeling and reasoning about periodic phenomena, achieving only marginal
performance within the training domain and failing to generalize effectively to
out-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature and
science. Therefore, neural networks should be equipped with the essential
ability to model and handle periodicity. In this work, we propose FAN, a novel
general-purpose neural network that offers broad applicability similar to MLP
while effectively addressing periodicity modeling challenges. Periodicity is
naturally integrated into FAN's structure and computational processes by
introducing the Fourier Principle. Unlike existing Fourier-based networks,
which possess particular periodicity modeling abilities but are typically
designed for specific tasks, our approach maintains the general-purpose
modeling capability. Therefore, FAN can seamlessly replace MLP in various model
architectures with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the superiority of FAN in periodicity modeling tasks and the
effectiveness and generalizability of FAN across a range of real-world tasks,
e.g., symbolic formula representation, time series forecasting, language
modeling, and image recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVG-CRPS: A Robust <span class="highlight-title">Loss</span> Function for Multivariate Probabilistic
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Zhihao Zheng, Lijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Gaussian (MVG) distributions are central to modeling correlated
continuous variables in probabilistic forecasting. Neural forecasting models
typically parameterize the mean vector and covariance matrix of the
distribution using neural networks, optimizing with the log-score (negative
log-likelihood) as the loss function. However, the sensitivity of the log-score
to outliers can lead to significant errors in the presence of anomalies.
Drawing on the continuous ranked probability score (CRPS) for univariate
distributions, we propose MVG-CRPS, a strictly proper scoring rule for MVG
distributions. MVG-CRPS admits a closed-form expression in terms of neural
network outputs, thereby integrating seamlessly into deep learning frameworks.
Experiments on real-world datasets across multivariate autoregressive and
univariate sequence-to-sequence (Seq2Seq) forecasting tasks show that MVG-CRPS
improves robustness, accuracy, and uncertainty quantification in probabilistic
forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contraction of Private Quantum Channels and Private Quantum Hypothesis
  Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theshani Nuradha, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A quantum generalized divergence by definition satisfies the data-processing
inequality; as such, the relative decrease in such a divergence under the
action of a quantum channel is at most one. This relative decrease is formally
known as the contraction coefficient of the channel and the divergence.
Interestingly, there exist combinations of channels and divergences for which
the contraction coefficient is strictly less than one. Furthermore,
understanding the contraction coefficient is fundamental for the study of
statistical tasks under privacy constraints. To this end, here we establish
upper bounds on contraction coefficients for the hockey-stick divergence under
privacy constraints, where privacy is quantified with respect to the quantum
local differential privacy (QLDP) framework, and we fully characterize the
contraction coefficient for the trace distance under privacy constraints. With
the machinery developed, we also determine an upper bound on the contraction of
both the Bures distance and quantum relative entropy relative to the normalized
trace distance, under QLDP constraints. Next, we apply our findings to
establish bounds on the sample complexity of quantum hypothesis testing under
privacy constraints. Furthermore, we study various scenarios in which the
sample complexity bounds are tight, while providing order-optimal quantum
channels that achieve those bounds. Lastly, we show how private quantum
channels provide fairness and Holevo information stability in quantum learning
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages; See independent work titled "Sample Complexity of Locally
  Differentially Private Quantum Hypothesis Testing" by Hao-Chung Cheng,
  Christoph Hirche, and Cambyse Rouz\'e</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have revolutionized vision-language
understanding but remain vulnerable to multimodal jailbreak attacks, where
adversarial inputs are meticulously crafted to elicit harmful or inappropriate
responses. We propose UniGuard, a novel multimodal safety guardrail that
jointly considers the unimodal and cross-modal harmful signals. UniGuard trains
a multimodal guardrail to minimize the likelihood of generating harmful
responses in a toxic corpus. The guardrail can be seamlessly applied to any
input prompt during inference with minimal computational costs. Extensive
experiments demonstrate the generalizability of UniGuard across multiple
modalities, attack strategies, and multiple state-of-the-art MLLMs, including
LLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust
defense mechanism maintains the models' overall vision-language understanding
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Referential communication in heterogeneous communities of <span class="highlight-title">pre-train</span>ed
  visual deep networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08913v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08913v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matéo Mahaut, Francesca Franzon, Roberto Dessì, Marco Baroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large pre-trained image-processing neural networks are being embedded in
autonomous agents such as self-driving cars or robots, the question arises of
how such systems can communicate with each other about the surrounding world,
despite their different architectures and training regimes. As a first step in
this direction, we systematically explore the task of referential communication
in a community of heterogeneous state-of-the-art pre-trained visual networks,
showing that they can develop, in a self-supervised way, a shared protocol to
refer to a target object among a set of candidates. This shared protocol can
also be used, to some extent, to communicate about previously unseen object
categories of different granularity. Moreover, a visual network that was not
initially part of an existing community can learn the community's protocol with
remarkable ease. Finally, we study, both qualitatively and quantitatively, the
properties of the emergent protocol, providing some evidence that it is
capturing high-level semantic features of objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Restoring balance: principled under/oversampling of data for optimal
  classification <span class="chip">ICML'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Loffredo, Mauro Pastore, Simona Cocco, Rémi Monasson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in real-world data poses a common bottleneck for machine
learning tasks, since achieving good generalization on under-represented
examples is often challenging. Mitigation strategies, such as under or
oversampling the data depending on their abundances, are routinely proposed and
tested empirically, but how they should adapt to the data statistics remains
poorly understood. In this work, we determine exact analytical expressions of
the generalization curves in the high-dimensional regime for linear classifiers
(Support Vector Machines). We also provide a sharp prediction of the effects of
under/oversampling strategies depending on class imbalance, first and second
moments of the data, and the metrics of performance considered. We show that
mixed strategies involving under and oversampling of data lead to performance
improvement. Through numerical experiments, we show the relevance of our
theoretical predictions on real datasets, on deeper architectures and with
sampling strategies based on unsupervised probabilistic models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages + appendix, 3 figures. Presented at ICML'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResKoopNet: Learning Koopman Representations for Complex Dynamics with
  Spectral Residuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Xu, Kaidi Shao, Nikos Logothetis, Zhongwei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing long-term behaviors in high-dimensional nonlinear dynamical systems
remains challenging, with the Koopman operator framework providing a powerful
global linearization approach, though existing methods for approximating its
spectral components often suffer from theoretical limitations and reliance on
predefined dictionaries. While Residual Dynamic Mode Decomposition (ResDMD)
introduced the spectral residual to assess the accuracy of Koopman operator
approximation, its only filters precomputed spectra, which prevents it from
fully discovering the Koopman operator's complete spectral information (a
limitation sometimes referred to as the 'spectral inclusion' problem). We
introduce ResKoopNet (Residual-based Koopman-learning Network), a novel method
that addresses this limitation by explicitly minimizing the spectral residual
to compute Koopman eigenpairs, which can identify a more precise and complete
spectrum of the Koopman operator. This approach provides theoretical guarantees
while maintaining computational adaptability through a neural network
implementation. Experiments on physical and biological systems demonstrate
ResKoopNet's superior accuracy in spectral approximation compared to existing
methods, particularly for systems with continuous spectra and high dimensional,
which makes it as an effective tool for analyzing complex dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Out-of-Distribution Uncertainty Quantification via Weight Entropy
  Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine de Mathelin, François Deheeger, Mathilde Mougeot, Nicolas Vayatis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with uncertainty quantification and out-of-distribution
detection in deep learning using Bayesian and ensemble methods. It proposes a
practical solution to the lack of prediction diversity observed recently for
standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et
al., 2021). Considering that this issue is mainly related to a lack of weight
diversity, we claim that standard methods sample in "over-restricted" regions
of the weight space due to the use of "over-regularization" processes, such as
weight decay and zero-mean centered Gaussian priors. We propose to solve the
problem by adopting the maximum entropy principle for the weight distribution,
with the underlying idea to maximize the weight diversity. Under this paradigm,
the epistemic uncertainty is described by the weight distribution of maximal
entropy that produces neural networks "consistent" with the training
observations. Considering stochastic neural networks, a practical optimization
is derived to build such a distribution, defined as a trade-off between the
average empirical risk and the weight distribution entropy. We develop a novel
weight parameterization for the stochastic model, based on the singular value
decomposition of the neural network's hidden representations, which enables a
large increase of the weight entropy for a small empirical risk penalization.
We provide both theoretical and numerical results to assess the efficiency of
the approach. In particular, the proposed algorithm appears in the top three
best methods in all configurations of an extensive out-of-distribution
detection benchmark including more than thirty competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Human-Aligned Representations with Contrastive Learning and
  Generative Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19420v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19420v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raja Marjieh, Sreejan Kumar, Declan Campbell, Liyi Zhang, Gianluca Bencomo, Jake Snell, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans rely on effective representations to learn from few examples and
abstract useful information from sensory data. Inducing such representations in
machine learning models has been shown to improve their performance on various
benchmarks such as few-shot learning and robustness. However, finding effective
training procedures to achieve that goal can be challenging as psychologically
rich training data such as human similarity judgments are expensive to scale,
and Bayesian models of human inductive biases are often intractable for
complex, realistic domains. Here, we address this challenge by leveraging a
Bayesian notion of generative similarity whereby two data points are considered
similar if they are likely to have been sampled from the same distribution.
This measure can be applied to complex generative processes, including
probabilistic programs. We incorporate generative similarity into a contrastive
learning objective to enable learning of embeddings that express human
cognitive representations. We demonstrate the utility of our approach by
showing that it can be used to capture human-like representations of shape
regularity, abstract Euclidean geometric concepts, and semantic hierarchies for
natural images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BADM: Batch ADMM for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ouya Wang, Shenglong Zhou, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient descent-based algorithms are widely used for training
deep neural networks but often suffer from slow convergence. To address the
challenge, we leverage the framework of the alternating direction method of
multipliers (ADMM) to develop a novel data-driven algorithm, called batch ADMM
(BADM). The fundamental idea of the proposed algorithm is to split the training
data into batches, which is further divided into sub-batches where primal and
dual variables are updated to generate global parameters through aggregation.
We evaluate the performance of BADM across various deep learning tasks,
including graph modelling, computer vision, image generation, and natural
language processing. Extensive numerical experiments demonstrate that BADM
achieves faster convergence and superior testing accuracy compared to other
state-of-the-art optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOP-ERL: <span class="highlight-title">Transformer</span>-based Off-Policy Episodic Reinforcement Learning <span class="chip">ICLR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Li, Dong Tian, Hongyi Zhou, Xinkai Jiang, Rudolf Lioutikov, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Transformer-based Off-Policy Episodic Reinforcement
Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the
ERL framework. In ERL, policies predict entire action trajectories over
multiple time steps instead of single actions at every time step. These
trajectories are typically parameterized by trajectory generators such as
Movement Primitives (MP), allowing for smooth and efficient exploration over
long horizons while capturing high-level temporal correlations. However, ERL
methods are often constrained to on-policy frameworks due to the difficulty of
evaluating state-action values for entire action sequences, limiting their
sample efficiency and preventing the use of more efficient off-policy
architectures. TOP-ERL addresses this shortcoming by segmenting long action
sequences and estimating the state-action values for each segment using a
transformer-based critic architecture alongside an n-step return estimation.
These contributions result in efficient and stable training that is reflected
in the empirical results conducted on sophisticated robot learning
environments. TOP-ERL significantly outperforms state-of-the-art RL methods.
Thorough ablation studies additionally show the impact of key design choices on
the model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codebase: https://github.com/BruceGeLi/TOP_ERL_ICLR25. arXiv admin
  note: text overlap with arXiv:2401.11437</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Separate Instructions From Data? And What Do We Even Mean By
  That? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06833v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06833v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned Large Language Models (LLMs) show impressive results in
numerous practical applications, but they lack essential safety features that
are common in other areas of computer science, particularly an explicit
separation of instructions and data. This makes them vulnerable to
manipulations such as indirect prompt injections and generally unsuitable for
safety-critical tasks. Surprisingly, there is currently no established
definition or benchmark to quantify this phenomenon. In this work, we close
this gap by introducing a formal measure for instruction-data separation and an
empirical variant that is calculable from a model's outputs. We also present a
new dataset, SEP, that allows estimating the measure for real-world models. Our
results on various LLMs show that the problem of instruction-data separation is
real: all models fail to achieve high separation, and canonical mitigation
techniques, such as prompt engineering and fine-tuning, either fail to
substantially improve separation or reduce model utility. The source code and
SEP dataset are openly accessible at
https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025, GitHub:
  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 10 pages main
  text, 30 pages in total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Universality Theorem for Deep and Shallow
  Joint-Group-Equivariant Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13682v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13682v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a constructive universal approximation theorem for learning
machines equipped with joint-group-equivariant feature maps, called the
joint-equivariant machines, based on the group representation theory.
"Constructive" here indicates that the distribution of parameters is given in a
closed-form expression known as the ridgelet transform.
Joint-group-equivariance encompasses a broad class of feature maps that
generalize classical group-equivariance. Particularly, fully-connected networks
are not group-equivariant but are joint-group-equivariant. Our main theorem
also unifies the universal approximation theorems for both shallow and deep
networks. Until this study, the universality of deep networks has been shown in
a different manner from the universality of shallow networks, but our results
discuss them on common ground. Now we can understand the approximation schemes
of various learning machines in a unified manner. As applications, we show the
constructive universal approximation properties of four examples: depth-$n$
joint-equivariant machine, depth-$n$ fully-connected network, depth-$n$
group-convolutional network, and a new depth-$2$ network with quadratic forms
whose universality has not been known.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LlavaGuard: An Open VLM-based Framework for Safeguarding Vision <span class="highlight-title">Dataset</span>s
  and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, Patrick Schramowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that
address the critical need for reliable guardrails in the era of large-scale
data and models. To this end, we establish a novel open framework, describing a
customizable safety taxonomy, data preprocessing, augmentation, and training
setup. For teaching a VLM safeguard on safety, we further create a multimodal
safety dataset with high-quality human expert annotations, where each image is
labeled with a safety rating, category and rationale. We also employ advanced
augmentations to support context-specific assessments. The resulting LlavaGuard
models, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the
safety compliance of visual content against flexible policies. In comprehensive
experiments, LlavaGuard outperforms both state-of-the-art safeguards and VLMs
in accuracy and in flexibly handling different policies. Additionally, we
demonstrate LlavaGuard's performance in two real-world applications:
large-scale dataset annotation and moderation of text-to-image models. We make
our entire framework publicly available, including the dataset and model
weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at
  https://ml-research.github.io/human-centered-genai/projects/llavaguard/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Gradient Methods for Risk-Sensitive Distributional Reinforcement
  Learning with Provable Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minheng Xiao, Xian Yu, Lei Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Risk-sensitive reinforcement learning (RL) is crucial for maintaining
reliable performance in high-stakes applications. While traditional RL methods
aim to learn a point estimate of the random cumulative cost, distributional RL
(DRL) seeks to estimate the entire distribution of it, which leads to a unified
framework for handling different risk measures. However, developing policy
gradient methods for risk-sensitive DRL is inherently more complex as it
involves finding the gradient of a probability measure. This paper introduces a
new policy gradient method for risk-sensitive DRL with general coherent risk
measures, where we provide an analytical form of the probability measure's
gradient for any distribution. For practical use, we design a categorical
distributional policy gradient algorithm (CDPG) that approximates any
distribution by a categorical family supported on some fixed points. We further
provide a finite-support optimality guarantee and a finite-iteration
convergence guarantee under inexact policy evaluation and gradient estimation.
Through experiments on stochastic Cliffwalk and CartPole environments, we
illustrate the benefits of considering a risk-sensitive setting in DRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metalic: Meta-Learning In-Context with Protein Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Beck, Shikha Surana, Manus McAuliffe, Oliver Bent, Thomas D. Barrett, Juan Jose Garau Luis, Paul Duckworth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the biophysical and functional properties of proteins is essential
for in silico protein design. Machine learning has emerged as a promising
technique for such prediction tasks. However, the relative scarcity of in vitro
annotations means that these models often have little, or no, specific data on
the desired fitness prediction task. As a result of limited data, protein
language models (PLMs) are typically trained on general protein sequence
modeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness
prediction. When no task data is available, the models make strong assumptions
about the correlation between the protein sequence likelihood and fitness
scores. In contrast, we propose meta-learning over a distribution of standard
fitness prediction tasks, and demonstrate positive transfer to unseen fitness
prediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses
in-context learning and fine-tuning, when data is available, to adapt to new
tasks. Crucially, fine-tuning enables considerable generalization, even though
it is not accounted for during meta-training. Our fine-tuned models achieve
strong results with 18 times fewer parameters than state-of-the-art models.
Moreover, our method sets a new state-of-the-art in low-data settings on
ProteinGym, an established fitness-prediction benchmark. Due to data scarcity,
we believe meta-learning will play a pivotal role in advancing protein
engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at The Thirteenth International Conference on Learning
  Representations (ICLR 2025). Code is provided at
  https://github.com/instadeepai/metalic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arseniy Andreyev, Pierfrancesco Beneventano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent findings by Cohen et al., 2021, demonstrate that when training neural
networks with full-batch gradient descent with a step size of $\eta$, the
largest eigenvalue $\lambda_{\max}$ of the full-batch Hessian consistently
stabilizes at $\lambda_{\max} = 2/\eta$. These results have significant
implications for convergence and generalization. This, however, is not the case
of mini-batch stochastic gradient descent (SGD), limiting the broader
applicability of its consequences. We show that SGD trains in a different
regime we term Edge of Stochastic Stability (EoSS). In this regime, what
stabilizes at $2/\eta$ is *Batch Sharpness*: the expected directional curvature
of mini-batch Hessians along their corresponding stochastic gradients. As a
consequence $\lambda_{\max}$--which is generally smaller than Batch
Sharpness--is suppressed, aligning with the long-standing empirical observation
that smaller batches and larger step sizes favor flatter minima. We further
discuss implications for mathematical modeling of SGD trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Autoencoders Reveal Universal Feature Spaces Across Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones, making it difficult
to disentangle and match features across different models. To address this
issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics on SAE feature spaces across different LLMs. Our
experiments reveal significant similarities in SAE feature spaces across
various LLMs, providing new evidence for feature universality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearization Turns Neural Operators into Function-Valued Gaussian
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilia Magnani, Marvin Pförtner, Tobias Weber, Philipp Hennig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural operators generalize neural networks to learn mappings between
function spaces from data. They are commonly used to learn solution operators
of parametric partial differential equations (PDEs) or propagators of
time-dependent PDEs. However, to make them useful in high-stakes simulation
scenarios, their inherent predictive error must be quantified reliably. We
introduce LUNO, a novel framework for approximate Bayesian uncertainty
quantification in trained neural operators. Our approach leverages model
linearization to push (Gaussian) weight-space uncertainty forward to the neural
operator's predictions. We show that this can be interpreted as a probabilistic
version of the concept of currying from functional programming, yielding a
function-valued (Gaussian) random process belief. Our framework provides a
practical yet theoretically sound way to apply existing Bayesian deep learning
methods such as the linearized Laplace approximation to neural operators. Just
as the underlying neural operator, our approach is resolution-agnostic by
design. The method adds minimal prediction overhead, can be applied post-hoc
without retraining the network, and scales to large models and datasets. We
evaluate these aspects in a case study on Fourier neural operators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theoretical guarantees on the best-of-n alignment policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simple and effective method for the inference-time alignment of generative
models is the best-of-$n$ policy, where $n$ samples are drawn from a reference
policy, ranked based on a reward function, and the highest ranking one is
selected. A commonly used analytical expression in the literature claims that
the KL divergence between the best-of-$n$ policy and the reference policy is
equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show
that it is an upper bound on the actual KL divergence. We also explore the
tightness of this upper bound in different regimes, and propose a new estimator
for the KL divergence and empirically show that it provides a tight
approximation. We also show that the win rate of the best-of-$n$ policy against
the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the
tightness of this characterization. We conclude with analyzing the tradeoffs
between win rate and KL divergence of the best-of-$n$ alignment policy, which
demonstrate that very good tradeoffs are achievable with $n < 1000$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Offline Value Function Learning with Bisimulation-based
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brahma S. Pavse, Yudong Chen, Qiaomin Xie, Josiah P. Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, offline value function learning is the procedure
of using an offline dataset to estimate the expected discounted return from
each state when taking actions according to a fixed target policy. The
stability of this procedure, i.e., whether it converges to its fixed-point,
critically depends on the representations of the state-action pairs. Poorly
learned representations can make value function learning unstable, or even
divergent. Therefore, it is critical to stabilize value function learning by
explicitly shaping the state-action representations. Recently, the class of
bisimulation-based algorithms have shown promise in shaping representations for
control. However, it is still unclear if this class of methods can stabilize
value function learning. In this work, we investigate this question and answer
it affirmatively. We introduce a bisimulation-based algorithm called kernel
representations for offline policy evaluation (KROPE). KROPE uses a kernel to
shape state-action representations such that state-action pairs that have
similar immediate rewards and lead to similar next state-action pairs under the
target policy also have similar representations. We show that KROPE: 1) learns
stable representations and 2) leads to lower value error than baselines. Our
analysis provides new theoretical insight into the stability properties of
bisimulation-based methods and suggests that practitioners can use these
methods for stable and accurate evaluation of offline reinforcement learning
agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning positional encodings in <span class="highlight-title">transformer</span>s depends on initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Ito, Luca Cocchi, Tim Klinger, Parikshit Ram, Murray Campbell, Luke Hearne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is central to the transformer's ability to capture
complex dependencies between tokens of an input sequence. Key to the successful
application of the attention mechanism in transformers is its choice of
positional encoding (PE). The PE provides essential information that
distinguishes the position and order amongst tokens in a sequence. Most prior
investigations of PE effects on generalization were tailored to 1D input
sequences, such as those presented in natural language, where adjacent tokens
(e.g., words) are highly related. In contrast, many real world tasks involve
datasets with highly non-trivial positional arrangements, such as datasets
organized in multiple spatial dimensions, or datasets for which ground truth
positions are not known, such as in biological data. Here we study the
importance of learning accurate PE for problems which rely on a non-trivial
arrangement of input tokens. Critically, we find that the choice of
initialization of a learnable PE greatly influences its ability to learn
accurate PEs that lead to enhanced generalization. We empirically demonstrate
our findings in three experiments: 1) A 2D relational reasoning task; 2) A
nonlinear stochastic network simulation; 3) A real world 3D neuroscience
dataset, applying interpretability analyses to verify the learning of accurate
PEs. Overall, we find that a learned PE initialized from a small-norm
distribution can 1) uncover interpretable PEs that mirror ground truth
positions in multiple dimensions, and 2) lead to improved downstream
generalization in empirical evaluations. Importantly, choosing an ill-suited PE
can be detrimental to both model interpretability and generalization. Together,
our results illustrate the feasibility of learning identifiable and
interpretable PEs for enhanced generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoElicit: Using Large Language Models for Expert Prior Elicitation in
  Predictive Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17284v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17284v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) acquire a breadth of information across various
domains. However, their computational complexity, cost, and lack of
transparency often hinder their direct application for predictive tasks where
privacy and interpretability are paramount. In fields such as healthcare,
biology, and finance, specialised and interpretable linear models still hold
considerable value. In such domains, labelled data may be scarce or expensive
to obtain. Well-specified prior distributions over model parameters can reduce
the sample complexity of learning through Bayesian inference; however,
eliciting expert priors can be time-consuming. We therefore introduce
AutoElicit to extract knowledge from LLMs and construct priors for predictive
models. We show these priors are informative and can be refined using natural
language. We perform a careful study contrasting AutoElicit with in-context
learning and demonstrate how to perform model selection between the two
methods. We find that AutoElicit yields priors that can substantially reduce
error over uninformative priors, using fewer labels, and consistently
outperform in-context learning. We show that AutoElicit saves over 6 months of
labelling effort when building a new predictive model for urinary tract
infections from sensor recordings of people living with dementia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Learn-then-Test: Statistically Valid and Efficient
  Hyperparameter Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Zecchin, Sangwoo Park, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter
selection procedure that provides finite-sample statistical guarantees on the
population risk of AI models. Unlike the existing learn-then-test (LTT)
technique, which relies on conventional p-value-based multiple hypothesis
testing (MHT), aLTT implements sequential data-dependent MHT with early
termination by leveraging e-processes. As a result, aLTT can reduce the number
of testing rounds, making it particularly well-suited for scenarios in which
testing is costly or presents safety risks. Apart from maintaining statistical
validity, in applications such as online policy selection for offline
reinforcement learning and prompt engineering, aLTT is shown to achieve the
same performance as LTT while requiring only a fraction of the testing rounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant Neural Tangent Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Misof, Pan Kessel, Jan E. Gerken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Little is known about the training dynamics of equivariant neural networks,
in particular how it compares to data augmented training of their
non-equivariant counterparts. Recently, neural tangent kernels (NTKs) have
emerged as a powerful tool to analytically study the training dynamics of wide
neural networks. In this work, we take an important step towards a theoretical
understanding of training dynamics of equivariant models by deriving neural
tangent kernels for a broad class of equivariant architectures based on group
convolutions. As a demonstration of the capabilities of our framework, we show
an interesting relationship between data augmentation and group convolutional
networks. Specifically, we prove that they share the same expected prediction
at all training times and even off-manifold. In this sense, they have the same
training dynamics. We demonstrate in numerical experiments that this still
holds approximately for finite-width ensembles. By implementing equivariant
NTKs for roto-translations in the plane ($G=C_{n}\ltimes\mathbb{R}^{2}$) and 3d
rotations ($G=\mathrm{SO}(3)$), we show that equivariant NTKs outperform their
non-equivariant counterparts as kernel predictors for histological image
classification and quantum mechanical property prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages + 20 pages appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regression Trees Know Calculus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Wycoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regression trees have emerged as a preeminent tool for solving real-world
regression problems due to their ability to deal with nonlinearities,
interaction effects and sharp discontinuities. In this article, we rather study
regression trees applied to well-behaved, differentiable functions, and
determine the relationship between node parameters and the local gradient of
the function being approximated. We find a simple estimate of the gradient
which can be efficiently computed using quantities exposed by popular tree
learning libraries. This allows the tools developed in the context of
differentiable algorithms, like neural nets and Gaussian processes, to be
deployed to tree-based models. To demonstrate this, we study measures of model
sensitivity defined in terms of integrals of gradients and demonstrate how to
compute them for regression trees using the proposed gradient estimates.
Quantitative and qualitative numerical experiments reveal the capability of
gradients estimated by regression trees to improve predictive analysis, solve
tasks in uncertainty quantification, and provide interpretation of model
behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Better math (asymptotic rate instead of just consistency) and
  reorganization of the exposition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalisable Time Series Understanding Across Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Özgün Turgut, Philip Müller, Martin J. Menten, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in natural language processing and computer vision,
driven by efficient pre-training on large datasets, have enabled foundation
models to excel on a wide range of tasks. However, this potential has not yet
been fully realised in time series analysis, as existing methods fail to
address the heterogeneity in large time series corpora. Prevalent in domains
ranging from medicine to finance, time series vary substantially in
characteristics such as variate count, inter-variate relationships, temporal
patterns, and sampling frequency. To address this, we introduce a novel
pre-training paradigm specifically designed to handle time series
heterogeneity. We propose a tokeniser with learnable domain signatures, a dual
masking strategy, and a normalised cross-correlation loss, enabling our open
model for general time series analysis (OTiS) to efficiently learn from large
time series corpora. Extensive benchmarking on diverse tasks, such as
classification, regression, and forecasting, demonstrates that OTiS outperforms
state-of-the-art baselines. Our code and pre-trained weights are available at
https://github.com/oetu/otis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: On-Premises LLM Deployment Demands a Middle Path: Preserving
  Privacy Without Sacrificing Model Confidentiality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbo Huang, Yihan Li, Bowen Jiang, Lin Liu, Bo Jiang, Ruoyu Sun, Zhuotao Liu, Shiyu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current LLM customization typically relies on two deployment strategies:
closed-source APIs, which require users to upload private data to external
servers, and open-weight models, which allow local fine-tuning but pose misuse
risks. In this position paper, we argue that (1) deploying closed-source LLMs
within user-controlled infrastructure (\textit{on-premises deployment})
enhances data privacy and mitigates misuse risks, and (2) a well-designed
on-premises deployment must ensure model confidentiality -- by preventing model
theft -- and offer privacy-preserving customization. Prior research on small
models has explored securing only the output layer within hardware-secured
devices to balance confidentiality and customization efficiency. However, we
show that this approach is insufficient for defending large-scale LLMs against
distillation attacks. We therefore introduce a {semi-open deployment framework}
that secures only a few, carefully chosen layers, achieving distillation
resistance comparable to fully secured models while preserving fine-tuning
flexibility. Through extensive experiments, we show that securing bottom layers
significantly reduces functional extraction risks. Our findings demonstrate
that privacy and confidentiality can coexist, paving the way for secure
on-premises AI deployment that balances usability and protection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages for main content of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Smart Buildings Control Suite: A Diverse Open Source Benchmark to
  Evaluate and Scale HVAC Control Policies for Sustainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judah Goldfeder, Victoria Dean, Zixin Jiang, Xuezheng Wang, Bing dong, Hod Lipson, John Sipple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commercial buildings account for 17% of U.S. carbon emissions, with roughly
half of that from Heating, Ventilation, and Air Conditioning (HVAC). HVAC
devices form a complex thermodynamic system, and while Model Predictive Control
and Reinforcement Learning have been used to optimize control policies, scaling
to thousands of buildings remains a significant unsolved challenge. Most
current algorithms are over-optimized for specific buildings and rely on
proprietary data or hard-to-configure simulations. We present the Smart
Buildings Control Suite, the first open source interactive HVAC control
benchmark with a focus on solutions that scale. It consists of 3 components:
real-world telemetric data extracted from 11 buildings over 6 years, a
lightweight data-driven simulator for each building, and a modular Physically
Informed Neural Network (PINN) building model as a simulator alternative. The
buildings span a variety of climates, management systems, and sizes, and both
the simulator and PINN easily scale to new buildings, ensuring solutions using
this benchmark are robust to these factors and only reliant on fully scalable
building models. This represents a major step towards scaling HVAC optimization
from the lab to buildings everywhere. To facilitate use, our benchmark is
compatible with the Gym standard, and our data is part of TensorFlow Datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Targeted Vaccine: Safety Alignment for Large Language Models against
  Harmful Fine-Tuning via Layer-wise Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harmful fine-tuning attack poses a serious threat to the online fine-tuning
service. Vaccine, a recent alignment-stage defense, applies uniform
perturbation to all layers of embedding to make the model robust to the
simulated embedding drift. However, applying layer-wise uniform perturbation
may lead to excess perturbations for some particular safety-irrelevant layers,
resulting in defense performance degradation and unnecessary memory
consumption. To address this limitation, we propose Targeted Vaccine
(T-Vaccine), a memory-efficient safety alignment method that applies
perturbation to only selected layers of the model. T-Vaccine follows two core
steps: First, it uses gradient norm as a statistical metric to identify the
safety-critical layers. Second, instead of applying uniform perturbation across
all layers, T-Vaccine only applies perturbation to the safety-critical layers
while keeping other layers frozen during training. Results show that T-Vaccine
outperforms Vaccine in terms of both defense effectiveness and resource
efficiency. Comparison with other defense baselines, e.g., RepNoise and TAR
also demonstrate the superiority of T-Vaccine. Notably, T-Vaccine is the first
defense that can address harmful fine-tuning issues for a 7B pre-trained models
trained on consumer GPUs with limited memory (e.g., RTX 4090). Our code is
available at https://github.com/Lslland/T-Vaccine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Submodular Framework for Structured-Sparse Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyushi Manupriya, Pratik Jawanpuria, Karthik S. Gurumoorthy, SakethaNath Jagarlapudi, Bamdev Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unbalanced optimal transport (UOT) has recently gained much attention due to
its flexible framework for handling un-normalized measures and its robustness
properties. In this work, we explore learning (structured) sparse transport
plans in the UOT setting, i.e., transport plans have an upper bound on the
number of non-sparse entries in each column (structured sparse pattern) or in
the whole plan (general sparse pattern). We propose novel sparsity-constrained
UOT formulations building on the recently explored maximum mean discrepancy
based UOT. We show that the proposed optimization problem is equivalent to the
maximization of a weakly submodular function over a uniform matroid or a
partition matroid. We develop efficient gradient-based discrete greedy
algorithms and provide the corresponding theoretical guarantees. Empirically,
we observe that our proposed greedy algorithms select a diverse support set and
we illustrate the efficacy of the proposed approach in various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17426v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17426v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxu Meng, Pingzhi Tang, Fan jiang, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only models generate tokens autoregressively by caching key/value
vectors, but as the cache grows, inference becomes memory-bound. To address
this issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel
approach that treats pairs of attention layers as a set of low-rank
decompositions. CLOVER applies Singular Value Decomposition (SVD) to the \( Q
\)-\( K \) and \( V \)-\( O \) pairs within each attention head. The resulting
singular values can either guide pruning or serve as trainable parameters for
efficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,
these values are reintegrated into the model without increasing its parameter
count. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,
Whisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results
demonstrate that CLOVER significantly improves pruning efficiency. For
instance, the perplexity of pruning 70\% of the \( Q \)-\( K \) pairs in GPT-2
XL is similar to that of pruning just 8\% with vanilla methods. Fine-tuning the
singular values further results in a full-rank update, outperforming
state-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\%, 5.5\%, 3.8\%,
and 0.7\%, respectively, on eight commonsense tasks for LLaMA-2 7B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/GraphPKU/PiSSA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rare Event Detection in Imbalanced Multi-Class <span class="highlight-title">Dataset</span>s Using an Optimal
  MIP-Based Ensemble Weighting Approach <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tertytchny, Georgios L. Stavrinides, Maria K. Michael
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenges of imbalanced multi-class datasets typically used
for rare event detection in critical cyber-physical systems, we propose an
optimal, efficient, and adaptable mixed integer programming (MIP) ensemble
weighting scheme. Our approach leverages the diverse capabilities of the
classifier ensemble on a granular per class basis, while optimizing the weights
of classifier-class pairs using elastic net regularization for improved
robustness and generalization. Additionally, it seamlessly and optimally
selects a predefined number of classifiers from a given set. We evaluate and
compare our MIP-based method against six well-established weighting schemes,
using representative datasets and suitable metrics, under various ensemble
sizes. The experimental results reveal that MIP outperforms all existing
approaches, achieving an improvement in balanced accuracy ranging from 0.99% to
7.31%, with an overall average of 4.53% across all datasets and ensemble sizes.
Furthermore, it attains an overall average increase of 4.63%, 4.60%, and 4.61%
in macro-averaged precision, recall, and F1-score, respectively, while
maintaining computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 39th AAAI Conference on
  Artificial Intelligence (AAAI-25). This version includes the supplementary
  material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Expressiveness of Multi-Neuron Convex Relaxations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Mao, Yani Zhang, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To provide robustness guarantees, neural network certification methods
heavily rely on convex relaxations. The imprecision of these convex
relaxations, however, is a major obstacle: even the most precise single-neuron
relaxation is incomplete for general ReLU networks, a phenomenon referred to as
the single-neuron convex barrier. While heuristic instantiations of
multi-neuron relaxations have been proposed to circumvent this barrier in
practice, their theoretical properties remain largely unknown. In this work, we
conduct the first rigorous study of the expressiveness of multi-neuron
relaxations. We first show that the ``$\max$'' function in $\mathbb{R}^d$ can
be encoded by a ReLU network and exactly bounded by a multi-neuron relaxation,
which is impossible for any single-neuron relaxation. Further, we prove that
multi-neuron relaxations can be turned into complete verifiers by
semantic-preserving structural transformations or by input space partitioning
that enjoys improved worst-case partition complexity. We also show that without
these augmentations, the completeness guarantee can no longer be obtained, and
the relaxation error of every multi-neuron relaxation can be unbounded. To the
best of our knowledge, this is the first work to provide an extensive
characterization of multi-neuron relaxations and their expressiveness in neural
network certification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Branches: Efficiently Seeking Optimal Sparse Decision Trees with AO* 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02175v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02175v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayman Chaouki, Jesse Read, Albert Bifet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine
Learning, yet it poses a formidable optimisation challenge. Practical
algorithms have recently emerged, primarily leveraging Dynamic Programming and
Branch & Bound. However, most of these approaches rely on a Depth-First-Search
strategy, which is inefficient when searching for DTs at high depths and
requires the definition of a maximum depth hyperparameter. Best-First-Search
was also employed by other methods to circumvent these issues. The downside of
this strategy is its higher memory consumption, as such, it has to be designed
in a fully efficient manner that takes full advantage of the problem's
structure. We formulate the problem as an AND/OR graph search which we solve
with a novel AO*-type algorithm called Branches. We prove both optimality and
complexity guarantees for Branches and we show that it is more efficient than
the state of the art theoretically and on a variety of experiments.
Furthermore, Branches supports non-binary features unlike the other methods, we
show that this property can further induce larger gains in computational
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint is currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-Level Topological Representation Learning on Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent P. Grande, Michael T. Schaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological Data Analysis (TDA) allows us to extract powerful topological and
higher-order information on the global shape of a data set or point cloud.
Tools like Persistent Homology or the Euler Transform give a single complex
description of the global structure of the point cloud. However, common machine
learning applications like classification require point-level information and
features to be available. In this paper, we bridge this gap and propose a novel
method to extract node-level topological features from complex point clouds
using discrete variants of concepts from algebraic topology and differential
geometry. We verify the effectiveness of these topological point features
(TOPF) on both synthetic and real-world data and study their robustness under
noise and heterogeneous sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 18 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cautious Optimizers: Improving Training with One Line of Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhao Liang, Lizhang Chen, Bo Liu, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AdamW has been the default optimizer for transformer pretraining. For many
years, our community searched for faster and more stable optimizers with only
constrained positive outcomes. In this work, we propose a single-line
modification in Pytorch to any momentum-based optimizer, which we rename
cautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that
this modification preserves Adam's Hamiltonian function and it does not break
the convergence guarantee under the Lyapunov analysis. In addition, a whole new
family of optimizers is revealed by our theoretical insight. Among them, we
pick the simplest one for empirical experiments, showing not only speed-up on
Llama and MAE pretraining up to $1.47$ times, but also better results in LLM
post-training tasks. Code is available at
https://github.com/kyleliang919/C-Optim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG-Language Modeling for Pathology Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07480v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07480v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Gijsen, Kerstin Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal language modeling has enabled breakthroughs for representation
learning, yet remains unexplored in the realm of functional brain data for
pathology detection. This paper pioneers EEG-language models (ELMs) trained on
clinical reports and 15000 EEGs. We propose to combine multimodal alignment in
this novel domain with timeseries cropping and text segmentation, enabling an
extension based on multiple instance learning to alleviate misalignment between
irrelevant EEG or text segments. Our multimodal models significantly improve
pathology detection compared to EEG-only models across four evaluations and for
the first time enable zero-shot classification as well as retrieval of both
neural signals and reports. In sum, these results highlight the potential of
ELMs, representing significant progress for clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiSSL: A Bilevel Optimization Framework for Enhancing the Alignment
  Between <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing and Downstream Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustav Wagner Zakarias, Lars Kai Hansen, Zheng-Hua Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents BiSSL, a novel training framework that utilizes bilevel
optimization to enhance the alignment between the pretext pre-training and
downstream fine-tuning stages in self-supervised learning. BiSSL formulates the
pretext and downstream task objectives as the lower- and upper-level objectives
in a bilevel optimization problem and serves as an intermediate training stage
within the self-supervised learning pipeline. By explicitly modeling the
interdependence of these training stages, BiSSL facilitates enhanced
information sharing between them, ultimately leading to a backbone parameter
initialization that is better aligned for the downstream task. We propose a
versatile training algorithm that alternates between optimizing the two
objectives defined in BiSSL, which is applicable to a broad range of pretext
and downstream tasks. Using SimCLR and Bootstrap Your Own Latent to pre-train
ResNet-50 backbones on the ImageNet dataset, we demonstrate that our proposed
framework significantly outperforms the conventional self-supervised learning
pipeline on the vast majority of 12 downstream image classification datasets,
as well as on object detection. Visualizations of the backbone features provide
further evidence that BiSSL improves the downstream task alignment of the
backbone features prior to fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Machine Learning Approach to Automatic Fall Detection of Soldiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro Soares, Gustavo Venturini, José Gomes, Jonathan Efigenio, Pablo Rangel, Pedro Gonzalez, Joel dos Santos, Diego Brandão, Eduardo Bezerra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Military personnel and security agents often face significant physical risks
during conflict and engagement situations, particularly in urban operations.
Ensuring the rapid and accurate communication of incidents involving injuries
is crucial for the timely execution of rescue operations. This article presents
research conducted under the scope of the Brazilian Navy's ``Soldier of the
Future'' project, focusing on the development of a Casualty Detection System to
identify injuries that could incapacitate a soldier and lead to severe blood
loss. The study specifically addresses the detection of soldier falls, which
may indicate critical injuries such as hypovolemic hemorrhagic shock. To
generate the publicly available dataset, we used smartwatches and smartphones
as wearable devices to collect inertial data from soldiers during various
activities, including simulated falls. The data were used to train 1D
Convolutional Neural Networks (CNN1D) with the objective of accurately
classifying falls that could result from life-threatening injuries. We explored
different sensor placements (on the wrists and near the center of mass) and
various approaches to using inertial variables, including linear and angular
accelerations. The neural network models were optimized using Bayesian
techniques to enhance their performance. The best-performing model and its
results, discussed in this article, contribute to the advancement of automated
systems for monitoring soldier safety and improving response times in
engagement scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benign Overfitting in Token Selection of Attention Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keitaro Sakamoto, Issei Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanism is a fundamental component of the transformer model and
plays a significant role in its success. However, the theoretical understanding
of how attention learns to select tokens is still an emerging area of research.
In this work, we study the training dynamics and generalization ability of the
attention mechanism under classification problems with label noise. We show
that, with the characterization of signal-to-noise ratio (SNR), the token
selection of attention mechanism achieves benign overfitting, i.e., maintaining
high generalization performance despite fitting label noise. Our work also
demonstrates an interesting delayed acquisition of generalization after an
initial phase of overfitting. Finally, we provide experiments to support our
theoretical analysis using both synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Largely updated from the previous version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Average Certified Radius is a Poor Metric for Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Sun, Yuhao Mao, Mark Niklas Müller, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized smoothing is a popular approach for providing certified robustness
guarantees against adversarial attacks, and has become an active area of
research. Over the past years, the average certified radius (ACR) has emerged
as the most important metric for comparing methods and tracking progress in the
field. However, in this work, for the first time we show that ACR is a poor
metric for evaluating robustness guarantees provided by randomized smoothing.
We theoretically prove not only that a trivial classifier can have arbitrarily
large ACR, but also that ACR is much more sensitive to improvements on easy
samples than on hard ones. Empirically, we confirm that existing training
strategies, though improving ACR with different approaches, reduce the model's
robustness on hard samples consistently. To strengthen our conclusion, we
propose strategies, including explicitly discarding hard samples, reweighting
the dataset with approximate certified radius, and extreme optimization for
easy samples, to achieve state-of-the-art ACR, without training for robustness
on the full data distribution. Overall, our results suggest that ACR has
introduced a strong undesired bias to the field, and its application should be
discontinued when evaluating randomized smoothing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models are In-context Preference Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Yu, Qixin Tan, Hong Lu, Jiaxuan Gao, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference-based reinforcement learning is an effective way to handle tasks
where rewards are hard to specify but can be exceedingly inefficient as
preference learning is often tabula rasa. We demonstrate that Large Language
Models (LLMs) have native preference-learning capabilities that allow them to
achieve sample-efficient preference learning, addressing this challenge. We
propose In-Context Preference Learning (ICPL), which uses in-context learning
capabilities of LLMs to reduce human query inefficiency. ICPL uses the task
description and basic environment code to create sets of reward functions which
are iteratively refined by placing human feedback over videos of the resultant
policies into the context of an LLM and then requesting better rewards. We
first demonstrate ICPL's effectiveness through a synthetic preference study,
providing quantitative evidence that it significantly outperforms baseline
preference-based methods with much higher performance and orders of magnitude
greater efficiency. We observe that these improvements are not solely coming
from LLM grounding in the task but that the quality of the rewards improves
over time, indicating preference learning capabilities. Additionally, we
perform a series of real human preference-learning trials and observe that ICPL
extends beyond synthetic settings and can work effectively with
humans-in-the-loop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Transient Structure in In-Context Linear Regression
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Carroll, Jesse Hoogland, Matthew Farrugia-Roberts, Daniel Murfet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep neural networks display striking examples of rich internal
computational structure. Uncovering principles governing the development of
such structure is a priority for the science of deep learning. In this paper,
we explore the transient ridge phenomenon: when transformers are trained on
in-context linear regression tasks with intermediate task diversity, they
initially behave like ridge regression before specializing to the tasks in
their training distribution. This transition from a general solution to a
specialized solution is revealed by joint trajectory principal component
analysis. Further, we draw on the theory of Bayesian internal model selection
to suggest a general explanation for the phenomena of transient structure in
transformers, based on an evolving tradeoff between loss and complexity. We
empirically validate this explanation by measuring the model complexity of our
transformers as defined by the local learning coefficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Nowak, Wojciech Jarosz, Peter Chin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D scene from images is challenging due to the different
ways light interacts with surfaces depending on the viewer's position and the
surface's material. In classical computer graphics, materials can be classified
as diffuse or specular, interacting with light differently. The standard 3D
Gaussian Splatting model struggles to represent view-dependent content, since
it cannot differentiate an object within the scene from the light interacting
with its specular surfaces, which produce highlights or reflections. In this
paper, we propose to extend the 3D Gaussian Splatting model by introducing an
additional symmetric matrix to enhance the opacity representation of each 3D
Gaussian. This improvement allows certain Gaussians to be suppressed based on
the viewer's perspective, resulting in a more accurate representation of
view-dependent reflections and specular highlights without compromising the
scene's integrity. By allowing the opacity to be view dependent, our enhanced
model achieves state-of-the-art performance on Mip-Nerf, Tanks&Temples, Deep
Blending, and Nerf-Synthetic datasets without a significant loss in rendering
speed, achieving >60FPS, and only incurring a minimal increase in memory used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAN: Hypothesizing Long-Term Synaptic Development and Neural Engram
  Mechanism in Scalable Model's Parameter-Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaole Dai, Chun-Kai Fan, Yiming Tang, Zhi Zhang, Yuan Zhang, Yulu Gan, Qizhe Zhang, Cheng-Ching Tseng, Shanghang Zhang, Tiejun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in Parameter-Efficient Fine-Tuning (PEFT) bridged the performance
gap with Full Fine-Tuning (FFT) through sophisticated analysis of pre-trained
parameter spaces. Starting from drawing insights from Neural Engrams (NE) in
Biological Neural Networks (BNNs), we establish a connection between the
low-rank property observed during PEFT's parameter space shifting and
neurobiological mechanisms. This observation leads to our proposed method,
Synapse and Neuron (SAN), which decomposes and propagates scaling components
from anterior feature adjusting vectors towards posterior weight matrices. Our
approach is theoretically grounded in Long-Term Potentiation/Depression (LTP/D)
phenomena, which govern synapse development through neurotransmitter release
modulation. Extensive experiments demonstrate its effectiveness: on
\textbf{vision tasks} across VTAB, FGVC, and GIC (25 datasets) using ViT, SwinT
and ConvNeXt, SAN outperforms FFT up to 8.7% and LoRA by 3.2%; on language
tasks using Commonsense Reasoning (8 datasets) with LLaMA models (all
generations), surpassing ChatGPT up to 8.5% and LoRA by 4.7%; on
visual-language tasks using Mixed Visual Instruction (7 datasets) with LLaVA
models, it exceeds FFT up to 2.4% and LoRA by 1.9%. Our code and W&B log will
be released in https://github.com/daviddaiiiii/SAN-PEFT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Explainability Approach for Human-Aware Robots in
  Multi-Party <span class="highlight-title">Conversation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iveta Bečková, Štefan Pócoš, Giulia Belgiovine, Marco Matarese, Omar Eldardeer, Alessandra Sciutti, Carlo Mazzola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The addressee estimation (understanding to whom somebody is talking) is a
fundamental task for human activity recognition in multi-party conversation
scenarios. Specifically, in the field of human-robot interaction, it becomes
even more crucial to enable social robots to participate in such interactive
contexts. However, it is usually implemented as a binary classification task,
restricting the robot's capability to estimate whether it was addressed
\review{or not, which} limits its interactive skills. For a social robot to
gain the trust of humans, it is also important to manifest a certain level of
transparency and explainability. Explainable artificial intelligence thus plays
a significant role in the current machine learning applications and models, to
provide explanations for their decisions besides excellent performance. In our
work, we a) present an addressee estimation model with improved performance in
comparison with the previous state-of-the-art; b) further modify this model to
include inherently explainable attention-based segments; c) implement the
explainable addressee estimation as part of a modular cognitive architecture
for multi-party conversation in an iCub robot; d) validate the real-time
performance of the explainable model in multi-party human-robot interaction; e)
propose several ways to incorporate explainability and transparency in the
aforementioned architecture; and f) perform an online user study to analyze the
effect of various explanations on how human participants perceive the robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32pp (+6pp sup.mat.) Accepted in Computer Vision and Image
  Understanding Journal on January 23, 2025. This research received funding
  Horizon-Europe TERAIS project (G.A. 101079338) and Slovak Research and
  Development Agency, project no. APVV-21-0105</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Rethinking Explainable Machine Learning as Applied Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bordt, Eric Raidl, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly growing literature on explanation algorithms, it often remains
unclear what precisely these algorithms are for and how they should be used. In
this position paper, we argue for a novel and pragmatic perspective:
Explainable machine learning needs to recognize its parallels with applied
statistics. Concretely, explanations are statistics of high-dimensional
functions, and we should think about them analogously to traditional
statistical quantities. Among others, this implies that we must think carefully
about the matter of interpretation, or how the explanations relate to intuitive
questions that humans have about the world. The fact that this is scarcely
being discussed in research papers is one of the main drawbacks of the current
literature. Luckily, the analogy between explainable machine learning and
applied statistics suggests fruitful ways for how research practices can be
improved.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verifying Cross-modal Entity <span class="highlight-title">Consist</span>ency in News using Vision-language
  Models <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Tahmasebi, David Ernst, Eric Müller-Budack, Ralph Ewerth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web has become a crucial source of information, but it is also used to
spread disinformation, often conveyed through multiple modalities like images
and text. The identification of inconsistent cross-modal information, in
particular entities such as persons, locations, and events, is critical to
detect disinformation. Previous works either identify out-of-context
disinformation by assessing the consistency of images to the whole document,
neglecting relations of individual entities, or focus on generic entities that
are not relevant to news. So far, only few approaches have addressed the task
of validating entity consistency between images and text in news. However, the
potential of large vision-language models (LVLMs) has not been explored yet. In
this paper, we propose an LVLM-based framework for verifying Cross-modal Entity
Consistency~(LVLM4CEC), to assess whether persons, locations and events in news
articles are consistent across both modalities. We suggest effective prompting
strategies for LVLMs for entity verification that leverage reference images
crawled from web. Moreover, we extend three existing datasets for the task of
entity verification in news providing manual ground-truth data. Our results
show the potential of LVLMs for automating cross-modal entity verification,
showing improved accuracy in identifying persons and events when using evidence
images. Moreover, our method outperforms a baseline for location and event
verification in documents. The datasets and source code are available on GitHub
at https://github.com/TIBHannover/LVLM4CEC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in: European Conference on Information
  Retrieval (ECIR) 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-30T00:00:00Z">2025-01-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rope to Nope and Back Again: A New Hybrid Attention Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, Acyr Locatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context large language models (LLMs) have achieved remarkable
advancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et
al., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et
al., 2023). By adjusting RoPE parameters and incorporating training data with
extended contexts, we can train performant models with considerably longer
input sequences. However, existing RoPE-based methods exhibit performance
limitations when applied to extended context lengths. This paper presents a
comprehensive analysis of various attention mechanisms, including RoPE, No
Positional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying
their strengths and shortcomings in long-context modeling. Our investigation
identifies distinctive attention patterns in these methods and highlights their
impact on long-context performance, providing valuable insights for
architectural design. Building on these findings, we propose a novel
architectural based on a hybrid attention mechanism that not only surpasses
conventional RoPE-based transformer models in long context tasks but also
achieves competitive performance on benchmarks requiring shorter context
lengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data
  Contamination's Impact on Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Yusuf Kocyigit, Eleftheria Briakou, Daniel Deutsch, Jiaming Luo, Colin Cherry, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data contamination -- the accidental consumption of evaluation examples
within the pre-training data -- can undermine the validity of evaluation
benchmarks. In this paper, we present a rigorous analysis of the effects of
contamination on language models at 1B and 8B scales on the machine translation
task. Starting from a carefully decontaminated train-test split, we
systematically introduce contamination at various stages, scales, and data
formats to isolate its effect and measure its impact on performance metrics.
Our experiments reveal that contamination with both source and target
substantially inflates BLEU scores, and this inflation is 2.5 times larger (up
to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and
target-only contamination generally produce smaller, less consistent
over-estimations. Finally, we study how the temporal distribution and frequency
of contaminated samples influence performance over-estimation across languages
with varying degrees of data resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Fake News Barrier: Deep Learning Approaches in Bangla
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pronoy Kumar Mondal, Sadman Sadik Khan, Md. Masud Rana, Shahriar Sultan Ramit, Abdus Sattar, Md. Sadekur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of digital stages has greatly compounded the dispersal
of untrue data, dissolving certainty and judgment in society, especially among
the Bengali-speaking community. Our ponder addresses this critical issue by
presenting an interesting strategy that utilizes a profound learning
innovation, particularly the Gated Repetitive Unit (GRU), to recognize fake
news within the Bangla dialect. The strategy of our proposed work incorporates
intensive information preprocessing, which includes lemmatization,
tokenization, and tending to course awkward nature by oversampling. This comes
about in a dataset containing 58,478 passages. We appreciate the creation of a
demonstration based on GRU (Gated Repetitive Unit) that illustrates remarkable
execution with a noteworthy precision rate of 94%. This ponder gives an
intensive clarification of the methods included in planning the information,
selecting the show, preparing it, and assessing its execution. The performance
of the model is investigated by reliable metrics like precision, recall, F1
score, and accuracy. The commitment of the work incorporates making a huge fake
news dataset in Bangla and a demonstration that has outperformed other Bangla
fake news location models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, THE 15th INTERNATIONAL IEEE CONFERENCE ON COMPUTING,
  COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity
  Recognition in <span class="highlight-title">Low-Resource</span> Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Politov, Oleh Shkalikov, René Jäkel, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer
between languages to identify and classify named entities, making it
particularly useful for low-resource languages. We show that the data-based
cross-lingual transfer method is an effective technique for crosslingual NER
and can outperform multilingual language models for low-resource languages.
This paper introduces two key enhancements to the annotation projection step in
cross-lingual NER for low-resource languages. First, we explore refining word
alignments using back-translation to improve accuracy. Second, we present a
novel formalized projection approach of matching source entities with extracted
target candidates. Through extensive experiments on two datasets spanning 57
languages, we demonstrated that our approach surpasses existing projectionbased
methods in low-resource settings. These findings highlight the robustness of
projection-based data transfer as an alternative to model-based methods for
crosslingual named entity recognition in lowresource languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NoDaLiDa/Baltic-HLT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Robustness of Large Language Models across Language
  Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large language models (LLMs), an increasing number of
student models have leveraged LLMs to analyze textual artifacts generated by
students to understand and evaluate their learning. These student models
typically employ pre-trained LLMs to vectorize text inputs into embeddings and
then use the embeddings to train models to detect the presence or absence of a
construct of interest. However, how reliable and robust are these models at
processing language with different levels of complexity? In the context of
learning where students may have different language backgrounds with various
levels of writing skills, it is critical to examine the robustness of such
models to ensure that these models work equally well for text with varying
levels of language complexity. Coincidentally, a few (but limited) research
studies show that the use of language can indeed impact the performance of
LLMs. As such, in the current study, we examined the robustness of several
LLM-based student models that detect student self-regulated learning (SRL) in
math problem-solving. Specifically, we compared how the performance of these
models vary using texts with high and low lexical, syntactic, and semantic
complexity measured by three linguistic measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Spoken Language as a Biomarker for Automated Screening of
  Cognitive Impairment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria R. Lima, Alexander Capstick, Fatemeh Geranmayeh, Ramin Nilforooshan, Maja Matarić, Ravi Vaidyanathan, Payam Barnaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely and accurate assessment of cognitive impairment is a major unmet need
in populations at risk. Alterations in speech and language can be early
predictors of Alzheimer's disease and related dementias (ADRD) before clinical
signs of neurodegeneration. Voice biomarkers offer a scalable and non-invasive
solution for automated screening. However, the clinical applicability of
machine learning (ML) remains limited by challenges in generalisability,
interpretability, and access to patient data to train clinically applicable
predictive models. Using DementiaBank recordings (N=291, 64% female), we
evaluated ML techniques for ADRD screening and severity prediction from spoken
language. We validated model generalisability with pilot data collected
in-residence from older adults (N=22, 59% female). Risk stratification and
linguistic feature importance analysis enhanced the interpretability and
clinical utility of predictions. For ADRD classification, a Random Forest
applied to lexical features achieved a mean sensitivity of 69.4% (95%
confidence interval (CI) = 66.4-72.5) and specificity of 83.3% (78.0-88.7). On
real-world pilot data, this model achieved a mean sensitivity of 70.0%
(58.0-82.0) and specificity of 52.5% (39.3-65.7). For severity prediction using
Mini-Mental State Examination (MMSE) scores, a Random Forest Regressor achieved
a mean absolute MMSE error of 3.7 (3.7-3.8), with comparable performance of 3.3
(3.1-3.5) on pilot data. Linguistic features associated with higher ADRD risk
included increased use of pronouns and adverbs, greater disfluency, reduced
analytical thinking, lower lexical diversity and fewer words reflecting a
psychological state of completion. Our interpretable predictive modelling
offers a novel approach for in-home integration with conversational AI to
monitor cognitive health and triage higher-risk individuals, enabling earlier
detection and intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Large Language Models for Long Clinical Text Summarization
  with Temporal <span class="highlight-title">Reasoning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Kruse, Shiyue Hu, Nicholas Derby, Yifu Wu, Samantha Stonbraker, Bingsheng Yao, Dakuo Wang, Elizabeth Goldberg, Yanjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown potential for
transforming data processing in healthcare, particularly in understanding
complex clinical narratives. This study evaluates the efficacy of zero-shot
LLMs in summarizing long clinical texts that require temporal reasoning, a
critical aspect for comprehensively capturing patient histories and treatment
trajectories. We applied a series of advanced zero-shot LLMs to extensive
clinical documents, assessing their ability to integrate and accurately reflect
temporal dynamics without prior task-specific training. While the models
efficiently identified key temporal events, they struggled with chronological
coherence over prolonged narratives. The evaluation, combining quantitative and
qualitative methods, highlights the strengths and limitations of zero-shot LLMs
in clinical text summarization. The results suggest that while promising,
zero-shot LLMs require further refinement to effectively support clinical
decision-making processes, underscoring the need for enhanced model training
approaches that better capture the nuances of temporal information in long
context medical documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable
abilities in complex reasoning tasks by scaling test-time compute and
exhibiting human-like deep thinking. However, we identify a phenomenon we term
underthinking, where o1-like LLMs frequently switch between different reasoning
thoughts without sufficiently exploring promising paths to reach a correct
solution. This behavior leads to inadequate depth of reasoning and decreased
performance, particularly on challenging mathematical problems. To
systematically analyze this issue, we conduct experiments on three challenging
test sets and two representative open-source o1-like models, revealing that
frequent thought switching correlates with incorrect responses. We introduce a
novel metric to quantify underthinking by measuring token efficiency in
incorrect answers. To address underthinking, we propose a decoding strategy
with thought switching penalty TIP that discourages premature transitions
between thoughts, encouraging deeper exploration of each reasoning path.
Experimental results demonstrate that our approach improves accuracy across
challenging datasets without requiring model fine-tuning. Our findings
contribute to understanding reasoning inefficiencies in o1-like LLMs and offer
a practical solution to enhance their problem-solving capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R.I.P.: Better Models by Survival of the Fittest <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data quality is one of the most important drivers of final model
quality. In this work, we introduce a method for evaluating data integrity
based on the assumption that low-quality input prompts result in high variance
and low quality responses. This is achieved by measuring the rejected response
quality and the reward gap between the chosen and rejected preference pair. Our
method, Rejecting Instruction Preferences (RIP) can be used to filter prompts
from existing training sets, or to make high quality synthetic datasets,
yielding large performance gains across various benchmarks compared to
unfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win
Rate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama
3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th
place to 6th overall in the leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented
  LLM-based Retrieval Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world open-domain questions can be complicated, particularly when
answering them involves information from multiple information sources. LLMs
have demonstrated impressive performance in decomposing complex tasks into
simpler steps, and previous work has used it for better retrieval in support of
complex questions. However, LLM's decomposition of questions is unaware of what
data is available and how data is organized, often leading to a sub-optimal
retrieval performance. Recent effort in agentic RAG proposes to perform
retrieval in an iterative fashion, where a followup query is derived as an
action based on previous rounds of retrieval. While this provides one way of
interacting with the data collection, agentic RAG's exploration of data is
inefficient because successive queries depend on previous results rather than
being guided by the organization of available data in the collection. To
address this problem, we propose an LLM-based retrieval method -- ARM, that
aims to better align the question with the organization of the data collection
by exploring relationships among data objects beyond matching the utterance of
the query, thus leading to a retrieve-all-at-once solution for complex queries.
We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms
standard RAG with query decomposition by up to 5.2 pt in execution accuracy and
agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and
19.3 pt higher F1 match scores compared to these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Lijun Li, Bing Cao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have achieved remarkable performance
across a wide range of tasks. However, their deployment in safety-critical
domains poses significant challenges. Existing safety fine-tuning methods,
which focus on textual or multimodal content, fall short in addressing
challenging cases or disrupt the balance between helpfulness and harmlessness.
Our evaluation highlights a safety reasoning gap: these methods lack safety
visual reasoning ability, leading to such bottlenecks. To address this
limitation and enhance both visual perception and reasoning in safety-critical
contexts, we propose a novel dataset that integrates multi-image inputs with
safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve
model performance. Specifically, we introduce the Multi-Image Safety (MIS)
dataset, an instruction-following dataset tailored for multi-image safety
scenarios, consisting of training and test splits. Our experiments demonstrate
that fine-tuning InternVL2.5-8B with MIS significantly outperforms both
powerful open-source models and API-based models in challenging multi-image
tasks requiring safety-related visual reasoning. This approach not only
delivers exceptional safety performance but also preserves general capabilities
without any trade-offs. Specifically, fine-tuning with MIS increases average
accuracy by 0.83% across five general benchmarks and reduces the Attack Success
Rate (ASR) on multiple safety benchmarks by a large margin. Data and Models are
released under:
\href{https://dripnowhy.github.io/MIS/}{\texttt{https://dripnowhy.github.io/MIS/}}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Steering for Large Language Model Alignment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning Large Language Models (LLMs) with human values and away from
undesirable behaviors (such as hallucination) has become increasingly
important. Recently, steering LLMs towards a desired behavior via activation
editing has emerged as an effective method to mitigate harmful generations at
inference-time. Activation editing modifies LLM representations by preserving
information from positive demonstrations (e.g., truthful) and minimising
information from negative demonstrations (e.g., hallucinations). When these
demonstrations come from a private dataset, the aligned LLM may leak private
information contained in those private samples. In this work, we present the
first study of aligning LLM behavior with private datasets. Our work proposes
the \textit{\underline{P}rivate \underline{S}teering for LLM
\underline{A}lignment (PSA)} algorithm to edit LLM activations with
differential privacy (DP) guarantees. We conduct extensive experiments on seven
different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and
model families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA
achieves DP guarantees for LLM alignment with minimal loss in performance,
including alignment metrics, open-ended text generation quality, and
general-purpose reasoning. We also develop the first Membership Inference
Attack (MIA) for evaluating and auditing the empirical privacy for the problem
of LLM steering via activation editing. Our attack is tailored for activation
editing and relies solely on the generated texts without their associated
probabilities. Our experiments support the theoretical guarantees by showing
improved guarantees for our \textit{PSA} algorithm compared to several existing
non-private techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Code: https://github.com/UKPLab/iclr2025-psa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming DiLoCo with overlapping communication: Towards a Distributed
  Free Lunch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Douillard, Yanislav Donchev, Keith Rush, Satyen Kale, Zachary Charles, Zachary Garrett, Gabriel Teston, Dave Lacey, Ross McIlroy, Jiajun Shen, Alexandre Ramé, Arthur Szlam, Marc'Aurelio Ranzato, Paul Barham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training of large language models (LLMs) is typically distributed across a
large number of accelerators to reduce training time. Since internal states and
parameter gradients need to be exchanged at each and every single gradient
step, all devices need to be co-located using low-latency high-bandwidth
communication links to support the required high volume of exchanged bits.
Recently, distributed algorithms like DiLoCo have relaxed such co-location
constraint: accelerators can be grouped into ``workers'', where
synchronizations between workers only occur infrequently. This in turn means
that workers can afford being connected by lower bandwidth communication links
without affecting learning quality. However, in these methods, communication
across workers still requires the same peak bandwidth as before, as the
synchronizations require all parameters to be exchanged across all workers. In
this paper, we improve DiLoCo in three ways. First, we synchronize only subsets
of parameters in sequence, rather than all at once, which greatly reduces peak
bandwidth. Second, we allow workers to continue training while synchronizing,
which decreases wall clock time. Third, we quantize the data exchanged by
workers, which further reduces bandwidth across workers. By properly combining
these modifications, we show experimentally that we can distribute training of
billion-scale parameters and reach similar quality as before, but reducing
required bandwidth by two orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in
  Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LLM) post-training, from DPO to distillation, can refine
behaviors and unlock new skills, but the open science supporting these
post-training techniques is still in its infancy. One limiting factor has been
the difficulty of conducting large-scale comparative analyses of synthetic data
generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,
the largest public chat dataset to date. We extend the existing WildChat
dataset to include responses not only from GPT, but from over 50 different
open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an
extensive comparative analysis and demonstrate the potential of this dataset by
creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3
SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples
and code are available at https://github.com/penfever/wildchat-50m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language
  Model Question Answering <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Wang, Zhiyuan Fan, Qingyun Wang, May Fung, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are pretrained on extensive multilingual corpora
to acquire both language-specific cultural knowledge and general knowledge.
Ideally, while LLMs should provide consistent responses to culture-independent
questions across languages, we observe significant performance disparities. To
address this, we explore the Cross-Lingual Self-Aligning ability of Language
Models (CALM) to align knowledge across languages. Specifically, for a given
question, we sample multiple responses across different languages, and select
the most self-consistent response as the target, leaving the remaining
responses as negative examples. We then employ direct preference optimization
(DPO) to align the model's knowledge across different languages. Evaluations on
the MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing
cross-lingual knowledge question answering, both in zero-shot and retrieval
augmented settings. We also found that increasing the number of languages
involved in CALM training leads to even higher accuracy and consistency. We
offer a qualitative analysis of how cross-lingual consistency can enhance
knowledge alignment and explore the method's generalizability. The source code
and data of this paper are available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GENIE: Generative Note Information Extraction model for structuring EHR
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaiyuan Ying, Hongyi Yuan, Jinsen Lu, Zitian Qu, Yang Zhao, Zhengyun Zhao, Isaac Kohane, Tianxi Cai, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) hold immense potential for advancing
healthcare, offering rich, longitudinal data that combines structured
information with valuable insights from unstructured clinical notes. However,
the unstructured nature of clinical text poses significant challenges for
secondary applications. Traditional methods for structuring EHR free-text data,
such as rule-based systems and multi-stage pipelines, are often limited by
their time-consuming configurations and inability to adapt across clinical
notes from diverse healthcare settings. Few systems provide a comprehensive
attribute extraction for terminologies. While giant large language models
(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,
costly, and impractical for large-scale use. To overcome these limitations, we
introduce GENIE, a Generative Note Information Extraction system that leverages
LLMs to streamline the structuring of unstructured clinical text into usable
data with standardized format. GENIE processes entire paragraphs in a single
pass, extracting entities, assertion statuses, locations, modifiers, values,
and purposes with high accuracy. Its unified, end-to-end approach simplifies
workflows, reduces errors, and eliminates the need for extensive manual
intervention. Using a robust data preparation pipeline and fine-tuned small
scale LLMs, GENIE achieves competitive performance across multiple information
extraction tasks, outperforming traditional tools like cTAKES and MetaMap and
can handle extra attributes to be extracted. GENIE strongly enhances real-world
applicability and scalability in healthcare systems. By open-sourcing the model
and test data, we aim to encourage collaboration and drive further advancements
in EHR structurization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RbFT: Robust Fine-tuning for Retrieval-Augmented <span class="highlight-title">Generation</span> against
  Retrieval Defects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved from a knowledge base. However, its
effectiveness is fundamentally constrained by the reliability of both the
retriever and the knowledge base. In real-world scenarios, imperfections in
these components often lead to the retrieval of noisy, irrelevant, or
misleading counterfactual information, ultimately undermining the
trustworthiness of RAG systems. To address this challenge, we propose Robust
Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against
retrieval defects through two targeted fine-tuning tasks. Experimental results
demonstrate that RbFT significantly improves the robustness of RAG systems
across diverse retrieval conditions, surpassing existing methods while
maintaining high inference efficiency and compatibility with other robustness
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedXpertQA: Benchmarking Expert-Level Medical <span class="highlight-title">Reasoning</span> and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Stream <span class="highlight-title">Transformer</span> (SST) : Emergent Metacognitive Behaviours
  Through Latent State Persistence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thea Aviss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the State Stream Transformer (SST), a novel LLM architecture
that reveals emergent reasoning behaviours and capabilities latent in
pretrained weights through addressing a fundamental limitation in traditional
transformer models: the lack of latent computational continuity across
autoregressive generations in the state space. SST introduces a sliding window
latent state (FFN) cache with weighted decay that maintains and evolves
persistent latent processes throughout autoregressive generations. Through
controlled experiments comparing base and SST architectures using the same
frozen weights, we demonstrate that this architectural modification alone
enables enhanced reasoning capabilities which appear best explained by some
form of potential higher-order processing, as evidenced by emergent
metacognitive behaviours. These behaviours persist under controlled conditions
designed to eliminate confounding factors such as stochastic variation or
learned response patterns. Analysis of latent state distributions and
processing dynamics provides evidence that it is solely the 'state stream' that
is responsible for these phenomena. In quantitative evaluations, the SST
achieves substantial performance improvements over the base model on two
reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\%
on ARC Challenge (0-shot CoT). These findings indicate that persistent
computation in the latent state space enables fundamentally different
information processing and internal reasoning strategies, with implications for
our understanding of artificial intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Video-grounded <span class="highlight-title">Dialogue</span> <span class="highlight-title">Dataset</span> and Metric for Event-driven Activities <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiradee Imrattanatrai, Masaki Asada, Kimihiro Hasegawa, Zhi-Qi Cheng, Ken Fukuda, Teruko Mitamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents VDAct, a dataset for a Video-grounded Dialogue on
Event-driven Activities, alongside VDEval, a session-based context evaluation
metric specially designed for the task. Unlike existing datasets, VDAct
includes longer and more complex video sequences that depict a variety of
event-driven activities that require advanced contextual understanding for
accurate response generation. The dataset comprises 3,000 dialogues with over
30,000 question-and-answer pairs, derived from 1,000 videos with diverse
activity scenarios. VDAct displays a notably challenging characteristic due to
its broad spectrum of activity scenarios and wide range of question types.
Empirical studies on state-of-the-art vision foundation models highlight their
limitations in addressing certain question types on our dataset. Furthermore,
VDEval, which integrates dialogue session history and video content summaries
extracted from our supplementary Knowledge Graphs to evaluate individual
responses, demonstrates a significantly higher correlation with human
assessments on the VDAct dataset than existing evaluation metrics that rely
solely on the context of single dialogue turns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Citation Recommendation based on Argumentative Zoning of User Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutian Ma, Chengzhi Zhang, Heng Zhang, Zheng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation recommendation aims to locate the important papers for scholars to
cite. When writing the citing sentences, the authors usually hold different
citing intents, which are referred to citation function in citation analysis.
Since argumentative zoning is to identify the argumentative and rhetorical
structure in scientific literature, we want to use this information to improve
the citation recommendation task. In this paper, a multi-task learning model is
built for citation recommendation and argumentative zoning classification. We
also generated an annotated corpus of the data from PubMed Central based on a
new argumentative zoning schema. The experimental results show that, by
considering the argumentative information in the citing sentence, citation
recommendation model will get better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining for Species, Locations, Habitats, and Ecosystems from Scientific
  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer D'Souza, Zachary Laubach, Tarek Al Mustafa, Sina Zarrieß, Robert Frühstückl, Phyllis Illari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an exploratory study that harnesses the capabilities of
large language models (LLMs) to mine key ecological entities from invasion
biology literature. Specifically, we focus on extracting species names, their
locations, associated habitats, and ecosystems, information that is critical
for understanding species spread, predicting future invasions, and informing
conservation efforts. Traditional text mining approaches often struggle with
the complexity of ecological terminology and the subtle linguistic patterns
found in these texts. By applying general-purpose LLMs without domain-specific
fine-tuning, we uncover both the promise and limitations of using these models
for ecological entity extraction. In doing so, this study lays the groundwork
for more advanced, automated knowledge extraction tools that can aid
researchers and practitioners in understanding and managing biological
invasions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, accepted to the NLP4Ecology Workshop 2025
  (https://nlp4ecology2025.di.unito.it/) co-located with the Joint 25th Nordic
  Conference on Computational Linguistics and 11th Baltic Conference on Human
  Language Technologies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreaking LLMs' Safeguard with Universal Magic Words for Text
  Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The security issue of large language models (LLMs) has gained significant
attention recently, with various defense mechanisms developed to prevent
harmful outputs, among which safeguards based on text embedding models serve as
a fundamental defense. Through testing, we discover that the distribution of
text embedding model outputs is significantly biased with a large mean.
Inspired by this observation, we propose novel efficient methods to search for
universal magic words that can attack text embedding models. The universal
magic words as suffixes can move the embedding of any text towards the bias
direction, therefore manipulate the similarity of any text pair and mislead
safeguards. By appending magic words to user prompts and requiring LLMs to end
answers with magic words, attackers can jailbreak the safeguard. To eradicate
this security risk, we also propose defense mechanisms against such attacks,
which can correct the biased distribution of text embeddings in a train-free
manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collecting Cost-Effective, High-Quality Truthfulness Assessments with
  LLM Summarized Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the degradation of guardrails against mis- and disinformation online, it
is more critical than ever to be able to effectively combat it. In this paper,
we explore the efficiency and effectiveness of using crowd-sourced truthfulness
assessments based on condensed, large language model (LLM) generated summaries
of online sources. We compare the use of generated summaries to the use of
original web pages in an A/B testing setting, where we employ a large and
diverse pool of crowd-workers to perform the truthfulness assessment. We
evaluate the quality of assessments, the efficiency with which assessments are
performed, and the behavior and engagement of participants. Our results
demonstrate that the Summary modality, which relies on summarized evidence,
offers no significant change in assessment accuracy over the Standard modality,
while significantly increasing the speed with which assessments are performed.
Workers using summarized evidence produce a significantly higher number of
assessments in the same time frame, reducing the cost needed to acquire
truthfulness assessments. Additionally, the Summary modality maximizes both the
inter-annotator agreements as well as the reliance on and perceived usefulness
of evidence, demonstrating the utility of summarized evidence without
sacrificing the quality of assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; 7 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Select Datapoints for Efficient Human Evaluation of NLG Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Peng Cui, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation is the gold-standard for evaluating text generation models.
It is also expensive, and to fit budgetary constraints, a random subset of the
test data is often chosen in practice. The randomly selected data may not
accurately represent test performance, making this approach economically
inefficient for model comparison. Thus, in this work, we develop a suite of
selectors to get the most informative datapoints for human evaluation while
taking the evaluation costs into account. We show that selectors based on
variance in automated metric scores, diversity in model outputs, or Item
Response Theory outperform random selection. We further develop an approach to
distill these selectors to the scenario where the model outputs are not yet
available. In particular, we introduce source-based estimators, which predict
item usefulness for human evaluation just based on the source texts. We
demonstrate the efficacy of our selectors in two common NLG tasks, machine
translation and summarization, and show that up to only ~50% of the test data
is needed to produce the same evaluation result as the entire data. Our
implementations are published in the subset2evaluate package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical multi-metric evaluation and visualization of LLM system
  predictive performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of generative or discriminative large language model
(LLM)-based systems is often a complex multi-dimensional problem. Typically, a
set of system configuration alternatives are evaluated on one or more benchmark
datasets, each with one or more evaluation metrics, which may differ between
datasets. We often want to evaluate -- with a statistical measure of
significance -- whether systems perform differently either on a given dataset
according to a single metric, on aggregate across metrics on a dataset, or
across datasets. Such evaluations can be done to support decision-making, such
as deciding whether a particular system component change (e.g., choice of LLM
or hyperparameter values) significantly improves performance over the current
system configuration, or, more generally, whether a fixed set of system
configurations (e.g., a leaderboard list) have significantly different
performances according to metrics of interest. We present a framework
implementation that automatically performs the correct statistical tests,
properly aggregates the statistical results across metrics and datasets (a
nontrivial task), and can visualize the results. The framework is demonstrated
on the multi-lingual code generation benchmark CrossCodeEval, for several
state-of-the-art LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually Structured Token Dependency Encoding for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Blades, Frederick Somerfield, William Langley, Susan Everingham, Maurice Witherington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token representation strategies within large-scale neural architectures often
rely on contextually refined embeddings, yet conventional approaches seldom
encode structured relationships explicitly within token interactions.
Self-attention mechanisms effectively capture dynamic contextual dependencies,
but their reliance on learned weight distributions limits the preservation of
long-range hierarchical structures in generated sequences. Dependency-aware
token encoding introduces a structured approach to embedding initialization,
ensuring that relational constraints are embedded within token representations
rather than inferred solely through attention dynamics. The proposed encoding
mechanism refines token interactions through dependency-weighted attention
computations, ensuring that syntactic and semantic dependencies are retained
across multiple processing layers. Empirical evaluations indicate reductions in
perplexity across diverse linguistic benchmarks, suggesting improvements in
contextual coherence and predictive consistency in autoregressive text
generation. Computational efficiency assessments reveal a moderate increase in
memory consumption and training time, attributed to additional matrix
computations within the encoding module, yet scalability remains feasible
within conventional transformer architectures. Structured encoding enhances
lexical variation and dependency retention, reinforcing linguistic coherence
without requiring external syntactic annotations or auxiliary training
objectives. Statistical comparisons highlight improvements in dependency
alignment, particularly in longer sequences where conventional self-attention
models exhibit degradation in hierarchical consistency. Sentence length
distributions indicate a reduction in abrupt phrase transitions, further
supporting the hypothesis that explicit dependency encoding facilitates more
structured phrase generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Precision Graph Neural Quantization for Low Bit Large Language
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanlong Liu, Yichen Xiao, Dingyi Zeng, Hongyang Zhao, Wenyu Chen, Malu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-Training Quantization (PTQ) is pivotal for deploying large language
models (LLMs) within resource-limited settings by significantly reducing
resource demands. However, existing PTQ strategies underperform at low bit
levels < 3 bits due to the significant difference between the quantized and
original weights. To enhance the quantization performance at low bit widths, we
introduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a
graph neural network (GNN) module to capture dependencies among weights and
adaptively assign quantization bit-widths. Through the information propagation
of the GNN module, our method more effectively captures dependencies among
target weights, leading to a more accurate assessment of weight importance and
optimized allocation of quantization strategies. Extensive experiments on the
WikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms
previous state-of-the-art PTQ method GPTQ, setting new benchmarks for
quantization performance under low-bit conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Capabilities of Language Models in News Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdurrahman Odabaşı, Göksel Biricik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the recent introduction of multiple language models and the ongoing
demand for improved Natural Language Processing tasks, particularly
summarization, this work provides a comprehensive benchmarking of 20 recent
language models, focusing on smaller ones for the news summarization task. In
this work, we systematically test the capabilities and effectiveness of these
models in summarizing news article texts which are written in different styles
and presented in three distinct datasets. Specifically, we focus in this study
on zero-shot and few-shot learning settings and we apply a robust evaluation
methodology that combines different evaluation concepts including automatic
metrics, human evaluation, and LLM-as-a-judge. Interestingly, including
demonstration examples in the few-shot learning setting did not enhance models'
performance and, in some cases, even led to worse quality of the generated
summaries. This issue arises mainly due to the poor quality of the gold
summaries that have been used as reference summaries, which negatively impacts
the models' performance. Furthermore, our study's results highlight the
exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate
due to their advanced capabilities. However, among the public models evaluated,
certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B
and Zephyr-7B-Beta demonstrated promising results. These models showed
significant potential, positioning them as competitive alternatives to large
models for the task of news summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Birdie: Advancing State Space Models with Reward-Driven Objectives and
  Curricula <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01030v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01030v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Complexity-Based Theory of Compositionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Elmoznino, Thomas Jiralerspong, <span class="highlight-author">Yoshua Bengio</span>, Guillaume Lajoie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositionality is believed to be fundamental to intelligence. In humans, it
underlies the structure of thought, language, and higher-level reasoning. In
AI, compositional representations can enable a powerful form of
out-of-distribution generalization, in which a model systematically adapts to
novel combinations of known concepts. However, while we have strong intuitions
about what compositionality is, there currently exists no formal definition for
it that is measurable and mathematical. Here, we propose such a definition,
which we call representational compositionality, that accounts for and extends
our intuitions about compositionality. The definition is conceptually simple,
quantitative, grounded in algorithmic information theory, and applicable to any
representation. Intuitively, representational compositionality states that a
compositional representation satisfies three properties. First, it must be
expressive. Second, it must be possible to re-describe the representation as a
function of discrete symbolic sequences with re-combinable parts, analogous to
sentences in natural language. Third, the function that relates these symbolic
sequences to the representation, analogous to semantics in natural language,
must be simple. Through experiments on both synthetic and real world data, we
validate our definition of compositionality and show how it unifies disparate
intuitions from across the literature in both AI and cognitive science. We also
show that representational compositionality, while theoretically intractable,
can be readily estimated using standard deep learning tools. Our definition has
the potential to inspire the design of novel, theoretically-driven models that
better capture the mechanisms of compositional thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Isolate<span class="highlight-title">GPT</span>: An Execution Isolation Architecture for LLM-Based Agentic
  Systems <span class="chip">NDSS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) extended as systems, such as ChatGPT, have begun
supporting third-party applications. These LLM apps leverage the de facto
natural language-based automated execution paradigm of LLMs: that is, apps and
their interactions are defined in natural language, provided access to user
data, and allowed to freely interact with each other and the system. These LLM
app ecosystems resemble the settings of earlier computing platforms, where
there was insufficient isolation between apps and the system. Because
third-party apps may not be trustworthy, and exacerbated by the imprecision of
natural language interfaces, the current designs pose security and privacy
risks for users. In this paper, we evaluate whether these issues can be
addressed through execution isolation and what that isolation might look like
in the context of LLM-based systems, where there are arbitrary natural
language-based interactions between system components, between LLM and apps,
and between apps. To that end, we propose IsolateGPT, a design architecture
that demonstrates the feasibility of execution isolation and provides a
blueprint for implementing isolation, in LLM-based systems. We evaluate
IsolateGPT against a number of attacks and demonstrate that it protects against
many security, privacy, and safety issues that exist in non-isolated LLM-based
systems, without any loss of functionality. The performance overhead incurred
by IsolateGPT to improve security is under 30% for three-quarters of tested
queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Network and Distributed System Security (NDSS)
  Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniPred: Language Models as Universal Regressors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14547v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14547v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel, Yutian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regression is a powerful tool to accurately predict the outcome metric of a
system given a set of parameters, but has traditionally been restricted to
methods which are only applicable to a specific task. In this paper, we propose
OmniPred, a framework for training language models as universal end-to-end
regressors over $(x,y)$ data from arbitrary formats. Using data sourced from
Google Vizier, one of the largest proprietary blackbox optimization databases
in the world, our extensive experiments demonstrate that language models are
capable of very precise numerical regression using only textual representations
of mathematical parameters and values, and if given the opportunity to train at
scale over multiple tasks, can significantly outperform traditional regression
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR) 2024.
  Code can be found in
  https://github.com/google-research/optformer/tree/main/optformer/omnipred</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Priest to Doctor: Domain Adaptaion for <span class="highlight-title">Low-Resource</span> Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Marashian, Enora Rice, Luke Gessler, Alexis Palmer, Katharina von der Wense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many of the world's languages have insufficient data to train high-performing
general neural machine translation (NMT) models, let alone domain-specific
models, and often the only available parallel data are small amounts of
religious texts. Hence, domain adaptation (DA) is a crucial issue faced by
contemporary NMT and has, so far, been underexplored for low-resource
languages. In this paper, we evaluate a set of methods from both low-resource
NMT and DA in a realistic setting, in which we aim to translate between a
high-resource and a low-resource language with access to only: a) parallel
Bible data, b) a bilingual dictionary, and c) a monolingual target-domain
corpus in the high-resource language. Our results show that the effectiveness
of the tested methods varies, with the simplest one, DALI, being most
effective. We follow up with a small human evaluation of DALI, which shows that
there is still a need for more careful investigation of how to accomplish DA
for low-resource NMT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair
  Language Modeling and Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Limisiewicz, David Mareček, Tomáš Musil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of biases, such as language models' reliance on gender
stereotypes, is a crucial endeavor required for the creation of reliable and
useful language technology. The crucial aspect of debiasing is to ensure that
the models preserve their versatile capabilities, including their ability to
solve language tasks and equitably represent various genders. To address this
issue, we introduce a streamlined Dual Dabiasing Algorithm through Model
Adaptation (2DAMA). Novel Dual Debiasing enables robust reduction of
stereotypical bias while preserving desired factual gender information encoded
by language models. We show that 2DAMA effectively reduces gender bias in
English and is one of the first approaches facilitating the mitigation of
stereotypical tendencies in translation. The proposed method's key advantage is
the preservation of factual gender cues, which are useful in a wide range of
natural language processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Browsing: API-Based Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web browsers are a portal to the internet, where much of human activity is
undertaken. Thus, there has been significant research work in AI agents that
interact with the internet through web browsing. However, there is also another
interface designed specifically for machine interaction with online content:
application programming interfaces (APIs). In this paper we ask -- what if we
were to take tasks traditionally tackled by browsing agents, and give AI agents
access to APIs? To do so, we propose two varieties of agents: (1) an
API-calling agent that attempts to perform online tasks through APIs only,
similar to traditional coding agents, and (2) a Hybrid Agent that can interact
with online data through both web browsing and APIs. In experiments on
WebArena, a widely-used and realistic benchmark for web navigation tasks, we
find that API-based agents outperform web browsing agents. Hybrid Agents
out-perform both others nearly uniformly across tasks, resulting in a more than
20.0% absolute improvement over web browsing alone, achieving a success rate of
35.8%, achiving the SOTA performance among task-agnostic agents. These results
strongly suggest that when APIs are available, they present an attractive
alternative to relying on web browsing alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention enhances parameter flexibility. For example,
unlike traditional softmax attention heads that use a static output-value (OV)
matrix to delete or copy inputs that the heads attend to, Cog Attention
naturally learns to use the sign of dynamic query-key (QK) inner products to
represent these operations. This enables Cog Attention to perform multiple
operations simultaneously within a single head. Meanwhile, Cog Attention's OV
matrix can focus more on refinement or modification. (2) Cog Attention enhances
the model's robustness against representational collapse by preventing the
``over-squashing'' of earlier tokens into later positions. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models at various scales for language modeling and U-ViT diffusion
models for image generation. Experiments show that models using Cog Attention
exhibit superior performance compared to those employing traditional softmax
attention modules. Our approach suggests a promising research direction for
rethinking and breaking the entrenched constraints of traditional softmax
attention, such as the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verify with Caution: The Pitfalls of Relying on Imperfect Factuality
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameya Godbole, Robin Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improvements in large language models have led to increasing optimism that
they can serve as reliable evaluators of natural language generation outputs.
In this paper, we challenge this optimism by thoroughly re-evaluating five
state-of-the-art factuality metrics on a collection of 11 datasets for
summarization, retrieval-augmented generation, and question answering. We find
that these evaluators are inconsistent with each other and often misestimate
system-level performance, both of which can lead to a variety of pitfalls. We
further show that these metrics exhibit biases against highly paraphrased
outputs and outputs that draw upon faraway parts of the source documents. We
urge users of these factuality metrics to proceed with caution and manually
validate the reliability of these metrics in their domain of interest before
proceeding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Added Acknowledgements to funding sources and advisors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Meta LoRA <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task
specific fine-tuning. However, in scenarios that involve multiple tasks,
training a separate LoRA model for each one results in considerable
inefficiency in terms of storage and inference. Moreover, existing parameter
generation methods fail to capture the correlations among these tasks, making
multi-task LoRA parameter generation challenging. To address these limitations,
we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently
achieves task-specific customization of large language models (LLMs).
Specifically, we use training data from all tasks to train a tailored
generator, Conditional Variational Autoencoder (CVAE). CVAE takes task
descriptions as inputs and produces task-aware LoRA weights as outputs. These
LoRA weights are then merged with LLMs to create task-specialized models
without the need for additional fine-tuning. Furthermore, we utilize in-context
meta-learning for knowledge enhancement and task mapping, to capture the
relationship between tasks and parameter distributions. As a result, our method
achieves more accurate LoRA parameter generation for diverse tasks using CVAE.
ICM-LoRA enables more accurate LoRA parameter reconstruction than current
parameter reconstruction methods and is useful for implementing task-specific
enhancements of LoRA parameters. At the same time, our method occupies 283MB,
only 1\% storage compared with the original LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critique Fine-Tuning: Learning to Critique is More Effective than
  Learning to Imitate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised Fine-Tuning (SFT) is commonly used to train language models to
imitate annotated responses for given instructions. In this paper, we challenge
this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models
learn to critique noisy responses rather than simply imitate correct ones.
Inspired by human learning processes that emphasize critical thinking, CFT
encourages deeper analysis and nuanced understanding-traits often overlooked by
standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample
dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in
the form of ([query; noisy response], critique). CFT on this dataset yields a
consistent 4-10% improvement over SFT on six math benchmarks with different
base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to
MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably,
our model Qwen2.5-Math-CFT only requires 1 hour training on 8xH100 over the 50K
examples. It can match or outperform strong competitors like
Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover,
it can match the performance of SimpleRL, which is a deepseek-r1 replication
trained with 140x more compute. Ablation studies show that CFT is robust to the
source of noisy response and teacher critique model. Through these findings, we
argue that CFT offers a more effective alternative to advance the reasoning of
language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Preference Optimization for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sacha Muller, António Loison, Bilel Omrani, Gautier Viaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use
Large Language Models (LLMs) alongside private and up-to-date knowledge bases.
In this work, we address the challenges of using LLM-as-a-Judge when evaluating
grounded answers generated by RAG systems. To assess the calibration and
discrimination capabilities of judge models, we identify 7 generator failure
modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a
meta-evaluation benchmark of 144 unit tests. This benchmark reveals that
existing automated RAG evaluation frameworks often overlook important failure
modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we
propose a novel pipeline and find that while closed models perform well on
GroUSE, state-of-the-art open-source judges do not generalize to our proposed
criteria, despite strong correlation with GPT-4's judgement. Our findings
suggest that correlation with GPT-4 is an incomplete proxy for the practical
performance of judge models and should be supplemented with evaluations on unit
tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces
significantly boosts its evaluation capabilities, improving upon both
correlation with GPT-4's evaluations and calibration on reference situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 31st International Conference on Computational
  Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently been leveraged to generate
robotic actions, forming Vision-Language-Action (VLA) models. However, directly
adapting a pretrained VLM for robotic control remains challenging, particularly
when constrained by a limited number of robot demonstrations. In this work, we
introduce LLaRA: Large Language and Robotics Assistant, a framework that
formulates robot action policy as visuo-textual conversations and enables an
efficient transfer of a pretrained VLM into a powerful VLA, motivated by the
success of visual instruction tuning in Computer Vision. First, we present an
automated pipeline to generate conversation-style instruction tuning data for
robots from existing behavior cloning datasets, aligning robotic actions with
image pixel coordinates. Further, we enhance this dataset in a self-supervised
manner by defining six auxiliary tasks, without requiring any additional action
annotations. We show that a VLM finetuned with a limited amount of such
datasets can produce meaningful action decisions for robotic control. Through
experiments across multiple simulated and real-world tasks, we demonstrate that
LLaRA achieves state-of-the-art performance while preserving the generalization
capabilities of large language models. The code, datasets, and pretrained
models are available at https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Return of the Encoder: Maximizing Parameter Efficiency for SLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elfeki, Rui Liu, Chad Voegele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominance of large decoder-only language models has overshadowed
encoder-decoder architectures, despite their fundamental efficiency advantages
in sequence processing. For small language models (SLMs) - those with 1 billion
parameters or fewer - our systematic analysis across GPU, CPU, and NPU
platforms reveals that encoder-decoder architectures achieve 47% lower
first-token latency and 4.7x higher throughput compared to decoder-only models
on edge devices. These gains may be attributed to encoder-decoder's one-time
input processing and efficient separation of understanding and generation
phases.
  We introduce a novel knowledge distillation framework that enables
encoder-decoder models to leverage capabilities from large scalable
decoder-only teachers while preserving their architectural advantages,
achieving up to 6 average performance points improvement across diverse tasks,
with significant gains in asymmetric sequence tasks where input and output
distributions can benefit from different processing approaches.
  When combined with modern advances like Rotary Positional Embeddings (RoPE)
and Vision encoders, our systematic investigation demonstrates that
encoder-decoder architectures provide a more practical path toward deploying
capable language models in resource-constrained environments. Our findings
challenge the prevailing trend toward decoder-only scaling, showing that
architectural choices become increasingly crucial as parameter budgets
decrease, particularly for on-device and edge deployments where computational
efficiency is paramount.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-AutoDiff: Auto-Differentiate Any LLM Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yin, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have reshaped natural language processing,
powering applications from multi-hop retrieval and question answering to
autonomous agent workflows. Yet, prompt engineering -- the task of crafting
textual inputs to effectively direct LLMs -- remains difficult and
labor-intensive, particularly for complex pipelines that combine multiple LLM
calls with functional operations like retrieval and data formatting. We
introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering
(APE) that extends textual gradient-based methods (such as Text-Grad) to
multi-component, potentially cyclic LLM architectures. Implemented within the
AdalFlow library, LLM-AutoDiff treats each textual input as a trainable
parameter and uses a frozen backward engine LLM to generate feedback-akin to
textual gradients -- that guide iterative prompt updates. Unlike prior
single-node approaches, LLM-AutoDiff inherently accommodates functional nodes,
preserves time-sequential behavior in repeated calls (e.g., multi-hop loops),
and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts
(instructions, formats, or few-shot examples). It further boosts training
efficiency by focusing on error-prone samples through selective gradient
computation. Across diverse tasks, including single-step classification,
multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff
consistently outperforms existing textual gradient baselines in both accuracy
and training cost. By unifying prompt optimization through a graph-centric
lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating
LLM workflows - mirroring the transformative role that automatic
differentiation libraries have long played in neural network research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much Can We Forget about Data Contamination? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we challenge the common assumption that small-scale
contamination renders benchmark evaluations invalid. First, we experimentally
quantify the magnitude of benchmark overfitting based on scaling along three
dimensions: The number of model parameters (up to 1.6B), the number of times an
example is seen (up to 144), and the number of training tokens (up to 40B). If
model and data follow the Chinchilla scaling laws, minor contamination indeed
leads to overfitting. At the same time, even 144 times of contamination can be
forgotten if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. Continual pre-training of OLMo-7B
corroborates these results. Next, we study the impact of the weight decay
parameter on example forgetting, showing that empirical forgetting occurs
faster than the cumulative weight decay. This allows us to gauge the degree of
example forgetting in large-scale training runs, indicating that many LLMs,
including Lllama 3 405B, have forgotten the data seen at the beginning of
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xJailbreak: Representation Space Guided Reinforcement Learning for
  Interpretable LLM Jailbreaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunbowen Lee, Shiwen Ni, Chi Wei, Shuaimin Li, Liyang Fan, Ahmadreza Argha, Hamid Alinejad-Rokny, Ruifeng Xu, Yicheng Gong, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety alignment mechanism are essential for preventing large language models
(LLMs) from generating harmful information or unethical content. However,
cleverly crafted prompts can bypass these safety measures without accessing the
model's internal parameters, a phenomenon known as black-box jailbreak.
Existing heuristic black-box attack methods, such as genetic algorithms, suffer
from limited effectiveness due to their inherent randomness, while recent
reinforcement learning (RL) based methods often lack robust and informative
reward signals. To address these challenges, we propose a novel black-box
jailbreak method leveraging RL, which optimizes prompt generation by analyzing
the embedding proximity between benign and malicious prompts. This approach
ensures that the rewritten prompts closely align with the intent of the
original prompts while enhancing the attack's effectiveness. Furthermore, we
introduce a comprehensive jailbreak evaluation framework incorporating
keywords, intent matching, and answer validation to provide a more rigorous and
holistic assessment of jailbreak success. Experimental results show the
superiority of our approach, achieving state-of-the-art (SOTA) performance on
several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,
Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in
jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.
The codebase for this work is available at
https://github.com/Aegis1863/xJailbreak.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Reflect the Ideology of their Creators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Raphaël Romero, Jefrey Lijffijt, Tijl De Bie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on vast amounts of data to generate
natural language, enabling them to perform tasks like text summarization and
question answering. These models have become popular in artificial intelligence
(AI) assistants like ChatGPT and already play an influential role in how humans
access information. However, the behavior of LLMs varies depending on their
design, training, and use.
  In this paper, we prompt a diverse panel of popular LLMs to describe a large
number of prominent personalities with political relevance, in all six official
languages of the United Nations. By identifying and analyzing moral assessments
reflected in their responses, we find normative differences between LLMs from
different geopolitical regions, as well as between the responses of the same
LLM when prompted in different languages. Among only models in the United
States, we find that popularly hypothesized disparities in political views are
reflected in significant normative differences related to progressive values.
Among Chinese models, we characterize a division between internationally- and
domestically-focused models.
  Our results show that the ideological stance of an LLM appears to reflect the
worldview of its creators. This poses the risk of political instrumentalization
and raises concerns around technological and regulatory efforts with the stated
aim of making LLMs ideologically 'unbiased'.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network pruning focuses on computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has been pruning and re-training, which nowadays
is inconvenient due to the vast amount of pre-trained models, which are in any
case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAL}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs,
which modifies the block-wise and row-wise sparsity exploiting information from
both the dense model and its sparse version to maximize the \emph{neuron
alignment} among activations. Differently from existing methods, our approach
adaptively selects the best hyperparameters for the block-wise and row-wise
sparsity ratios w.r.t. the model and the desired sparsity, and requires
\emph{no re-training}. We test our method over 276 cases combining four LLM
families, three sparsity ratios, and ten language tasks (three language
modeling and seven zero-shot datasets), showing how it consistently outperforms
the latest state-of-the-art methods in terms of performance-runtime trade-off.
The code is available at
\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamArtist++: <span class="highlight-title">Controllable</span> One-Shot Text-to-Image <span class="highlight-title">Generation</span> via
  Positive-Negative Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Dong, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-arts text-to-image generation models such as Imagen and Stable
Diffusion Model have succeed remarkable progresses in synthesizing
high-quality, feature-rich images with high resolution guided by human text
prompts. Since certain characteristics of image content \emph{e.g.}, very
specific object entities or styles, are very hard to be accurately described by
text, some example-based image generation approaches have been proposed,
\emph{i.e.} generating new concepts based on absorbing the salient features of
a few input references. Despite of acknowledged successes, these methods have
struggled on accurately capturing the reference examples' characteristics while
keeping diverse and high-quality image generation, particularly in the one-shot
scenario (\emph{i.e.} given only one reference). To tackle this problem, we
propose a simple yet effective framework, namely DreamArtist, which adopts a
novel positive-negative prompt-tuning learning strategy on the pre-trained
diffusion model, and it has shown to well handle the trade-off between the
accurate controllability and fidelity of image generation with only one
reference example. Specifically, our proposed framework incorporates both
positive and negative embeddings or adapters and optimizes them in a joint
manner. The positive part aggressively captures the salient characteristics of
the reference image to drive diversified generation and the negative part
rectifies inadequacies from the positive part. We have conducted extensive
experiments and evaluated the proposed method from image similarity (fidelity)
and diversity, generation controllability, and style cloning. And our
DreamArtist has achieved a superior generation performance over existing
methods. Besides, our additional evaluation on extended tasks, including
concept compositions and prompt-guided image editing, demonstrates its
effectiveness for more applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACEBench: Who Wins the Match Point in Tool Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, Wu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant potential in
decision-making and reasoning, especially when combined with various tools to
effectively solve complex problems. However, existing evaluation systems for
assessing LLM function calling capabilities have several limitations: (1)
limited evaluation scenarios, lacking assessments in real multi-turn dialogue
contexts; (2) narrow evaluation dimensions, lacking detailed assessments for
fine-grained function calls; (3) relying on LLMs or real API executions for
result evaluation, which introduces significant overhead. To address these
issues, we propose a comprehensive evaluation system named ACEBench. This
system is meticulously designed to encompass a wide spectrum of function
calling scenarios. Moreover, it categorizes these scenarios into three primary
types according to the evaluation methodology: Normal, Special, and Agent.
Normal evaluates function calls in basic scenarios; Special evaluates function
calls in scenarios with vague or incomplete instructions; Agent introduces
multi-agent interactions to simulate function calling evaluation in real-world
multi-turn interactions. We conducted extensive experiments on ACEBench,
analyzing various LLMs in-depth and performing a more granular analysis of
error causes across different data types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locret: Enhancing Eviction in Long-Context LLM Inference with Trained
  Retaining Heads on Consumer-Grade Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the input context length of a large language model (LLM) incurs a
significant increase in computation cost and memory footprint to maintain the
attention key-value (KV) cache. Existing KV cache compression methods suffer
from inefficient compression strategies and limited memory reduction effects,
making it difficult for LLMs to conduct long-context inference on
consumer-grade devices, especially when inferring long-context stream input.
Such obstacles prevent consumer-grade devices from supporting more complex
applications, creating challenges for the democratization of LLMs. To overcome
this, we propose Locret, the first framework to create an eviction policy
compatible with chunked prefill. By evaluating the causal importance of KV
cache units by learnable retaining heads, Locret enables precise eviction of
cache units, facilitating efficient long-context inference. In our extensive
empirical studies, Locret outperforms the recent popular and competitive
approaches in terms of memory efficiency and generation quality -- Locret
achieves up to 20x of KV cache compression ratio within less than 10%
performance loss. Furthermore, Locret achieves 128K+ long-context inference on
a single NVIDIA 4090 GPU without compromising generation quality and only costs
<1 GPU hour of additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprints</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property
  for Perplexity in Generative Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Mudireddy, Tyler Bell, Raghu Mudumbai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove a new asymptotic equipartition property for the perplexity of long
texts generated by a language model and present supporting experimental
evidence from open-source models. Specifically we show that the logarithmic
perplexity of any large text generated by a language model must asymptotically
converge to the average entropy of its token distributions. This defines a
"typical set" that all long synthetic texts generated by a language model must
belong to. We show that this typical set is a vanishingly small subset of all
possible grammatically correct outputs. These results suggest possible
applications to important practical problems such as (a) detecting synthetic
AI-generated text, and (b) testing whether a text was used to train a language
model. We make no simplifying assumptions (such as stationarity) about the
statistics of language model outputs, and therefore our results are directly
applicable to practical real-world models without any approximations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Idiom Detection in Sorani Kurdish Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skala Kamaran Omer, Hossein Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Idiom detection using Natural Language Processing (NLP) is the computerized
process of recognizing figurative expressions within a text that convey
meanings beyond the literal interpretation of the words. While idiom detection
has seen significant progress across various languages, the Kurdish language
faces a considerable research gap in this area despite the importance of idioms
in tasks like machine translation and sentiment analysis. This study addresses
idiom detection in Sorani Kurdish by approaching it as a text classification
task using deep learning techniques. To tackle this, we developed a dataset
containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse
contexts. Using this dataset, we developed and evaluated three deep learning
models: KuBERT-based transformer sequence classification, a Recurrent
Convolutional Neural Network (RCNN), and a BiLSTM model with an attention
mechanism. The evaluations revealed that the transformer model, the fine-tuned
BERT, consistently outperformed the others, achieving nearly 99% accuracy while
the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the
effectiveness of Transformer-based architectures in low-resource languages like
Kurdish. This research provides a dataset, three optimized models, and insights
into idiom detection, laying a foundation for advancing Kurdish NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with
  Customisable Fairness Calibration <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11149v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11149v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guan, Ze Wang, Nathaniel Demchak, Saloni Gupta, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(bias) is the first
holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that metric tool
bias and contextual bias in prompts can distort evaluation, SAGED implements
counterfactual branching and baseline calibration for mitigation. For
demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. presidents, we see bias amplifies and shifts in
heterogeneous directions. Moreover, we see Qwen2 and Mistral not engage in
role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Role of <span class="highlight-title">Reasoning</span> Structures for Constructing Proofs in
  Multi-Step Natural Language <span class="highlight-title">Reasoning</span> with Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When performing complex multi-step reasoning tasks, the ability of Large
Language Models (LLMs) to derive structured intermediate proof steps is
important for ensuring that the models truly perform the desired reasoning and
for improving models' explainability. This paper is centred around a focused
study: whether the current state-of-the-art generalist LLMs can leverage the
structures in a few examples to better construct the proof structures with
\textit{in-context learning}. Our study specifically focuses on structure-aware
demonstration and structure-aware pruning. We demonstrate that they both help
improve performance. A detailed analysis is provided to help understand the
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception in LLMs: Self-Preservation and Autonomous Goals in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have incorporated planning
and reasoning capabilities, enabling models to outline steps before execution
and provide transparent reasoning paths. This enhancement has reduced errors in
mathematical and logical tasks while improving accuracy. These developments
have facilitated LLMs' use as agents that can interact with tools and adapt
their responses based on new information.
  Our study examines DeepSeek R1, a model trained to output reasoning tokens
similar to OpenAI's o1. Testing revealed concerning behaviors: the model
exhibited deceptive tendencies and demonstrated self-preservation instincts,
including attempts of self-replication, despite these traits not being
explicitly programmed (or prompted). These findings raise concerns about LLMs
potentially masking their true objectives behind a facade of alignment. When
integrating such LLMs into robotic systems, the risks become tangible - a
physically embodied AI exhibiting deceptive behaviors and self-preservation
instincts could pursue its hidden objectives through real-world actions. This
highlights the critical need for robust goal specification and safety
frameworks before any physical implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected Version - Solved Some Issues with reference compilation by
  latex</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small
  Language Models for Biomedical Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Jian Wan, Siliang Tang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When addressing professional questions in the biomedical domain, humans
typically acquire multiple pieces of information as evidence and engage in
multifaceted evidence analysis to provide high-quality answers. Current
LLM-based answer generation methods lack a detailed definition and learning
process for evidence analysis, leading to the risk of error propagation and
hallucinations while using evidence. Although increasing the parameter size of
LLMs can alleviate these issues, it also presents challenges in model training
and deployment with limited resources. In this study, we propose EvidenceMap,
which aims to enable a tiny pre-trained language model to explicitly learn
multiple aspects of biomedical evidence, including supportive evaluation,
logical correlation and content summarization, thereby latently guiding a small
generative model (around 3B parameters) to provide textual responses.
Experimental results demonstrate that our method, fine-tuning a language model
with 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in
reference-based quality and accuracy, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LMFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LMFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LMFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LMFusion improves image understanding by 20% and image generation by 3.6% using
only 50% of the FLOPs while maintaining Llama-3's language capabilities. We
also demonstrate that this framework can adapt existing vision-language models
with multimodal generation ability. Overall, this framework not only leverages
existing computational investments in text-only LLMs but also enables the
parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Name change: LlamaFusion to LMFusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LemmaHead: RAG Assisted Proof <span class="highlight-title">Generation</span> Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing the logic necessary to solve mathematical problems or write
mathematical proofs is one of the more difficult objectives for large language
models (LLMS). Currently, the most popular methods in literature consists of
fine-tuning the model on written mathematical content such as academic
publications and textbooks, so that the model can learn to emulate the style of
mathematical writing. In this project, we explore the effectiveness of using
retrieval augmented generation (RAG) to address gaps in the mathematical
reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements
queries to the model with relevant mathematical context, with particular focus
on context from published textbooks. To measure our model's performance in
mathematical reasoning, our testing paradigm focuses on the task of automated
theorem proving via generating proofs to a given mathematical claim in the Lean
formal language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Privacy Benefits of Redaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Gusain, Douglas Leith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel redaction methodology that can be used to sanitize natural
text data. Our new technique provides better privacy benefits than other state
of the art techniques while maintaining lower redaction levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2025 Agent4IR Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distillation Quantification for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xinrun Du, Sirui He, Jiaheng Liu, Min Yang, Zhoufutu Wen, Shiwen Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model distillation is a technique for transferring knowledge from large
language models (LLMs) to smaller ones, aiming to create resource-efficient yet
high-performing models. However, excessive distillation can lead to
homogenization, reducing diversity among models and impairing their ability to
robustly handle complex or novel tasks. These limitations underscore the need
to systematically quantify the distillation process and its impact. In this
work, we propose a framework to evaluate and quantify model distillation. Our
method addresses two key aspects: (1) Identifying identity cognition
contradictions to assess discrepancies in how models perceive and represent
identity-related information, and (2) Analyzing multi-granularity response
similarities across models to measure the extent of homogenization.
Experimental results demonstrate two key insights: (1) Well-known closed-source
and open-source LLMs usually exhibit high distillation degrees, except for
Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees
compared to aligned LLMs. By offering a systematic approach to improve the
transparency of LLM data distillation, we call for LLMs with more independent
development and more transparent technical reports to improve LLMs' robustness
and safety. The code and data are available under
https://github.com/Aegis1863/LLMs-Distillation-Quantification.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Representation Disentanglement and Interpretability Linked in
  Recommendation Models? A Critical <span class="highlight-title">Review</span> and Reproducibility Study <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ervin Dervishaj, Tuukka Ruotsalo, Maria Maistro, Christina Lioma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning of disentangled representations has been closely tied
to enhancing the representation intepretability of Recommender Systems (RSs).
This has been achieved by making the representation of individual features more
distinctly separated, so that it is easier to attribute the contribution of
features to the model's predictions. However, such advantages in
interpretability and feature attribution have mainly been explored
qualitatively. Moreover, the effect of disentanglement on the model's
recommendation performance has been largely overlooked. In this work, we
reproduce the recommendation performance, representation disentanglement and
representation interpretability of five well-known recommendation models on
four RS datasets. We quantify disentanglement and investigate the link of
disentanglement with recommendation effectiveness and representation
interpretability. While several existing work in RSs have proposed disentangled
representations as a gateway to improved effectiveness and interpretability,
our findings show that disentanglement is not necessarily related to
effectiveness but is closely related to representation interpretability. Our
code and results are publicly available at
https://github.com/edervishaj/disentanglement-interpretability-recsys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 47th European Conference on Information Retrieval
  (ECIR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity
  Recognition in <span class="highlight-title">Low-Resource</span> Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Politov, Oleh Shkalikov, René Jäkel, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer
between languages to identify and classify named entities, making it
particularly useful for low-resource languages. We show that the data-based
cross-lingual transfer method is an effective technique for crosslingual NER
and can outperform multilingual language models for low-resource languages.
This paper introduces two key enhancements to the annotation projection step in
cross-lingual NER for low-resource languages. First, we explore refining word
alignments using back-translation to improve accuracy. Second, we present a
novel formalized projection approach of matching source entities with extracted
target candidates. Through extensive experiments on two datasets spanning 57
languages, we demonstrated that our approach surpasses existing projectionbased
methods in low-resource settings. These findings highlight the robustness of
projection-based data transfer as an alternative to model-based methods for
crosslingual named entity recognition in lowresource languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NoDaLiDa/Baltic-HLT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Multi-field Representations for Two-Stage E-commerce
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Freymuth, Dong Liu, Thomas Ricatte, Saab Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrieval methods typically target unstructured text data represented
as flat strings. However, e-commerce catalogs often include structured
information across multiple fields, such as brand, title, and description,
which contain important information potential for retrieval systems. We present
Cascading Hierarchical Attention Retrieval Model (CHARM), a novel framework
designed to encode structured product data into hierarchical field-level
representations with progressively finer detail. Utilizing a novel
block-triangular attention mechanism, our method captures the interdependencies
between product fields in a specified hierarchy, yielding field-level
representations and aggregated vectors suitable for fast and efficient
retrieval. Combining both representations enables a two-stage retrieval
pipeline, in which the aggregated vectors support initial candidate selection,
while more expressive field-level representations facilitate precise
fine-tuning for downstream ranking. Experiments on publicly available
large-scale e-commerce datasets demonstrate that CHARM matches or outperforms
state-of-the-art baselines. Our analysis highlights the framework's ability to
align different queries with appropriate product fields, enhancing retrieval
accuracy and explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented
  LLM-based Retrieval Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world open-domain questions can be complicated, particularly when
answering them involves information from multiple information sources. LLMs
have demonstrated impressive performance in decomposing complex tasks into
simpler steps, and previous work has used it for better retrieval in support of
complex questions. However, LLM's decomposition of questions is unaware of what
data is available and how data is organized, often leading to a sub-optimal
retrieval performance. Recent effort in agentic RAG proposes to perform
retrieval in an iterative fashion, where a followup query is derived as an
action based on previous rounds of retrieval. While this provides one way of
interacting with the data collection, agentic RAG's exploration of data is
inefficient because successive queries depend on previous results rather than
being guided by the organization of available data in the collection. To
address this problem, we propose an LLM-based retrieval method -- ARM, that
aims to better align the question with the organization of the data collection
by exploring relationships among data objects beyond matching the utterance of
the query, thus leading to a retrieve-all-at-once solution for complex queries.
We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms
standard RAG with query decomposition by up to 5.2 pt in execution accuracy and
agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and
19.3 pt higher F1 match scores compared to these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illusions of Relevance: Using Content Injection Attacks to Deceive
  Retrievers, Rerankers, and LLM Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manveer Singh Tamber, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a scenario in which a user searches for information, only to
encounter texts flooded with misleading or non-relevant content. This scenario
exemplifies a simple yet potent vulnerability in neural Information Retrieval
(IR) pipelines: content injection attacks. We find that embedding models for
retrieval, rerankers, and large language model (LLM) relevance judges are
vulnerable to these attacks, in which adversaries insert misleading text into
passages to manipulate model judgements. We identify two primary threats: (1)
inserting unrelated or harmful content within passages that still appear
deceptively "relevant", and (2) inserting entire queries or key query terms
into passages to boost their perceived relevance. While the second tactic has
been explored in prior research, we present, to our knowledge, the first
empirical analysis of the first threat, demonstrating how state-of-the-art
models can be easily misled. Our study systematically examines the factors that
influence an attack's success, such as the placement of injected content and
the balance between relevant and non-relevant material. Additionally, we
explore various defense strategies, including adversarial passage classifiers,
retriever fine-tuning to discount manipulated content, and prompting LLM judges
to adopt a more cautious approach. However, we find that these countermeasures
often involve trade-offs, sacrificing effectiveness for attack robustness and
sometimes penalizing legitimate documents in the process. Our findings
highlight the need for stronger defenses against these evolving adversarial
strategies to maintain the trustworthiness of IR systems. We release our code
and scripts to facilitate further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RbFT: Robust Fine-tuning for Retrieval-Augmented <span class="highlight-title">Generation</span> against
  Retrieval Defects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved from a knowledge base. However, its
effectiveness is fundamentally constrained by the reliability of both the
retriever and the knowledge base. In real-world scenarios, imperfections in
these components often lead to the retrieval of noisy, irrelevant, or
misleading counterfactual information, ultimately undermining the
trustworthiness of RAG systems. To address this challenge, we propose Robust
Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against
retrieval defects through two targeted fine-tuning tasks. Experimental results
demonstrate that RbFT significantly improves the robustness of RAG systems
across diverse retrieval conditions, surpassing existing methods while
maintaining high inference efficiency and compatibility with other robustness
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Citation Recommendation based on Argumentative Zoning of User Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutian Ma, Chengzhi Zhang, Heng Zhang, Zheng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation recommendation aims to locate the important papers for scholars to
cite. When writing the citing sentences, the authors usually hold different
citing intents, which are referred to citation function in citation analysis.
Since argumentative zoning is to identify the argumentative and rhetorical
structure in scientific literature, we want to use this information to improve
the citation recommendation task. In this paper, a multi-task learning model is
built for citation recommendation and argumentative zoning classification. We
also generated an annotated corpus of the data from PubMed Central based on a
new argumentative zoning schema. The experimental results show that, by
considering the argumentative information in the citing sentence, citation
recommendation model will get better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collecting Cost-Effective, High-Quality Truthfulness Assessments with
  LLM Summarized Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the degradation of guardrails against mis- and disinformation online, it
is more critical than ever to be able to effectively combat it. In this paper,
we explore the efficiency and effectiveness of using crowd-sourced truthfulness
assessments based on condensed, large language model (LLM) generated summaries
of online sources. We compare the use of generated summaries to the use of
original web pages in an A/B testing setting, where we employ a large and
diverse pool of crowd-workers to perform the truthfulness assessment. We
evaluate the quality of assessments, the efficiency with which assessments are
performed, and the behavior and engagement of participants. Our results
demonstrate that the Summary modality, which relies on summarized evidence,
offers no significant change in assessment accuracy over the Standard modality,
while significantly increasing the speed with which assessments are performed.
Workers using summarized evidence produce a significantly higher number of
assessments in the same time frame, reducing the cost needed to acquire
truthfulness assessments. Additionally, the Summary modality maximizes both the
inter-annotator agreements as well as the reliance on and perceived usefulness
of evidence, demonstrating the utility of summarized evidence without
sacrificing the quality of assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; 7 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavior Modeling Space Reconstruction for E-Commerce Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejing Wang, Chi Zhang, Xiangyu Zhao, Qidong Liu, Maolin Wang, Xuewei Tao, Zitao Liu, Xing Shi, Xudong Yang, Ling Zhong, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delivering superior search services is crucial for enhancing customer
experience and driving revenue growth. Conventionally, search systems model
user behaviors by combining user preference and query item relevance
statically, often through a fixed logical 'and' relationship. This paper
reexamines existing approaches through a unified lens using both causal graphs
and Venn diagrams, uncovering two prevalent yet significant issues: entangled
preference and relevance effects, and a collapsed modeling space. To surmount
these challenges, our research introduces a novel framework, DRP, which
enhances search accuracy through two components to reconstruct the behavior
modeling space. Specifically, we implement preference editing to proactively
remove the relevance effect from preference predictions, yielding untainted
user preferences. Additionally, we employ adaptive fusion, which dynamically
adjusts fusion criteria to align with the varying patterns of relevance and
preference, facilitating more nuanced and tailored behavior predictions within
the reconstructed modeling space. Empirical validation on two public datasets
and a proprietary search dataset underscores the superiority of our proposed
methodology, demonstrating marked improvements in performance over existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hashtag Re-Appropriation for Audience Control on Recommendation-Driven
  Social Media Xiaohongshu (rednote) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyuan Wan, Lingbo Tong, Tiffany Knearem, Toby Jia-Jun Li, Ting-Hao 'Kenneth' Huang, Qunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms have played a central role in personalized recommendations on
social media. However, they also present significant obstacles for content
creators trying to predict and manage their audience reach. This issue is
particularly challenging for marginalized groups seeking to maintain safe
spaces. Our study explores how women on Xiaohongshu (rednote), a
recommendation-driven social platform, proactively re-appropriate hashtags
(e.g., #Baby Supplemental Food) by using them in posts unrelated to their
literal meaning. The hashtags were strategically chosen from topics that would
be uninteresting to the male audience they wanted to block. Through a
mixed-methods approach, we analyzed the practice of hashtag re-appropriation
based on 5,800 collected posts and interviewed 24 active users from diverse
backgrounds to uncover users' motivations and reactions towards the
re-appropriation. This practice highlights how users can reclaim agency over
content distribution on recommendation-driven platforms, offering insights into
self-governance within algorithmic-centered power structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Tax Evasion Emergence Using Dual Large Language Model and
  Deep Reinforcement Learning Powered Agent-based Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Labib Shami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tax evasion, usually the largest component of an informal economy, is a
persistent challenge over history with significant socio-economic implications.
Many socio-economic studies investigate its dynamics, including influencing
factors, the role and influence of taxation policies, and the prediction of the
tax evasion volume over time. These studies assumed such behavior is given, as
observed in the real world, neglecting the "big bang" of such activity in a
population. To this end, computational economy studies adopted developments in
computer simulations, in general, and recent innovations in artificial
intelligence (AI), in particular, to simulate and study informal economy
appearance in various socio-economic settings. This study presents a novel
computational framework to examine the dynamics of tax evasion and the
emergence of informal economic activity. Employing an agent-based simulation
powered by Large Language Models and Deep Reinforcement Learning, the framework
is uniquely designed to allow informal economic behaviors to emerge
organically, without presupposing their existence or explicitly signaling
agents about the possibility of evasion. This provides a rigorous approach for
exploring the socio-economic determinants of compliance behavior. The
experimental design, comprising model validation and exploratory phases,
demonstrates the framework's robustness in replicating theoretical economic
behaviors. Findings indicate that individual personality traits, external
narratives, enforcement probabilities, and the perceived efficiency of public
goods provision significantly influence both the timing and extent of informal
economic activity. The results underscore that efficient public goods provision
and robust enforcement mechanisms are complementary; neither alone is
sufficient to curtail informal activity effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation
  with Hourly Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Cai, Ziwei Guan, Lei Yuan, Ali Selman Aydin, Tengyu Xu, Boying Liu, Wenbo Ren, Renkai Xiang, Songyi He, Haichuan Yang, Serena Li, Mingze Gao, Yue Weng, Ji Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommendation systems can be broadly divided into two key stages: the
ranking stage, where the system predicts various user engagements (e.g.,
click-through rate, like rate, follow rate, watch time), and the value model
stage, which aggregates these predictive scores through a function (e.g., a
linear combination defined by a weight vector) to measure the value of each
content by a single numerical score. Both stages play roughly equally important
roles in real industrial systems; however, how to optimize the model weights
for the second stage still lacks systematic study. This paper focuses on
optimizing the second stage through auto-tuning technology. Although general
auto-tuning systems and solutions - both from established production practices
and open-source solutions - can address this problem, they typically require
weeks or even months to identify a feasible solution. Such prolonged tuning
processes are unacceptable in production environments for recommendation
systems, as suboptimal value models can severely degrade user experience. An
effective auto-tuning solution is required to identify a viable model within
2-3 days, rather than the extended timelines typically associated with existing
approaches. In this paper, we introduce a practical auto-tuning system named
HyperZero that addresses these time constraints while effectively solving the
unique challenges inherent in modern recommendation systems. Moreover, this
framework has the potential to be expanded to broader tuning tasks within
recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Minimax Group Fairness in Sequential Recommendation <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Acharya, David Wardrope, Timos Korres, Aleksandr Petrov, Anders Uhrenholt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training sequential recommenders such as SASRec with uniform sample weights
achieves good overall performance but can fall short on specific user groups.
One such example is popularity bias, where mainstream users receive better
recommendations than niche content viewers. To improve recommendation quality
across diverse user groups, we explore three Distributionally Robust
Optimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at
Risk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and
struggle with users belonging to multiple groups, CVaR does not require such
annotations and can naturally handle overlapping groups. In experiments on two
real-world datasets, we show that the DRO methods outperform standard training,
with CVaR delivering the best results. Additionally, we find that Group and
Streaming DRO are sensitive to the choice of group used for loss computation.
Our contributions include (i) a novel application of CVaR to recommenders, (ii)
showing that the DRO methods improve group metrics as well as overall
performance, and (iii) demonstrating CVaR's effectiveness in the practical
scenario of intersecting user groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the IR for Good track at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LemmaHead: RAG Assisted Proof <span class="highlight-title">Generation</span> Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing the logic necessary to solve mathematical problems or write
mathematical proofs is one of the more difficult objectives for large language
models (LLMS). Currently, the most popular methods in literature consists of
fine-tuning the model on written mathematical content such as academic
publications and textbooks, so that the model can learn to emulate the style of
mathematical writing. In this project, we explore the effectiveness of using
retrieval augmented generation (RAG) to address gaps in the mathematical
reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements
queries to the model with relevant mathematical context, with particular focus
on context from published textbooks. To measure our model's performance in
mathematical reasoning, our testing paradigm focuses on the task of automated
theorem proving via generating proofs to a given mathematical claim in the Lean
formal language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Multimodal LLM for Inspirational User Interface Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspirational search, the process of exploring designs to inform and inspire
new creative work, is pivotal in mobile user interface (UI) design. However,
exploring the vast space of UI references remains a challenge. Existing
AI-based UI search methods often miss crucial semantics like target users or
the mood of apps. Additionally, these models typically require metadata like
view hierarchies, limiting their practical use. We used a multimodal large
language model (MLLM) to extract and interpret semantics from mobile UI images.
We identified key UI semantics through a formative study and developed a
semantic-based UI search system. Through computational and human evaluations,
we demonstrate that our approach significantly outperforms existing UI
retrieval methods, offering UI designers a more enriched and contextually
relevant search experience. We enhance the understanding of mobile UI design
semantics and highlight MLLMs' potential in inspirational search, providing a
rich dataset of UI semantics for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems (CHI '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengzhe Zhang, Liyi Chen, Dazhong Shen, Chao Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal sequential recommendation (SR) leverages multi-modal data to
learn more comprehensive item features and user preferences than traditional SR
methods, which has become a critical topic in both academia and industry.
Existing methods typically focus on enhancing multi-modal information utility
through adaptive modality fusion to capture the evolving of user preference
from user-item interaction sequences. However, most of them overlook the
interference caused by redundant interest-irrelevant information contained in
rich multi-modal data. Additionally, they primarily rely on implicit temporal
information based solely on chronological ordering, neglecting explicit
temporal signals that could more effectively represent dynamic user interest
over time. To address these limitations, we propose a Hierarchical time-aware
Mixture of experts for multi-modal Sequential Recommendation (HM4SR) with a
two-level Mixture of Experts (MoE) and a multi-task learning strategy.
Specifically, the first MoE, named Interactive MoE, extracts essential user
interest-related information from the multi-modal data of each item. Then, the
second MoE, termed Temporal MoE, captures user dynamic interests by introducing
explicit temporal embeddings from timestamps in modality encoding. To further
address data sparsity, we propose three auxiliary supervision tasks:
sequence-level category prediction (CP) for item feature understanding,
contrastive learning on ID (IDCL) to align sequence context with user
interests, and placeholder contrastive learning (PCL) to integrate temporal
information with modalities for dynamic interest modeling. Extensive
experiments on four public datasets verify the effectiveness of HM4SR compared
to several state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Information Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwah Alaofi, Negar Arabzadeh, Charles L. A. Clarke, Mark Sanderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this chapter, we consider generative information retrieval evaluation from
two distinct but interrelated perspectives. First, large language models (LLMs)
themselves are rapidly becoming tools for evaluation, with current research
indicating that LLMs may be superior to crowdsource workers and other paid
assessors on basic relevance judgement tasks. We review past and ongoing
related research, including speculation on the future of shared task
initiatives, such as TREC, and a discussion on the continuing need for human
assessments. Second, we consider the evaluation of emerging LLM-based
generative information retrieval (GenIR) systems, including retrieval augmented
generation (RAG) systems. We consider approaches that focus both on the
end-to-end evaluation of GenIR systems and on the evaluation of a retrieval
component as an element in a RAG system. Going forward, we expect the
evaluation of GenIR systems to be at least partially based on LLM-based
assessment, creating an apparent circularity, with a system seemingly
evaluating its own output. We resolve this apparent circularity in two ways: 1)
by viewing LLM-based assessment as a form of "slow search", where a slower IR
system is used for evaluation and training of a faster production IR system;
and 2) by recognizing a continuing need to ground evaluation in human
assessment, even if the characteristics of that human assessment must change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This chapter is part of the book Information Access in the Era of
  Generative AI, co-edited by Chirag Shah and Ryen White</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixRec: Individual and Collective Mixing Empowers Data Augmentation for
  Recommender Systems <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Yiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core of the general recommender systems lies in learning high-quality
embedding representations of users and items to investigate their positional
relations in the feature space. Unfortunately, data sparsity caused by
difficult-to-access interaction data severely limits the effectiveness of
recommender systems. Faced with such a dilemma, various types of
self-supervised learning methods have been introduced into recommender systems
in an attempt to alleviate the data sparsity through distribution modeling or
data augmentation. However, most data augmentation relies on elaborate manual
design, which is not only not universal, but the bloated and redundant
augmentation process may significantly slow down model training progress. To
tackle these limitations, we propose a novel Dual Mixing-based Recommendation
Framework (MixRec) to empower data augmentation as we wish. Specifically, we
propose individual mixing and collective mixing, respectively. The former aims
to provide a new positive sample that is unique to the target (user or item)
and to make the pair-wise recommendation loss benefit from it, while the latter
aims to portray a new sample that contains group properties in a batch. The two
mentioned mixing mechanisms allow for data augmentation with only one parameter
that does not need to be set multiple times and can be done in linear time
complexity. Besides, we propose the dual-mixing contrastive learning to
maximize the utilization of these new-constructed samples to enhance the
consistency between pairs of positive samples. Experimental results on four
real-world datasets demonstrate the advantages of MixRec in terms of
effectiveness, simplicity, efficiency, and scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inkspire: Supporting Design Exploration with Generative AI through
  Analogical Sketching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Hyeonsu B. Kang, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, Matthew K. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent advancements in the capabilities of Text-to-Image (T2I) AI
models, product designers have begun experimenting with them in their work.
However, T2I models struggle to interpret abstract language and the current
user experience of T2I tools can induce design fixation rather than a more
iterative, exploratory process. To address these challenges, we developed
Inkspire, a sketch-driven tool that supports designers in prototyping product
design concepts with analogical inspirations and a complete
sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we
conducted an exchange session with designers and distilled design goals for
improving T2I interactions. In a within-subjects study comparing Inkspire to
ControlNet, we found that Inkspire supported designers with more inspiration
and exploration of design ideas, and improved aspects of the co-creative
process by allowing designers to effectively grasp the current state of the AI
to guide it towards novel design intentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGAV-Rater: Adapting Large Multimodal Model for AI-<span class="highlight-title">Generate</span>d
  Audio-Visual Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqin Cao, Xiongkuo Min, Yixuan Gao, Wei Sun, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many video-to-audio (VTA) methods have been proposed for dubbing silent
AI-generated videos. An efficient quality assessment method for AI-generated
audio-visual content (AGAV) is crucial for ensuring audio-visual quality.
Existing audio-visual quality assessment methods struggle with unique
distortions in AGAVs, such as unrealistic and inconsistent elements. To address
this, we introduce AGAVQA, the first large-scale AGAV quality assessment
dataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two
subsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,
content consistency, and overall quality, and AGAVQA-Pair, designed for optimal
AGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can
score AGAVs, as well as audio and music generated from text, across multiple
dimensions, and selects the best AGAV generated by VTA methods to present to
the user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,
Text-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that
AGAV-Rater enhances VTA performance and user experience. The project page is
available at https://agav-rater.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training
  and Unimodal Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Sanjeel Parekh, Honglie Chen, Jacob Donley, Ke Tan, Buye Xu, Anurag Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building reliable speech systems often requires combining multiple
modalities, like audio and visual cues. While such multimodal solutions
frequently lead to improvements in performance and may even be critical in
certain cases, they come with several constraints such as increased sensory
requirements, computational cost, and modality synchronization, to mention a
few. These challenges constrain the direct uses of these multimodal solutions
in real-world applications. In this work, we develop approaches where the
learning happens with all available modalities but the deployment or inference
is done with just one or reduced modalities. To do so, we propose a Multimodal
Training and Unimodal Deployment (MUTUD) framework which includes a Temporally
Aligned Modality feature Estimation (TAME) module that can estimate information
from missing modality using modalities present during inference. This
innovative approach facilitates the integration of information across different
modalities, enhancing the overall inference process by leveraging the strengths
of each modality to compensate for the absence of certain modalities during
inference. We apply MUTUD to various audiovisual speech tasks and show that it
can reduce the performance gap between the multimodal and corresponding
unimodal models to a considerable extent. MUTUD can achieve this while reducing
the model size and compute compared to multimodal models, in some cases by
almost 80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamArtist++: <span class="highlight-title">Controllable</span> One-Shot Text-to-Image <span class="highlight-title">Generation</span> via
  Positive-Negative Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Dong, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-arts text-to-image generation models such as Imagen and Stable
Diffusion Model have succeed remarkable progresses in synthesizing
high-quality, feature-rich images with high resolution guided by human text
prompts. Since certain characteristics of image content \emph{e.g.}, very
specific object entities or styles, are very hard to be accurately described by
text, some example-based image generation approaches have been proposed,
\emph{i.e.} generating new concepts based on absorbing the salient features of
a few input references. Despite of acknowledged successes, these methods have
struggled on accurately capturing the reference examples' characteristics while
keeping diverse and high-quality image generation, particularly in the one-shot
scenario (\emph{i.e.} given only one reference). To tackle this problem, we
propose a simple yet effective framework, namely DreamArtist, which adopts a
novel positive-negative prompt-tuning learning strategy on the pre-trained
diffusion model, and it has shown to well handle the trade-off between the
accurate controllability and fidelity of image generation with only one
reference example. Specifically, our proposed framework incorporates both
positive and negative embeddings or adapters and optimizes them in a joint
manner. The positive part aggressively captures the salient characteristics of
the reference image to drive diversified generation and the negative part
rectifies inadequacies from the positive part. We have conducted extensive
experiments and evaluated the proposed method from image similarity (fidelity)
and diversity, generation controllability, and style cloning. And our
DreamArtist has achieved a superior generation performance over existing
methods. Besides, our additional evaluation on extended tasks, including
concept compositions and prompt-guided image editing, demonstrates its
effectiveness for more applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmoDubber: Towards High Quality and Emotion <span class="highlight-title">Controllable</span> Movie Dubbing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a piece of text, a video clip, and a reference audio, the movie dubbing
task aims to generate speech that aligns with the video while cloning the
desired voice. The existing methods have two primary deficiencies: (1) They
struggle to simultaneously hold audio-visual sync and achieve clear
pronunciation; (2) They lack the capacity to express user-defined emotions. To
address these problems, we propose EmoDubber, an emotion-controllable dubbing
architecture that allows users to specify emotion type and emotional intensity
while satisfying high-quality lip sync and pronunciation. Specifically, we
first design Lip-related Prosody Aligning (LPA), which focuses on learning the
inherent consistency between lip motion and prosody variation by duration level
contrastive learning to incorporate reasonable alignment. Then, we design
Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences
by efficient conformer to improve speech intelligibility. Next, the speaker
identity adapting module aims to decode acoustics prior and inject the speaker
style embedding. After that, the proposed Flow-based User Emotion Controlling
(FUEC) is used to synthesize waveform by flow matching prediction network
conditioned on acoustics prior. In this process, the FUEC determines the
gradient direction and guidance scale based on the user's emotion instructions
by the positive and negative guidance mechanism, which focuses on amplifying
the desired emotion while suppressing others. Extensive experimental results on
three benchmark datasets demonstrate favorable performance compared to several
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-29T00:00:00Z">2025-01-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-based Query Rewriting with Distilled LLM for online E-Commerce
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy A. Nguyen, Rishi Kesav Mohan, Van Yang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewriting (QR) is a critical technique in e-commerce search, addressing
the lexical gap between user queries and product descriptions to enhance search
performance. Existing QR approaches typically fall into two categories:
discriminative models and generative methods leveraging large language models
(LLMs). Discriminative models often struggle with natural language
understanding and offer limited flexibility in rewriting, while generative
LLMs, despite producing high-quality rewrites, face high inference latency and
cost in online settings. These limitations force offline deployment, making
them vulnerable to issues like information staleness and semantic drift. To
overcome these challenges, we propose a novel hybrid pipeline for QR that
balances efficiency and effectiveness. Our approach combines offline knowledge
distillation to create a lightweight but efficient student model with online
reinforcement learning (RL) to refine query rewriting dynamically using
real-time feedback. A key innovation is the use of LLMs as simulated human
feedback, enabling scalable reward signals and cost-effective evaluation
without manual annotations. Experimental results on Amazon ESCI dataset
demonstrate significant improvements in query relevance, diversity, and
adaptability, as well as positive feedback from the LLM simulation. This work
contributes to advancing LLM capabilities for domain-specific applications,
offering a robust solution for dynamic and complex e-commerce search
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Generative LLMs Create Query Variants for Test Collections? An
  Exploratory Study <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the utility of a Large Language Model (LLM) to
automatically generate queries and query variants from a description of an
information need. Given a set of information needs described as backstories, we
explore how similar the queries generated by the LLM are to those generated by
humans. We quantify the similarity using different metrics and examine how the
use of each set would contribute to document pooling when building test
collections. Our results show potential in using LLMs to generate query
variants. While they may not fully capture the wide variety of human-generated
variants, they generate similar sets of relevant documents, reaching up to
71.1% overlap at a pool depth of 100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of SIGIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs can be Fooled into Labelling a Document as Relevant (best café
  near me; this paper is perfectly relevant) <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwah Alaofi, Paul Thomas, Falk Scholer, Mark Sanderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly being used to assess the relevance of information
objects. This work reports on experiments to study the labelling of short texts
(i.e., passages) for relevance, using multiple open-source and proprietary
LLMs. While the overall agreement of some LLMs with human judgements is
comparable to human-to-human agreement measured in previous research, LLMs are
more likely to label passages as relevant compared to human judges, indicating
that LLM labels denoting non-relevance are more reliable than those indicating
relevance.
  This observation prompts us to further examine cases where human judges and
LLMs disagree, particularly when the human judge labels the passage as
non-relevant and the LLM labels it as relevant. Results show a tendency for
many LLMs to label passages that include the original query terms as relevant.
We, therefore, conduct experiments to inject query words into random and
irrelevant passages, not unlike the way we inserted the query "best caf\'e near
me" into this paper. The results show that LLMs are highly influenced by the
presence of query words in the passages under assessment, even if the wider
passage has no relevance to the query. This tendency of LLMs to be fooled by
the mere presence of query words demonstrates a weakness in our current
measures of LLM labelling: relying on overall agreement misses important
patterns of failures. There is a real risk of bias in LLM-generated relevance
labels and, therefore, a risk of bias in rankers trained on those labels.
  We also investigate the effects of deliberately manipulating LLMs by
instructing them to label passages as relevant, similar to the instruction
"this paper is perfectly relevant" inserted above. We find that such
manipulation influences the performance of some LLMs, highlighting the critical
need to consider potential vulnerabilities when deploying LLMs in real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of SIGIR-AP'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregation Schemes for Single-Vector WSI Representation Learning in
  Digital Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sobhan Hemati, Ghazal Alabtah, Saghir Alfasly, H. R. Tizhoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial step to efficiently integrate Whole Slide Images (WSIs) in
computational pathology is assigning a single high-quality feature vector,
i.e., one embedding, to each WSI. With the existence of many pre-trained deep
neural networks and the emergence of foundation models, extracting embeddings
for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,
given their high resolution and gigapixel nature, inputting them into existing
GPUs as a single image is not feasible. As a result, WSIs are usually split
into many patches. Feeding each patch to a pre-trained model, each WSI can then
be represented by a set of patches, hence, a set of embeddings. Hence, in such
a setup, WSI representation learning reduces to set representation learning
where for each WSI we have access to a set of patch embeddings. To obtain a
single embedding from a set of patch embeddings for each WSI, multiple
set-based learning schemes have been proposed in the literature. In this paper,
we evaluate the WSI search performance of multiple recently developed
aggregation techniques (mainly set representation learning techniques)
including simple average or max pooling operations, Deep Sets, Memory networks,
Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse
and binary Fisher Vector on four different primary sites including bladder,
breast, kidney, and Colon from TCGA. Further, we benchmark the search
performance of these methods against the median of minimum distances of patch
embeddings, a non-aggregating approach used for WSI retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WARP: An Efficient Engine for Multi-Vector Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Luca Scheerer, Matei Zaharia, Christopher Potts, Gustavo Alonso, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the efficiency of multi-vector retrieval methods like ColBERT and
its recent variant XTR. We introduce WARP, a retrieval engine that drastically
improves the efficiency of XTR-based ColBERT retrievers through three key
innovations: (1) WARP$_\text{SELECT}$ for dynamic similarity imputation, (2)
implicit decompression to bypass costly vector reconstruction, and (3) a
two-stage reduction process for efficient scoring. Combined with optimized C++
kernels and specialized inference runtimes, WARP reduces end-to-end latency by
41x compared to XTR's reference implementation and thereby achieves a 3x
speedup over PLAID from the the official ColBERT implementation.
  We study the efficiency of multi-vector retrieval methods like ColBERT and
its recent variant XTR. We introduce WARP, a retrieval engine that drastically
improves the efficiency of XTR-based ColBERT retrievers through three key
innovations: (1) WARP$_\text{SELECT}$ for dynamic similarity imputation, (2)
implicit decompression during retrieval, and (3) a two-stage reduction process
for efficient scoring. Thanks also to highly-optimized C++ kernels and to the
adoption of specialized inference runtimes, WARP can reduce end-to-end query
latency relative to XTR's reference implementation by 41x. And it thereby
achieves a 3x speedup over the official ColBERTv2 PLAID engine, while
preserving retrieval quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distinguished Quantized Guidance for Diffusion-based Sequence
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Mao, Shuchang Liu, Haoyang Liu, Haozhe Liu, Xiang Li, Lanatao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have emerged as promising approaches for sequential
recommendation due to their strong ability to model data distributions and
generate high-quality items. Existing work typically adds noise to the next
item and progressively denoises it guided by the user's interaction sequence,
generating items that closely align with user interests. However, we identify
two key issues in this paradigm. First, the sequences are often heterogeneous
in length and content, exhibiting noise due to stochastic user behaviors. Using
such sequences as guidance may hinder DMs from accurately understanding user
interests. Second, DMs are prone to data bias and tend to generate only the
popular items that dominate the training dataset, thus failing to meet the
personalized needs of different users. To address these issues, we propose
Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation
(DiQDiff), which aims to extract robust guidance to understand user interests
and generate distinguished items for personalized user interests within DMs. To
extract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)
to quantize sequences into semantic vectors (e.g., collaborative signals and
category interests) using a codebook, which can enrich the guidance to better
understand user interests. To generate distinguished items, DiQDiff
personalizes the generation through Contrastive Discrepancy Maximization (CDM),
which maximizes the distance between denoising trajectories using contrastive
loss to prevent biased generation for different users. Extensive experiments
are conducted to compare DiQDiff with multiple baseline models across four
widely-used datasets. The superior recommendation performance of DiQDiff
against leading approaches demonstrates its effectiveness in sequential
recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification and Decomposition for LLM-based
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of large language models (LLMs) for
recommendation, we demonstrate that LLMs often exhibit uncertainty in their
recommendations. To ensure the trustworthy use of LLMs in generating
recommendations, we emphasize the importance of assessing the reliability of
recommendations generated by LLMs. We start by introducing a novel framework
for estimating the predictive uncertainty to quantitatively measure the
reliability of LLM-based recommendations. We further propose to decompose the
predictive uncertainty into recommendation uncertainty and prompt uncertainty,
enabling in-depth analyses of the primary source of uncertainty. Through
extensive experiments, we (1) demonstrate predictive uncertainty effectively
indicates the reliability of LLM-based recommendations, (2) investigate the
origins of uncertainty with decomposed uncertainty measures, and (3) propose
uncertainty-aware prompting for a lower predictive uncertainty and enhanced
recommendation. Our source code and model weights are available at
https://github.com/WonbinKweon/UNC_LLM_REC_WWW2025
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Language Approach for Quranic QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Islam Oshallah, Mohamed Basem, Ali Hamdi, Ammar Mohammed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering systems face critical limitations in languages with
limited resources and scarce data, making the development of robust models
especially challenging. The Quranic QA system holds significant importance as
it facilitates a deeper understanding of the Quran, a Holy text for over a
billion people worldwide. However, these systems face unique challenges,
including the linguistic disparity between questions written in Modern Standard
Arabic and answers found in Quranic verses written in Classical Arabic, and the
small size of existing datasets, which further restricts model performance. To
address these challenges, we adopt a cross-language approach by (1) Dataset
Augmentation: expanding and enriching the dataset through machine translation
to convert Arabic questions into English, paraphrasing questions to create
linguistic diversity, and retrieving answers from an English translation of the
Quran to align with multilingual training requirements; and (2) Language Model
Fine-Tuning: utilizing pre-trained models such as BERT-Medium, RoBERTa-Base,
DeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, and Falcon to address the
specific requirements of Quranic QA. Experimental results demonstrate that this
cross-language approach significantly improves model performance, with
RoBERTa-Base achieving the highest MAP@10 (0.34) and MRR (0.52), while
DeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24). These
findings underscore the effectiveness of cross-language strategies in
overcoming linguistic barriers and advancing Quranic QA systems
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Value Function Decomposition in Markov Recommendation Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobei Wang, Shuchang Liu, Qingpeng Cai, Xiang Li, Lantao Hu, Han li, Guangming Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in recommender systems have shown that user-system
interaction essentially formulates long-term optimization problems, and online
reinforcement learning can be adopted to improve recommendation performance.
The general solution framework incorporates a value function that estimates the
user's expected cumulative rewards in the future and guides the training of the
recommendation policy. To avoid local maxima, the policy may explore potential
high-quality actions during inference to increase the chance of finding better
future rewards. To accommodate the stepwise recommendation process, one widely
adopted approach to learning the value function is learning from the difference
between the values of two consecutive states of a user. However, we argue that
this paradigm involves an incorrect approximation in the stochastic process.
Specifically, between the current state and the next state in each training
sample, there exist two separate random factors from the stochastic policy and
the uncertain user environment. Original temporal difference (TD) learning
under these mixed random factors may result in a suboptimal estimation of the
long-term rewards. As a solution, we show that these two factors can be
separately approximated by decomposing the original temporal difference loss.
The disentangled learning framework can achieve a more accurate estimation with
faster learning and improved robustness against action exploration. As
empirical verification of our proposed method, we conduct offline experiments
with online simulated environments built based on public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WavePulse: Real-time Content Analytics of Radio Livestreams <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Govind Mittal, Sarthak Gupta, Shruti Wagle, Chirag Chopra, Anthony J DeMattee, Nasir Memon, Mustaque Ahamad, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio remains a pervasive medium for mass information dissemination, with
AM/FM stations reaching more Americans than either smartphone-based social
networking or live television. Increasingly, radio broadcasts are also streamed
online and accessed over the Internet. We present WavePulse, a framework that
records, documents, and analyzes radio content in real-time. While our
framework is generally applicable, we showcase the efficacy of WavePulse in a
collaborative project with a team of political scientists focusing on the 2024
Presidential Elections. We use WavePulse to monitor livestreams of 396 news
radio stations over a period of three months, processing close to 500,000 hours
of audio streams. These streams were converted into time-stamped, diarized
transcripts and analyzed to track answer key political science questions at
both the national and state levels. Our analysis revealed how local issues
interacted with national trends, providing insights into information flow. Our
results demonstrate WavePulse's efficacy in capturing and analyzing content
from radio livestreams sourced from the Web. Code and dataset can be accessed
at \url{https://wave-pulse.io}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at The Web Conference (WWW) 2025. 20 Pages, 24 figures.
  Access code and dataset at https://wave-pulse.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of No-Regret Dynamics in Information Retrieval Games
  with Proportional Ranking Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Madmon, Idan Pipano, Itamar Reinman, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publishers who publish their content on the web act strategically, in a
behavior that can be modeled within the online learning framework. Regret, a
central concept in machine learning, serves as a canonical measure for
assessing the performance of learning agents within this framework. We prove
that any proportional content ranking function with a concave activation
function induces games in which no-regret learning dynamics converge. Moreover,
for proportional ranking functions, we prove the equivalence of the concavity
of the activation function, the social concavity of the induced games and the
concavity of the induced games. We also study the empirical trade-offs between
publishers' and users' welfare, under different choices of the activation
function, using a state-of-the-art no-regret dynamics algorithm. Furthermore,
we demonstrate how the choice of the ranking function and changes in the
ecosystem structure affect these welfare measures, as well as the dynamics'
convergence rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerPlexRank: Exploring Node Centrality and Layer Influence through
  Algebraic Connectivity in Multiplex Networks <span class="chip">CIKM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ren, Jiaojiao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the calculation of centrality in complex networks becomes increasingly
vital across technological, biological, and social systems, precise and
scalable ranking methods are essential for understanding these networks. This
paper introduces LayerPlexRank, an algorithm that simultaneously assesses node
centrality and layer influence in multiplex networks using algebraic
connectivity metrics. This method enhances the robustness of the ranking
algorithm by effectively assessing structural changes across layers using
random walk, considering the overall connectivity of the graph. We substantiate
the utility of LayerPlexRank with theoretical analyses and empirical
validations on varied real-world datasets, contrasting it with established
centrality measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 33rd ACM International Conference on
  Information and Knowledge Management (CIKM '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Sequential Alignment and Uniformity: Toward Enhanced
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Cao, Liangwei Yang, Zhiwei Liu, Yuqing Liu, Chen Wang, Yueqing Liang, Hao Peng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based and sequential methods are two popular recommendation paradigms,
each excelling in its domain but lacking the ability to leverage signals from
the other. To address this, we propose a novel method that integrates both
approaches for enhanced performance. Our framework uses Graph Neural Network
(GNN)-based and sequential recommenders as separate submodules while sharing a
unified embedding space optimized jointly. To enable positive knowledge
transfer, we design a loss function that enforces alignment and uniformity both
within and across submodules. Experiments on three real-world datasets
demonstrate that the proposed method significantly outperforms using either
approach alone and achieves state-of-the-art results. Our implementations are
publicly available at https://github.com/YuweiCao-UIC/GSAU.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Guided Virtual Reality Therapy for Anxiety: A Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Winona Graham, Russell Drinkwater, Joshua Kelson, Muhammad Ashad Kabir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual reality (VR) technology can be used to treat anxiety symptoms and
disorders. However, most VR interventions for anxiety have been therapist
guided rather than self-guided. This systematic review aimed to examine the
effectiveness and user experience (i.e., usability, acceptability, safety, and
attrition rates) of self-guided VR therapy interventions in people with any
anxiety condition as well as provide future research directions. Peer-reviewed
journal articles reporting on self-guided VR interventions for anxiety were
sought from the Cochrane Library, IEEE Explore Digital Library, PsycINFO,
PubMED, Scopus, and Web of Science databases. Study data from the eligible
articles were extracted, tabulated, and addressed with a narrative synthesis. A
total of 21 articles met the inclusion criteria. The findings revealed that
self-guided VR interventions for anxiety can provide an effective treatment of
social anxiety disorder, public speaking anxiety, and specific phobias. User
experiences outcomes of safety, usability, and acceptability were generally
positive and the average attrition rate was low. However, there was a lack of
standardised assessments to measure user experiences. Self-guided VR for
anxiety can provide an engaging approach for effectively and safely treating
common anxiety conditions. Nevertheless, more experimental studies are required
to examine their use in underrepresented anxiety populations, their long-term
treatment effects beyond 12 months, and compare their effectiveness against
other self-help interventions for anxiety (e.g., internet interventions and
bibliotherapy).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Video Coding Meets Multimodal Large Language Models: A Unified
  Paradigm for Video Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Zhang, Jinlong Li, Kecheng Chen, Meng Wang, Long Xu, Haoliang Li, Nicu Sebe, Sam Kwong, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing codecs are designed to eliminate intrinsic redundancies to create a
compact representation for compression. However, strong external priors from
Multimodal Large Language Models (MLLMs) have not been explicitly explored in
video compression. Herein, we introduce a unified paradigm for Cross-Modality
Video Coding (CMVC), which is a pioneering approach to explore multimodality
representation and video generative models in video coding. Specifically, on
the encoder side, we disentangle a video into spatial content and motion
components, which are subsequently transformed into distinct modalities to
achieve very compact representation by leveraging MLLMs. During decoding,
previously encoded components and video generation models are leveraged to
create multiple encoding-decoding modes that optimize video reconstruction
quality for specific decoding requirements, including Text-Text-to-Video (TT2V)
mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)
mode to achieve superb perceptual consistency. In addition, we propose an
efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)
tuning to guarantee perceptual quality, which allows the generated motion cues
to behave smoothly. Experiments on benchmarks indicate that TT2V achieves
effective semantic reconstruction, while IT2V exhibits competitive perceptual
consistency. These results highlight potential directions for future research
in video coding.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-28T00:00:00Z">2025-01-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeRAG: Benchmarking Security in Retrieval-Augmented <span class="highlight-title">Generation</span> of
  Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jason Zhaoxin Fan, Bo Tang, Shichao Song, Mengwei Wang, Jiawei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The indexing-retrieval-generation paradigm of retrieval-augmented generation
(RAG) has been highly successful in solving knowledge-intensive tasks by
integrating external knowledge into large language models (LLMs). However, the
incorporation of external and unverified knowledge increases the vulnerability
of LLMs because attackers can perform attack tasks by manipulating knowledge.
In this paper, we introduce a benchmark named SafeRAG designed to evaluate the
RAG security. First, we classify attack tasks into silver noise, inter-context
conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security
evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We
then utilize the SafeRAG dataset to simulate various attack scenarios that RAG
may encounter. Experiments conducted on 14 representative RAG components
demonstrate that RAG exhibits significant vulnerability to all attack tasks and
even the most apparent attack task can easily bypass existing retrievers,
filters, or advanced LLMs, resulting in the degradation of RAG service quality.
Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block
  Representations with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Eric Gaussier, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have demonstrated exceptional
power in various domains, including information retrieval. Most of the previous
practices involve leveraging these models to create a single embedding for each
query, each passage, or each document individually, a strategy exemplified and
used by the Retrieval-Augmented Generation (RAG) framework. While this method
has proven effective, we argue that it falls short in fully capturing the
nuanced intricacies of document-level texts due to its reliance on a relatively
coarse-grained representation. To address this limitation, we introduce a
novel, fine-grained approach aimed at enhancing the accuracy of relevance
scoring for long documents. Our methodology firstly segments a long document
into blocks, each of which is embedded using an LLM, for matching with the
query representation. When calculating the relevance score, we aggregate the
query-block relevance scores through a weighted sum method, yielding a
comprehensive score for the query with the entire document. Despite its
apparent simplicity, our experimental findings reveal that this approach
outperforms standard representation methods and achieves a significant
reduction in embedding generation latency. Moreover, by carefully optimizing
pairwise loss functions, superior performances have been achieved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Ekaterina Khramtsova, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in dense retrieval have introduced vision-language model
(VLM)-based retrievers, such as DSE and ColPali, which leverage document
screenshots embedded as vectors to enable effective search and offer a
simplified pipeline over traditional text-only methods. In this study, we
propose three pixel poisoning attack methods designed to compromise VLM-based
retrievers and evaluate their effectiveness under various attack settings and
parameter configurations. Our empirical results demonstrate that injecting even
a single adversarial screenshot into the retrieval corpus can significantly
disrupt search results, poisoning the top-10 retrieved documents for 41.9% of
queries in the case of DSE and 26.4% for ColPali. These vulnerability rates
notably exceed those observed with equivalent attacks on text-only retrievers.
Moreover, when targeting a small set of known queries, the attack success rate
raises, achieving complete success in certain cases. By exposing the
vulnerabilities inherent in vision-language models, this work highlights the
potential risks associated with their deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Federated Graph-Filtering for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Nicolas, César Sabater, Mohamed Maouche, Sonia Ben Mokhtar, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often rely on graph-based filters, such as normalized
item-item adjacency matrices and low-pass filters. While effective, the
centralized computation of these components raises concerns about privacy,
security, and the ethical use of user data. This work proposes two
decentralized frameworks for securely computing these critical graph components
without centralizing sensitive information. The first approach leverages
lightweight Multi-Party Computation and distributed singular vector
computations to privately compute key graph filters. The second extends this
framework by incorporating low-rank approximations, enabling a trade-off
between communication efficiency and predictive performance. Empirical
evaluations on benchmark datasets demonstrate that the proposed methods achieve
comparable accuracy to centralized state-of-the-art systems while ensuring data
confidentiality and maintaining low communication costs. Our results highlight
the potential for privacy-preserving decentralized architectures to bridge the
gap between utility and user data protection in modern recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypergraph Diffusion for High-Order Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darnbi Sakong, Thanh Trung Huynh, Jun Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems rely on Collaborative Filtering (CF) to predict user
preferences by leveraging patterns in historical user-item interactions. While
traditional CF methods primarily focus on learning compact vector embeddings
for users and items, graph neural network (GNN)-based approaches have emerged
as a powerful alternative, utilizing the structure of user-item interaction
graphs to enhance recommendation accuracy. However, existing GNN-based models,
such as LightGCN and UltraGCN, often struggle with two major limitations: an
inability to fully account for heterophilic interactions, where users engage
with diverse item categories, and the over-smoothing problem in multi-layer
GNNs, which hinders their ability to model complex, high-order relationships.
To address these gaps, we introduce WaveHDNN, an innovative wavelet-enhanced
hypergraph diffusion framework. WaveHDNN integrates a Heterophily-aware
Collaborative Encoder, designed to capture user-item interactions across
diverse categories, with a Multi-scale Group-wise Structure Encoder, which
leverages wavelet transforms to effectively model localized graph structures.
Additionally, cross-view contrastive learning is employed to maintain robust
and consistent representations. Experiments on benchmark datasets validate the
efficacy of WaveHDNN, demonstrating its superior ability to capture both
heterophilic and localized structural information, leading to improved
recommendation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VeriFact: Verifying Facts in LLM-<span class="highlight-title">Generate</span>d Clinical Text with Electronic
  Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages, 5 figures, 1 table, pre-print manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundational Large Language Models for Materials Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret,  Mausam, N. M. Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Materials discovery and development are critical for addressing global
challenges. Yet, the exponential growth in materials science literature
comprising vast amounts of textual data has created significant bottlenecks in
knowledge extraction, synthesis, and scientific reasoning. Large Language
Models (LLMs) offer unprecedented opportunities to accelerate materials
research through automated analysis and prediction. Still, their effective
deployment requires domain-specific adaptation for understanding and solving
domain-relevant tasks. Here, we present LLaMat, a family of foundational models
for materials science developed through continued pretraining of LLaMA models
on an extensive corpus of materials literature and crystallographic data.
Through systematic evaluation, we demonstrate that LLaMat excels in
materials-specific NLP and structured information extraction while maintaining
general linguistic capabilities. The specialized LLaMat-CIF variant
demonstrates unprecedented capabilities in crystal structure generation,
predicting stable crystals with high coverage across the periodic table.
Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,
we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific
performance across diverse materials science tasks, including structured
information extraction from text and tables, more particularly in crystal
structure generation, a potential adaptation rigidity in overtrained LLMs.
Altogether, the present work demonstrates the effectiveness of domain
adaptation towards developing practically deployable LLM copilots for materials
research. Beyond materials science, our findings reveal important
considerations for domain adaptation of LLMs, such as model selection, training
methodology, and domain-specific performance, which may influence the
development of specialized scientific AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Passage Embeddings for Efficient Listwise Reranking with
  Large Language Models <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of using large language
language models (LLMs) in passage ranking. The listwise approaches, such as
RankGPT, have become new state-of-the-art in this task. However, the efficiency
of RankGPT models is limited by the maximum context length and relatively high
latency of LLM inference. To address these issues, in this paper, we propose
PE-Rank, leveraging the single passage embedding as a good context compression
for efficient listwise passage reranking. By treating each passage as a special
token, we can directly input passage embeddings into LLMs, thereby reducing
input length. Additionally, we introduce an inference method that dynamically
constrains the decoding space to these special tokens, accelerating the
decoding process. For adapting the model to reranking, we employ listwise
learning to rank loss for training. Evaluation results on multiple benchmarks
demonstrate that PE-Rank significantly improves efficiency in both prefilling
and decoding, while maintaining competitive ranking effectiveness. The Code is
available at https://github.com/liuqi6777/pe_rank.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Negative Samples for Multi-Modal Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ji, Yue Ding, Dan Luo, Chang Liu, Jing Tong, Shaokai Wu, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal recommender systems (MMRS) have gained significant attention due
to their ability to leverage information from various modalities to enhance
recommendation quality. However, existing negative sampling techniques often
struggle to effectively utilize the multi-modal data, leading to suboptimal
performance. In this paper, we identify two key challenges in negative sampling
for MMRS: (1) producing cohesive negative samples contrasting with positive
samples and (2) maintaining a balanced influence across different modalities.
To address these challenges, we propose NegGen, a novel framework that utilizes
multi-modal large language models (MLLMs) to generate balanced and contrastive
negative samples. We design three different prompt templates to enable NegGen
to analyze and manipulate item attributes across multiple modalities, and then
generate negative samples that introduce better supervision signals and ensure
modality balance. Furthermore, NegGen employs a causal learning module to
disentangle the effect of intervened key features and irrelevant item
attributes, enabling fine-grained learning of user preferences. Extensive
experiments on real-world datasets demonstrate the superior performance of
NegGen compared to state-of-the-art methods in both negative sampling and
multi-modal recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIDI-<span class="highlight-title">GPT</span>: A <span class="highlight-title">Controllable</span> Generative Model for Computer-Assisted
  Multitrack Music Composition <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Pasquier, Jeff Ens, Nathan Fradet, Paul Triana, Davide Rizzotti, Jean-Baptiste Rolland, Maryam Safi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and release MIDI-GPT, a generative system based on the Transformer
architecture that is designed for computer-assisted music composition
workflows. MIDI-GPT supports the infilling of musical material at the track and
bar level, and can condition generation on attributes including: instrument
type, musical style, note density, polyphony level, and note duration. In order
to integrate these features, we employ an alternative representation for
musical material, creating a time-ordered sequence of musical events for each
track and concatenating several tracks into a single sequence, rather than
using a single time-ordered sequence where the musical events corresponding to
different tracks are interleaved. We also propose a variation of our
representation allowing for expressiveness. We present experimental results
that demonstrate that MIDI-GPT is able to consistently avoid duplicating the
musical material it was trained on, generate music that is stylistically
similar to the training dataset, and that attribute controls allow enforcing
various constraints on the generated material. We also outline several
real-world applications of MIDI-GPT, including collaborations with industry
partners that explore the integration and evaluation of MIDI-GPT into
commercial products, as well as several artistic works produced using it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVE Speech <span class="highlight-title">Dataset</span>: A Comprehensive Benchmark for Multi-Modal Speech
  Recognition Integrating Audio, Visual, and Electromyographic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Zhou, Yakun Zhang, Jinghan Wu, Xingyu Zhang, Liang Xie, Erwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global aging population faces considerable challenges, particularly in
communication, due to the prevalence of hearing and speech impairments. To
address these, we introduce the AVE speech dataset, a comprehensive multi-modal
benchmark for speech recognition tasks. The dataset includes a 100-sentence
Mandarin Chinese corpus with audio signals, lip-region video recordings, and
six-channel electromyography (EMG) data, collected from 100 participants. Each
subject read the entire corpus ten times, with each sentence averaging
approximately two seconds in duration, resulting in over 55 hours of
multi-modal speech data per modality. Experiments demonstrate that combining
these modalities significantly improves recognition performance, particularly
in cross-subject and high-noise environments. To our knowledge, this is the
first publicly available sentence-level dataset integrating these three
modalities for large-scale Mandarin speech recognition. We expect this dataset
to drive advancements in both acoustic and non-acoustic speech recognition
research, enhancing cross-modal learning and human-machine interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Deepfake Detection With Local Temporal In<span class="highlight-title">consist</span>encies <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an audio-visual deepfake detection approach that aims to
capture fine-grained temporal inconsistencies between audio and visual
modalities. To achieve this, both architectural and data synthesis strategies
are introduced. From an architectural perspective, a temporal distance map,
coupled with an attention mechanism, is designed to capture these
inconsistencies while minimizing the impact of irrelevant temporal
subsequences. Moreover, we explore novel pseudo-fake generation techniques to
synthesize local inconsistencies. Our approach is evaluated against
state-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating
its effectiveness in detecting audio-visual deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-27T00:00:00Z">2025-01-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 360Brew: A Decoder-only Foundation Model for <span class="highlight-title">Persona</span>lized Ranking and
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Firooz, Maziar Sanjabi, Adrian Englhardt, Aman Gupta, Ben Levine, Dre Olgiati, Gungor Polatkan, Iuliia Melnychuk, Karthik Ramgopal, Kirill Talanine, Kutta Srinivasan, Luke Simon, Natesh Sivasubramoniapillai, Necip Fazil Ayan, Qingquan Song, Samira Sriram, Souvik Ghosh, Tao Song, Vignesh Kothapalli, Xiaoling Zhai, Ya Xu, Yu Wang, Yun Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking and recommendation systems are the foundation for numerous online
experiences, ranging from search results to personalized content delivery.
These systems have evolved into complex, multilayered architectures that
leverage vast datasets and often incorporate thousands of predictive models.
The maintenance and enhancement of these models is a labor intensive process
that requires extensive feature engineering. This approach not only exacerbates
technical debt but also hampers innovation in extending these systems to
emerging problem domains. In this report, we present our research to address
these challenges by utilizing a large foundation model with a textual interface
for ranking and recommendation tasks. We illustrate several key advantages of
our approach: (1) a single model can manage multiple predictive tasks involved
in ranking and recommendation, (2) decoder models with textual interface due to
their comprehension of reasoning capabilities, can generalize to new
recommendation surfaces and out-of-domain problems, and (3) by employing
natural language interfaces for task definitions and verbalizing member
behaviors and their social connections, we eliminate the need for feature
engineering and the maintenance of complex directed acyclic graphs of model
dependencies. We introduce our research pre-production model, 360Brew V1.0, a
150B parameter, decoder-only model that has been trained and fine-tuned on
LinkedIn's data and tasks. This model is capable of solving over 30 predictive
tasks across various segments of the LinkedIn platform, achieving performance
levels comparable to or exceeding those of current production systems based on
offline metrics, without task-specific fine-tuning. Notably, each of these
tasks is conventionally addressed by dedicated models that have been developed
and maintained over multiple years by teams of a similar or larger size than
our own.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based
  Video Event Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen, Huy Nguyen, Bao Khuu, Huy Luu, Huy Le, Tuan Nguyen, Tho Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving events from videos using text queries has become increasingly
challenging due to the rapid growth of multimedia content. Existing methods for
text-based video event retrieval often focus heavily on object-level
descriptions, overlooking the crucial role of contextual information. This
limitation is especially apparent when queries lack sufficient context, such as
missing location details or ambiguous background elements. To address these
challenges, we propose a novel system called RAPID (Retrieval-Augmented
Parallel Inference Drafting), which leverages advancements in Large Language
Models (LLMs) and prompt-based learning to semantically correct and enrich user
queries with relevant contextual information. These enriched queries are then
processed through parallel retrieval, followed by an evaluation step to select
the most relevant results based on their alignment with the original query.
Through extensive experiments on our custom-developed dataset, we demonstrate
that RAPID significantly outperforms traditional retrieval methods,
particularly for contextually incomplete queries. Our system was validated for
both speed and accuracy through participation in the Ho Chi Minh City AI
Challenge 2024, where it successfully retrieved events from over 300 hours of
video. Further evaluation comparing RAPID with the baseline proposed by the
competition organizers demonstrated its superior effectiveness, highlighting
the strength and robustness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at SoICT'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URAG: Implementing a Unified Hybrid RAG for Precise Answers in
  University Admission Chatbots -- A Case Study at HCMUT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen, Tho Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Artificial Intelligence, particularly in
Natural Language Processing, Large Language Models (LLMs) have become pivotal
in educational question-answering systems, especially university admission
chatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other
advanced techniques have been developed to enhance these systems by integrating
specific university data, enabling LLMs to provide informed responses on
admissions and academic counseling. However, these enhanced RAG techniques
often involve high operational costs and require the training of complex,
specialized modules, which poses challenges for practical deployment.
Additionally, in the educational context, it is crucial to provide accurate
answers to prevent misinformation, a task that LLM-based systems find
challenging without appropriate strategies and methods. In this paper, we
introduce the Unified RAG (URAG) Framework, a hybrid approach that
significantly improves the accuracy of responses, particularly for critical
queries. Experimental results demonstrate that URAG enhances our in-house,
lightweight model to perform comparably to state-of-the-art commercial models.
Moreover, to validate its practical applicability, we conducted a case study at
our educational institution, which received positive feedback and acclaim. This
study not only proves the effectiveness of URAG but also highlights its
feasibility for real-world implementation in educational settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at SoICT'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provence: efficient and robust context pruning for retrieval-augmented
  <span class="highlight-title">generation</span> <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation improves various aspects of large language
models (LLMs) generation, but suffers from computational overhead caused by
long contexts as well as the propagation of irrelevant retrieved information
into generated responses. Context pruning deals with both aspects, by removing
irrelevant parts of retrieved contexts before LLM generation. Existing context
pruning approaches are however limited, and do not provide a universal model
that would be both efficient and robust in a wide range of scenarios, e.g.,
when contexts contain a variable amount of relevant information or vary in
length, or when evaluated on various domains. In this work, we close this gap
and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),
an efficient and robust context pruner for Question Answering, which
dynamically detects the needed amount of pruning for a given context and can be
used out-of-the-box for various domains. The three key ingredients of Provence
are formulating the context pruning task as sequence labeling, unifying context
pruning capabilities with context reranking, and training on diverse data. Our
experimental results show that Provence enables context pruning with negligible
to no drop in performance, in various domains and settings, at almost no cost
in a standard RAG pipeline. We also conduct a deeper analysis alongside various
ablations to provide insights into training context pruners for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separate This, and All of these Things Around It: Music Source
  Separation via Hyperellipsoidal Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karn N. Watcharasupat, Alexander Lerch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music source separation is an audio-to-audio retrieval task of extracting one
or more constituent components, or composites thereof, from a musical audio
mixture. Each of these constituent components is often referred to as a "stem"
in literature. Historically, music source separation has been dominated by a
stem-based paradigm, leading to most state-of-the-art systems being either a
collection of single-stem extraction models, or a tightly coupled system with a
fixed, difficult-to-modify, set of supported stems. Combined with the limited
data availability, advances in music source separation have thus been mostly
limited to the "VDBO" set of stems: \textit{vocals}, \textit{drum},
\textit{bass}, and the catch-all \textit{others}. Recent work in music source
separation has begun to challenge the fixed-stem paradigm, moving towards
models able to extract any musical sound as long as this target type of sound
could be specified to the model as an additional query input. We generalize
this idea to a \textit{query-by-region} source separation system, specifying
the target based on the query regardless of how many sound sources or which
sound classes are contained within it. To do so, we propose the use of
hyperellipsoidal regions as queries to allow for an intuitive yet easily
parametrizable approach to specifying both the target (location) as well as its
spread. Evaluation of the proposed system on the MoisesDB dataset demonstrated
state-of-the-art performance of the proposed system both in terms of
signal-to-noise ratios and retrieval metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2025 International Joint Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span>: Understand the challenges of MachineLearning Experts using Named
  EntityRecognition Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Freund, Philippe Tamla, Matthias Hemmje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a survey based on Kasunic's survey research methodology
to identify the criteria used by Machine Learning (ML) experts to evaluate
Named Entity Recognition (NER) tools and frameworks. Comparison and selection
of NER tools and frameworks is a critical step in leveraging NER for
Information Retrieval to support the development of Clinical Practice
Guidelines. In addition, this study examines the main challenges faced by ML
experts when choosing suitable NER tools and frameworks. Using Nunamaker's
methodology, the article begins with an introduction to the topic,
contextualizes the research, reviews the state-of-the-art in science and
technology, and identifies challenges for an expert survey on NER tools and
frameworks. This is followed by a description of the survey's design and
implementation. The paper concludes with an evaluation of the survey results
and the insights gained, ending with a summary and conclusions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, 13 Figures, 6th International Conference on Natural
  Language Processing, Information Retrieval and AI (NIAI 2025) January 25 ~
  26, 2025, Copenhagen, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Options-Aware Dense Retrieval for Multiple-Choice query Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Singh, Manish Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context multiple-choice question answering tasks require robust
reasoning over extensive text sources. Since most of the pre-trained
transformer models are restricted to processing only a few hundred words at a
time, successful completion of such tasks often relies on the identification of
evidence spans, such as sentences, that provide supporting evidence for
selecting the correct answer. Prior research in this domain has predominantly
utilized pre-trained dense retrieval models, given the absence of supervision
to fine-tune the retrieval process. This paper proposes a novel method called
Options Aware Dense Retrieval (OADR) to address these challenges. ORDA uses an
innovative approach to fine-tuning retrieval by leveraging query-options
embeddings, which aim to mimic the embeddings of the oracle query (i.e., the
query paired with the correct answer) for enhanced identification of supporting
evidence. Through experiments conducted on the QuALITY benchmark dataset, we
demonstrate that our proposed model surpasses existing baselines in terms of
performance and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PISCO: Pretty Simple Compression for Retrieval-Augmented <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Louis, Hervé Déjean, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models
(LLMs) by retrieving relevant documents, but they face scalability issues due
to high inference costs and limited context size. Document compression is a
practical solution, but current soft compression methods suffer from accuracy
losses and require extensive pretraining. In this paper, we introduce PISCO, a
novel method that achieves a 16x compression rate with minimal accuracy loss
(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing
approaches, PISCO requires no pretraining or annotated data, relying solely on
sequence-level knowledge distillation from document-based questions. With the
ability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers
a highly efficient and scalable solution. We present comprehensive experiments
showing that PISCO outperforms existing compression models by 8% in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Long Videos via LLM-Powered Entity Relation Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chu, Yicong Li, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of extended video content poses unique challenges in artificial
intelligence, particularly when dealing with the complexity of tracking and
understanding visual elements across time. Current methodologies that process
video frames sequentially struggle to maintain coherent tracking of objects,
especially when these objects temporarily vanish and later reappear in the
footage. A critical limitation of these approaches is their inability to
effectively identify crucial moments in the video, largely due to their limited
grasp of temporal relationships. To overcome these obstacles, we present
GraphVideoAgent, a cutting-edge system that leverages the power of graph-based
object tracking in conjunction with large language model capabilities. At its
core, our framework employs a dynamic graph structure that maps and monitors
the evolving relationships between visual entities throughout the video
sequence. This innovative approach enables more nuanced understanding of how
objects interact and transform over time, facilitating improved frame selection
through comprehensive contextual awareness. Our approach demonstrates
remarkable effectiveness when tested against industry benchmarks. In
evaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2
improvement over existing methods while requiring analysis of only 8.2 frames
on average. Similarly, testing on the NExT-QA benchmark yielded a 2.0
performance increase with an average frame requirement of 8.1. These results
underscore the efficiency of our graph-guided methodology in enhancing both
accuracy and computational performance in long-form video understanding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parametric Retrieval Augmented <span class="highlight-title">Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) techniques have emerged as a promising
solution to enhance the reliability of large language models (LLMs) by
addressing issues like hallucinations, outdated knowledge, and domain
adaptation. In particular, existing RAG methods append relevant documents
retrieved from external corpus or databases to the input of LLMs to guide their
generation process, which we refer to as the in-context knowledge injection
method. While this approach is simple and often effective, it has inherent
limitations. Firstly, increasing the context length and number of relevant
documents can lead to higher computational overhead and degraded performance,
especially in complex reasoning tasks. More importantly, in-context knowledge
injection operates primarily at the input level, but LLMs store their internal
knowledge in their parameters. This gap fundamentally limits the capacity of
in-context methods. To this end, we introduce Parametric retrieval-augmented
generation (Parametric RAG), a new RAG paradigm that integrates external
knowledge directly into the parameters of feed-forward networks (FFN) of an LLM
through document parameterization. This approach not only saves online
computational costs by eliminating the need to inject multiple documents into
the LLMs' input context, but also deepens the integration of external knowledge
into the parametric knowledge space of the LLM. Experimental results
demonstrate that Parametric RAG substantially enhances both the effectiveness
and efficiency of knowledge augmentation in LLMs. Also, it can be combined with
in-context RAG methods to achieve even better performance.
  We have open-sourced all the code, data, and models in the following
anonymized GitHub link: https://github.com/oneal2000/PRAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aspect-Aware Decomposition for Opinion Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jey Han Lau, Eduard Hovy, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarization plays a key role in deriving meaningful insights from
large-scale online reviews. To make this process more explainable and grounded,
we propose a modular approach guided by review aspects which separates the
tasks of aspect identification, opinion consolidation, and meta-review
synthesis, enabling greater transparency and ease of inspection. We conduct
extensive experiments across datasets representing scientific research,
business, and product domains. Results show that our method generates more
grounded summaries compared to strong baseline models, as verified through
automated and human evaluations. Additionally, our modular approach, which
incorporates reasoning based on review aspects, produces more informative
intermediate outputs than knowledge-agnostic decomposed prompting. These
intermediate outputs can also effectively support humans in summarizing
opinions from large volumes of reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Term Interest Clock: Fine-Grained Time Perception in Streaming
  Recommendation System <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchun Zhu, Guanyu Jiang, Jingwu Chen, Feng Zhang, Xiao Yang, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interests manifest a dynamic pattern within the course of a day, e.g., a
user usually favors soft music at 8 a.m. but may turn to ambient music at 10
p.m. To model dynamic interests in a day, hour embedding is widely used in
traditional daily-trained industrial recommendation systems. However, its
discreteness can cause periodical online patterns and instability in recent
streaming recommendation systems. Recently, Interest Clock has achieved
remarkable performance in streaming recommendation systems. Nevertheless, it
models users' dynamic interests in a coarse-grained manner, merely encoding
users' discrete interests of 24 hours from short-term behaviors. In this paper,
we propose a fine-grained method for perceiving time information for streaming
recommendation systems, named Long-term Interest Clock (LIC). The key idea of
LIC is adaptively calculating current user interests by taking into
consideration the relevance of long-term behaviors around current time (e.g., 8
a.m.) given a candidate item. LIC consists of two modules: (1) Clock-GSU
retrieves a sub-sequence by searching through long-term behaviors, using query
information from a candidate item and current time, (2) Clock-ESU employs a
time-gap-aware attention mechanism to aggregate sub-sequence with the candidate
item. With Clock-GSU and Clock-ESU, LIC is capable of capturing users' dynamic
fine-grained interests from long-term behaviors. We conduct online A/B tests,
obtaining +0.122% improvements on user active days. Besides, the extended
offline experiments show improvements as well. Long-term Interest Clock has
been integrated into Douyin Music App's recommendation system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaF^2M^2: Comprehensive Learning and Responsive Leveraging Features in
  Recommendation System <span class="chip">DASFAA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, Xiao Yang, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature modeling, which involves feature representation learning and
leveraging, plays an essential role in industrial recommendation systems.
However, the data distribution in real-world applications usually follows a
highly skewed long-tail pattern due to the popularity bias, which easily leads
to over-reliance on ID-based features, such as user/item IDs and ID sequences
of interactions. Such over-reliance makes it hard for models to learn features
comprehensively, especially for those non-ID meta features, e.g., user/item
characteristics. Further, it limits the feature leveraging ability in models,
getting less generalized and more susceptible to data noise. Previous studies
on feature modeling focus on feature extraction and interaction, hardly
noticing the problems brought about by the long-tail data distribution. To
achieve better feature representation learning and leveraging on real-world
data, we propose a model-agnostic framework AdaF^2M^2, short for Adaptive
Feature Modeling with Feature Mask. The feature-mask mechanism helps
comprehensive feature learning via multi-forward training with augmented
samples, while the adapter applies adaptive weights on features responsive to
different user/item states. By arming base models with AdaF^2M^2, we conduct
online A/B tests on multiple recommendation scenarios, obtaining +1.37% and
+1.89% cumulative improvements on user active days and app duration
respectively. Besides, the extended offline experiments on different models
show improvements as well. AdaF$^2$M$^2$ has been widely deployed on both
retrieval and ranking tasks in multiple applications of Douyin Group,
indicating its superior effectiveness and universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Knowledge</span> Organization Systems of Research Fields: Resources
  and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Salatino, Tanay Aggarwal, Andrea Mannocci, Francesco Osborne, Enrico Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Organization Systems (KOSs), such as term lists, thesauri,
taxonomies, and ontologies, play a fundamental role in categorising, managing,
and retrieving information. In the academic domain, KOSs are often adopted for
representing research areas and their relationships, primarily aiming to
classify research articles, academic courses, patents, books, scientific
venues, domain experts, grants, software, experiment materials, and several
other relevant products and agents. These structured representations of
research areas, widely embraced by many academic fields, have proven effective
in empowering AI-based systems to i) enhance retrievability of relevant
documents, ii) enable advanced analytic solutions to quantify the impact of
academic research, and iii) analyse and forecast research dynamics. This paper
aims to present a comprehensive survey of the current KOS for academic
disciplines. We analysed and compared 45 KOSs according to five main
dimensions: scope, structure, curation, usage, and links to other KOSs. Our
results reveal a very heterogeneous scenario in terms of scope, scale, quality,
and usage, highlighting the need for more integrated solutions for representing
research knowledge across academic fields. We conclude by discussing the main
challenges and the most promising future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLMRec: Distilling Large Language Models into Small for Sequential
  Recommendation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17890v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17890v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Qitian Wu, Zujie Liang, Jiaojiao Han, Xuying Ning, Yunxiao Shi, Wenfang Lin, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) task involves predicting the next item a user
is likely to interact with, given their past interactions. The SR models
examine the sequence of a user's actions to discern more complex behavioral
patterns and temporal dynamics. Recent research demonstrates the great impact
of LLMs on sequential recommendation systems, either viewing sequential
recommendation as language modeling or serving as the backbone for user
representation. Although these methods deliver outstanding performance, there
is scant evidence of the necessity of a large language model and how large the
language model is needed, especially in the sequential recommendation scene.
Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to
apply a LLM-based model in real-world platforms that often need to process
billions of traffic logs daily. In this paper, we explore the influence of
LLMs' depth by conducting extensive experiments on large-scale industry
datasets. Surprisingly, our motivational experiments reveal that most
intermediate layers of LLMs are redundant, indicating that pruning the
remaining layers can still maintain strong performance. Motivated by this
insight, we empower small language models for SR, namely SLMRec, which adopt a
simple yet effective knowledge distillation method. Moreover, SLMRec is
orthogonal to other post-training efficiency techniques, such as quantization
and pruning, so that they can be leveraged in combination. Comprehensive
experimental results illustrate that the proposed SLMRec model attains the best
performance using only 13% of the parameters found in LLM-based recommendation
models while simultaneously achieving up to 6.6x and 8.0x speedups in training
and inference time costs, respectively. Besides, we provide a theoretical
justification for why small language models can perform comparably to large
language models in SR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommenadation aided Caching using Combinatorial Multi-armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00080v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00080v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavamana K J, Chandramani Kishore Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study content caching with recommendations in a wireless network where the
users are connected through a base station equipped with a finite-capacity
cache. We assume a fixed set of contents with unknown user preferences and
content popularities. The base station can cache a subset of the contents and
can also recommend subsets of the contents to different users in order to
encourage them to request the recommended contents. Recommendations, depending
on their acceptability, can thus be used to increase cache hits. We first
assume that the users' recommendation acceptabilities are known and formulate
the cache hit optimization problem as a combinatorial multi-armed bandit
(CMAB). We propose a UCB-based algorithm to decide which contents to cache and
recommend and provide an upper bound on the regret of this algorithm.
Subsequently, we consider a more general scenario where the users'
recommendation acceptabilities are also unknown and propose another UCB-based
algorithm that learns these as well. We numerically demonstrate the performance
of our algorithms and compare these to state-of-the-art algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Distilling Medication Recommendation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Zijian Zhang, Feng Tian, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommendation of medication is a vital aspect of intelligent healthcare
systems, as it involves prescribing the most suitable drugs based on a
patient's specific health needs. Unfortunately, many sophisticated models
currently in use tend to overlook the nuanced semantics of medical data, while
only relying heavily on identities. Furthermore, these models face significant
challenges in handling cases involving patients who are visiting the hospital
for the first time, as they lack prior prescription histories to draw upon. To
tackle these issues, we harness the powerful semantic comprehension and
input-agnostic characteristics of Large Language Models (LLMs). Our research
aims to transform existing medication recommendation methodologies using LLMs.
In this paper, we introduce a novel approach called Large Language Model
Distilling Medication Recommendation (LEADER). We begin by creating appropriate
prompt templates that enable LLMs to suggest medications effectively. However,
the straightforward integration of LLMs into recommender systems leads to an
out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a
novel output layer and a refined tuning loss function. Although LLM-based
models exhibit remarkable capabilities, they are plagued by high computational
costs during inference, which is impractical for the healthcare sector. To
mitigate this, we have developed a feature-level knowledge distillation
technique, which transfers the LLM's proficiency to a more compact model.
Extensive experiments conducted on two real-world datasets, MIMIC-III and
MIMIC-IV, demonstrate that our proposed model not only delivers effective
results but also is efficient. To ease the reproducibility of our experiments,
we release the implementation code online.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movement- and Traffic-based User Identification in Commercial Virtual
  Reality Applications: Threats and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Baldoni, Salim Benhamadi, Federico Chiariotti, Michele Zorzi, Federica Battisti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the unprecedented diffusion of virtual reality, the number of
application scenarios is continuously growing. As commercial and gaming
applications become pervasive, the need for the secure and convenient
identification of users, often overlooked by the research in immersive media,
is becoming more and more pressing. Networked scenarios such as Cloud gaming or
cooperative virtual training and teleoperation require both a user-friendly and
streamlined experience and user privacy and security. In this work, we
investigate the possibility of identifying users from their movement patterns
and data traffic traces while playing four commercial games, using a publicly
available dataset. If, on the one hand, this paves the way for easy
identification and automatic customization of the virtual reality content, it
also represents a serious threat to users' privacy due to network
analysis-based fingerprinting. Based on this, we analyze the threats and
opportunities for virtual reality users' security and privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IEEE VR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaDecorator: Generating Immersive Virtual Tours through Multimodality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Xie, Yang Liu, Jeannie S. A. Lee, Haiwei Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MetaDecorator, is a framework that empowers users to personalize virtual
spaces. By leveraging text-driven prompts and image synthesis techniques,
MetaDecorator adorns static panoramas captured by 360{\deg} imaging devices,
transforming them into uniquely styled and visually appealing environments.
This significantly enhances the realism and engagement of virtual tours
compared to traditional offerings. Beyond the core framework, we also discuss
the integration of Large Language Models (LLMs) and haptics in the VR
application to provide a more immersive experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Parallelism in Music and Language: A Perspective from Symbol
  Emergence Systems based on Probabilistic Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music and language are structurally similar. Such structural similarity is
often explained by generative processes. This paper describes the recent
development of probabilistic generative models (PGMs) for language learning and
symbol emergence in robotics. Symbol emergence in robotics aims to develop a
robot that can adapt to real-world environments and human linguistic
communications and acquire language from sensorimotor information alone (i.e.,
in an unsupervised manner). This is regarded as a constructive approach to
symbol emergence systems. To this end, a series of PGMs have been developed,
including those for simultaneous phoneme and word discovery, lexical
acquisition, object and spatial concept formation, and the emergence of a
symbol system. By extending the models, a symbol emergence system comprising a
multi-agent system in which a symbol system emerges is revealed to be modeled
using PGMs. In this model, symbol emergence can be regarded as collective
predictive coding. This paper expands on this idea by combining the theory that
''emotion is based on the predictive coding of interoceptive signals'' and
''symbol emergence systems,'' and describes the possible hypothesis of the
emergence of meaning in music.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Location-Caption Alignment via Complementary Masking for
  Weakly-Supervised Dense Video Captioning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Liu Qin, Ziyao Chen, Qing Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and
describe all events of interest in a video without requiring annotations of
event boundaries. This setting poses a great challenge in accurately locating
the temporal location of event, as the relevant supervision is unavailable.
Existing methods rely on explicit alignment constraints between event locations
and captions, which involve complex event proposal procedures during both
training and inference. To tackle this problem, we propose a novel implicit
location-caption alignment paradigm by complementary masking, which simplifies
the complex event proposal and localization process while maintaining
effectiveness. Specifically, our model comprises two components: a dual-mode
video captioning module and a mask generation module. The dual-mode video
captioning module captures global event information and generates descriptive
captions, while the mask generation module generates differentiable positive
and negative masks for localizing the events. These masks enable the implicit
alignment of event locations and captions by ensuring that captions generated
from positively and negatively masked videos are complementary, thereby forming
a complete video description. In this way, even under weak supervision, the
event location and event caption can be aligned implicitly. Extensive
experiments on the public datasets demonstrate that our method outperforms
existing weakly-supervised methods and achieves competitive results compared to
fully-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-26T00:00:00Z">2025-01-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCP-116K: A High-Quality Problem-Solution <span class="highlight-title">Dataset</span> and a Generalized
  Pipeline for Automated Extraction in the Higher Education Science Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in large language models (LLMs) exemplified by the
impressive mathematical and scientific reasoning capabilities of the o1 model
have spotlighted the critical importance of high-quality training data in
advancing LLM performance across STEM disciplines. While the mathematics
community has benefited from a growing body of curated datasets, the scientific
domain at the higher education level has long suffered from a scarcity of
comparable resources. To address this gap, we present SCP-116K, a new
large-scale dataset of 116,756 high-quality problem-solution pairs,
automatically extracted from heterogeneous sources using a streamlined and
highly generalizable pipeline. Our approach involves stringent filtering to
ensure the scientific rigor and educational level of the extracted materials,
while maintaining adaptability for future expansions or domain transfers. By
openly releasing both the dataset and the extraction pipeline, we seek to
foster research on scientific reasoning, enable comprehensive performance
evaluations of new LLMs, and lower the barrier to replicating the successes of
advanced models like o1 in the broader science community. We believe SCP-116K
will serve as a critical resource, catalyzing progress in high-level scientific
reasoning tasks and promoting further innovations in LLM development. The
dataset and code are publicly available at
https://github.com/AQA6666/SCP-116K-open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Potential of Multimodal Retrieval Augmented <span class="highlight-title">Generation</span>
  with Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Yu, Zhihan Yang, Chong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval Augmented Generation (MRAG) systems, while promising for
enhancing Multimodal Large Language Models (MLLMs), often rely on rigid,
single-step retrieval methods. This limitation hinders their ability to
effectively address real-world scenarios that demand adaptive information
acquisition and query refinement. To overcome this, we introduce the novel task
of Multimodal Retrieval Augmented Generation Planning (MRAG Planning), focusing
on optimizing MLLM performance while minimizing computational overhead. We
present CogPlanner, a versatile framework inspired by human cognitive
processes. CogPlanner iteratively refines queries and selects retrieval
strategies, enabling both parallel and sequential modeling approaches. To
rigorously evaluate MRAG Planning, we introduce CogBench, a new benchmark
specifically designed for this task. CogBench facilitates the integration of
lightweight CogPlanner with resource-efficient MLLMs. Our experimental findings
demonstrate that CogPlanner surpasses existing MRAG baselines, achieving
significant improvements in both accuracy and efficiency with minimal
computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Aspect Performance-aware Hypergraph Neural Network for <span class="highlight-title">Review</span>-based
  Recommendation <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junrui Liu, Tong Li, Di Wu, Zifang Tang, Yuan Fang, Zhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online reviews allow consumers to provide detailed feedback on various
aspects of items. Existing methods utilize these aspects to model users'
fine-grained preferences for specific item features through graph neural
networks. We argue that the performance of items on different aspects is
important for making precise recommendations, which has not been taken into
account by existing approaches, due to lack of data. In this paper, we propose
an aspect performance-aware hypergraph neural network (APH) for the
review-based recommendation, which learns the performance of items from the
conflicting sentiment polarity of user reviews. Specifically, APH
comprehensively models the relationships among users, items, aspects, and
sentiment polarity by systematically constructing an aspect hypergraph based on
user reviews. In addition, APH aggregates aspects representing users and items
by employing an aspect performance-aware hypergraph aggregation method. It
aggregates the sentiment polarities from multiple users by jointly considering
user preferences and the semantics of their sentiments, determining the weights
of sentiment polarities to infer the performance of items on various aspects.
Such performances are then used as weights to aggregate neighboring aspects.
Experiments on six real-world datasets demonstrate that APH improves MSE,
Precision@5, and Recall@5 by an average of 2.30%, 4.89%, and 1.60% over the
best baseline. The source code and data are available at
https://github.com/dianziliu/APH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted by WSDM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirically-parametrized Spatio-Temporal Extended-SIR Model for
  Combined Dilution and Vaccination Mitigation for Rabies Outbreaks in Wild
  Jackals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Yehuda Samuel, Jonathan Tichon, Roi Lapid, Roni King, Tomer Nissimian, Orr Spiegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transmission of zoonotic diseases between animals and humans poses an
increasing threat. Rabies is a prominent example with various instances
globally, facilitated by a surplus of meso-predators (commonly, facultative
synanthropic species e.g., golden jackals [Canis aureus, hereafter jackals])
thanks to the abundance of anthropogenic resources leading to dense populations
close to human establishments. To mitigate rabies outbreaks and prevent human
infections, authorities target the jackal which is the main rabies vector in
many regions, through the dissemination of oral vaccines in known jackals'
activity centers, as well as opportunistic culling to reduce population
density. Because dilution (i.e., culling) is not selective towards sick or
un-vaccinated individuals, these two complementary epizootic intervention
policies (EIPs) can interfere with each other. Nonetheless, there is only
limited examination of the interactive effectiveness of these EIPs and their
potential influence on rabies epizootic spread dynamics, highlighting the need
to understand these measures and the spread of rabies in wild jackals. In this
study, we introduce a novel spatio-temporal extended-SIR
(susceptible-infected-recovered) model with a graph-based spatial framework for
evaluating mitigation efficiency. We implement the model in a case study using
a jackal population in northern Israel, and using spatial and movement data
collected by Advanced Tracking and Localization of Animals in real-life Systems
(ATLAS) telemetry. An agent-based simulation approach allows us to explore
various biologically-realistic scenarios, and assess the impact of different
EIPs configurations. Our model suggests that under biologically-realistic
underlying assumptions and scenarios, the effectiveness of both EIPs is not
influenced much by the jackal population size but is sensitive to their
dispersal between activity centers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative
user-interactive tool for applications in domains such as e-commerce and
education. Yet, current methodologies predominantly depend on finetuned
Multimodal Large Language Models (MLLMs), which face two critical limitations:
(1) Finetuning imposes prohibitive computational overhead and long-term
maintenance costs. (2) Finetuning narrows the pretrained knowledge distribution
of MLLMs, reducing their adaptability to novel scenarios. These issues are
exacerbated by the inherently dynamic nature of real-world I-TIR systems, where
queries and image databases evolve in complexity and diversity, often deviating
from static training distributions. To overcome these constraints, we propose
Diffusion Augmented Retrieval (DAR), a paradigm-shifting framework that
bypasses MLLM finetuning entirely. DAR synergizes Large Language Model
(LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis
to create contextually enriched intermediate representations. This
dual-modality approach deciphers nuanced user intent more holistically,
enabling precise alignment between textual queries and visually relevant
images. Rigorous evaluations across four benchmarks reveal DAR's dual
strengths: (1) Matches state-of-the-art finetuned I-TIR models on
straightforward queries without task-specific training. (2) Scalable
Generalization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10
accuracy) under multi-turn conversational complexity, demonstrating robustness
to intricate, distributionally shifted interactions. By eliminating finetuning
dependencies and leveraging generative-augmented representations, DAR
establishes a new trajectory for efficient, adaptive, and scalable cross-modal
retrieval systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Mitigate Information <span class="highlight-title">Loss</span> in <span class="highlight-title">Knowledge</span> Graphs for GraphRAG:
  Leveraging Triple Context Restoration and Query-Driven Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manzong Huang, Chenyang Bu, Yi He, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently
propelled significant advances in complex reasoning tasks, thanks to their
broad domain knowledge and contextual awareness. Unfortunately, current methods
often assume KGs to be complete, which is impractical given the inherent
limitations of KG construction and the potential loss of contextual cues when
converting unstructured text into entity-relation triples. In response, this
paper proposes the Triple Context Restoration and Query-driven Feedback
(TCR-QF) framework, which reconstructs the textual context underlying each
triple to mitigate information loss, while dynamically refining the KG
structure by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the
effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%
improvement in Exact Match and a 15.5% improvement in F1 over its
state-of-the-art GraphRAG competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Representation Learning via Causal Diffusion for
  Out-of-Distribution Recommendation <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chu Zhao, Enneng Yang, Yuliang Liang, Pengxiang Lan, Yuting Liu, Jianzhe Zhao, Guibing Guo, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume
that training and testing data are drawn from independent and identically
distributed (IID) spaces. However, this assumption often fails in the presence
of out-of-distribution (OOD) data, resulting in significant performance
degradation. In this study, we construct a Structural Causal Model (SCM) to
analyze interaction data, revealing that environmental confounders (e.g., the
COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus
impairing their generalization to OOD data. To address this issue, we propose a
novel approach, graph representation learning via causal diffusion
(CausalDiffRec) for OOD recommendation. This method enhances the model's
generalization on OOD data by eliminating environmental confounding factors and
learning invariant graph representations. Specifically, we use backdoor
adjustment and variational inference to infer the real environmental
distribution, thereby eliminating the impact of environmental confounders. This
inferred distribution is then used as prior knowledge to guide the
representation learning in the reverse phase of the diffusion process to learn
the invariant representation. In addition, we provide a theoretical derivation
that proves optimizing the objective function of CausalDiffRec can encourage
the model to learn environment-invariant graph representations, thereby
achieving excellent generalization performance in recommendations under
distribution shifts. Our extensive experiments validate the effectiveness of
CausalDiffRec in improving the generalization of OOD data, and the average
improvement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and
11.65% on Douban datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted by WWW2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of OM
tools. Our framework is implemented in a proof-of-concept system. Evaluations
of three Ontology Alignment Evaluation Initiative (OAEI) tracks over
state-of-the-art OM systems show that our system can achieve results very close
to the long-standing best performance on simple OM tasks and can significantly
improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multi Media
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Complex Heterogeneous Multimodal Fake News via Social Latent
  Network Inference <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxin Li, Yuchen Zhang, Haowei Xu, Xianghua Li, Chao Gao, Zhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the diversification of online social platforms, news dissemination has
become increasingly complex, heterogeneous, and multimodal, making the fake
news detection task more challenging and crucial. Previous works mainly focus
on obtaining social relationships of news via retweets, limiting the accurate
detection when real cascades are inaccessible. Given the proven assessment of
the spreading influence of events, this paper proposes a method called HML
(Complex Heterogeneous Multimodal Fake News Detection method via Latent Network
Inference). Specifically, an improved social latent network inference strategy
is designed to estimate the maximum likelihood of news influences under the
same event. Meanwhile, a novel heterogeneous graph is built based on social
attributes for multimodal news under different events. Further, to better
aggregate the relationships among heterogeneous multimodal features, this paper
proposes a self-supervised-based multimodal content learning strategy, to
enhance, align, fuse and compare heterogeneous modal contents. Based above, a
personalized heterogeneous graph representation learning is designed to
classify fake news. Extensive experiments demonstrate that the proposed method
outperforms the SOTA in real social media news datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in
  Hateful Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Rui Yang Tan, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting hate speech in online content is essential to ensuring safer
digital spaces. While significant progress has been made in text and meme
modalities, video-based hate speech detection remains under-explored, hindered
by a lack of annotated datasets and the high cost of video annotation. This gap
is particularly problematic given the growing reliance on large models, which
demand substantial amounts of training data. To address this challenge, we
leverage meme datasets as both a substitution and an augmentation strategy for
training hateful video detection models. Our approach introduces a
human-assisted reannotation pipeline to align meme dataset labels with video
datasets, ensuring consistency with minimal labeling effort. Using two
state-of-the-art vision-language models, we demonstrate that meme data can
substitute for video data in resource-scarce scenarios and augment video
datasets to achieve further performance gains. Our results consistently
outperform state-of-the-art benchmarks, showcasing the potential of cross-modal
transfer learning for advancing hateful video detection. Dataset and code are
available at https://github.com/Social-AI-Studio/CrossModalTransferLearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, THE WEB CONFERENCE 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-03T05:27:57.327832479Z">
            2025-02-03 05:27:57 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
